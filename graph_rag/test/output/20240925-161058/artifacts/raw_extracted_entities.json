{
    "id": "8fb263160e5ad503810c873365e6fad1",
    "chunk": "Blog\nLLM Powered Autonomous Agents | Lil'Log\n\nLil'Log\nLLM Powered Autonomous Agents\n    \nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n\n\n \n\n\nTable of Contents\n\n\n\nAgent System Overview\n\nComponent One: Planning\n\nTask Decomposition\n\nSelf-Reflection\n\n\nComponent Two: Memory\n\nTypes of Memory\n\nMaximum Inner Product Search (MIPS)\n\n\nComponent Three: Tool Use\n\nCase Studies\n\nScientific Discovery Agent\n\nGenerative Agents Simulation\n\nProof-of-Concept Examples\n\n\nChallenges\n\nCitation\n\nReferences\n\n\n\n\n\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn a LLM-powered autonomous agent system, LLM functions as the agent\u2019s brain, complemented by several key components:\n\nPlanning\n\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n\n\nMemory\n\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n\n\nTool use\n\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n\n\n\n\nFig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \u201cthink step by step\u201d to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model\u2019s thinking process.\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a",
    "chunk_id": "8fb263160e5ad503810c873365e6fad1",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "c7cc77a29264e3243a4810a38ec3fd2c",
    "chunk": " into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model\u2019s thinking process.\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into \u201cProblem PDDL\u201d, then (2) requests a classical planner to generate a PDDL plan based on an existing \u201cDomain PDDL\u201d, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\nSelf-Reflection#\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\nThought: ...\nAction: ...\nObservation: ...\n... (Repeated many times)\n\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step is removed.\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the",
    "chunk_id": "c7cc77a29264e3243a4810a38ec3fd2c",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "8a6f4d8721ab342d674d8129f7668cb6",
    "chunk": " and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step is removed.\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\n\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent\u2019s working memory, up to three, to be used as context for querying LLM.\n\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on",
    "chunk_id": "8a6f4d8721ab342d674d8129f7668cb6",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "ecc6df204fa26593abb5e1380f5d7779",
    "chunk": "geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\n\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by",
    "chunk_id": "ecc6df204fa26593abb5e1380f5d7779",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "f65a83ee80c75b5ee0d3f90419b7ae45",
    "chunk": " is task-agnostic.\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\n\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\nComponent Two: Memory#\n(Big thank you to ChatGPT for helping me draft this section. I\u2019ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\nTypes of Memory#\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\n\n\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n\n\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n\n\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n\nExplicit \/ declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\nImplicit \/ procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n\n\n\n\nFig. 8. Categorization of human memory.\nWe can roughly consider the following mappings:\n\nSensory memory as learning embedding representations for raw inputs, including text",
    "chunk_id": "f65a83ee80c75b5ee0d3f90419b7ae45",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "1cf39b6c5bae4b8820810c24e7233c9f",
    "chunk": " is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\nImplicit \/ procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n\n\n\n\nFig. 8. Categorization of human memory.\nWe can roughly consider the following mappings:\n\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n\nMaximum Inner Product Search (MIPS)#\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\nA couple common choices of ANN algorithms for fast MIPS:\n\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. \u201csix degrees of separation\u201d feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can\u2019t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between",
    "chunk_id": "1cf39b6c5bae4b8820810c24e7233c9f",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "79b71c9bf45d207c11b1d10672a35bf9",
    "chunk": ". When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can\u2019t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\n\n\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\nComponent Three: Tool Use#\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\n\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\nMRKL (Karpas et al. 2022), short for \u201cModular Reasoning, Knowledge and Language\u201d, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of \u201cexpert\u201d modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\nBoth TALM (Tool Augmented Language Models;",
    "chunk_id": "79b71c9bf45d207c11b1d10672a35bf9",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "8581539bab4047888f24cad08dd15342",
    "chunk": " fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the \u201cExternal APIs\u201d section of Prompt Engineering.\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\n\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\nThe system comprises of 4 stages:\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\nInstruction:\n\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to",
    "chunk_id": "8581539bab4047888f24cad08dd15342",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "2b251fb8a3ee7ea3306ce96fcd54c154",
    "chunk": " user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\nInstruction:\n\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\n\n(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\n\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\n\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\nIn the API-Bank workflow, L",
    "chunk_id": "2b251fb8a3ee7ea3306ce96fcd54c154",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "c86e4bdceac1b0b9ee79d0fb2a85d29e",
    "chunk": ", calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\n\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\n\nWhether an API call is needed.\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\n\nThis benchmark evaluates the agent\u2019s tool use capabilities at three levels:\n\nLevel-1 evaluates the ability to call the API. Given an API\u2019s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user\u2019s requirement and learn how to use them by reading documentation.\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight\/hotel\/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\n\nCase Studies#\nScientific Discovery Agent#\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\n\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input\/output.\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\n\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery,",
    "chunk_id": "c86e4bdceac1b0b9ee79d0fb2a85d29e",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "773d4acdfb70293e0b9a57df94796a16",
    "chunk": " equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\n\ninquired about current trends in anticancer drug discovery;\nselected a target;\nrequested a scaffold targeting these compounds;\nOnce the compound was identified, the model attempted its synthesis.\n\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\nGenerative Agents Simulation#\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents\u2019 experience in natural language.\n\nEach element is an observation, an event directly provided by the agent.\n- Inter-agent communication can trigger new natural language statements.\n\n\nRetrieval model: surfaces the context to inform the agent\u2019s behavior, according to relevance, recency and importance.\n\nRecency: recent events have higher scores\nImportance: distinguish mundane from core memories. Ask LM directly.\nRelevance: based on how related it is to the current situation \/ query.\n\n\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent\u2019s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations\/statements. Then ask LM to answer those questions.\n\n\nPlanning & Reacting: translate the reflections and",
    "chunk_id": "773d4acdfb70293e0b9a57df94796a16",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "bf2382547acbfbef02b09520413a611f",
    "chunk": " on how related it is to the current situation \/ query.\n\n\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent\u2019s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations\/statements. Then ask LM to answer those questions.\n\n\nPlanning & Reacting: translate the reflections and the environment information into actions\n\nPlanning is essentially in order to optimize believability at the moment vs in time.\nPrompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\nEnvironment information is present in a tree structure.\n\n\n\n\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\nProof-of-Concept Examples#\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\nYou are {{ai-name}}, {{user-provided AI bot description}}.\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n\nGOALS:\n\n1. {{user-provided goal 1}}\n2. {{user-provided goal 2}}\n3. ...\n4. ...\n5. ...\n\nConstraints:\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n3. No user assistance\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n5. Use subprocesses for commands that will not terminate within a few minutes\n\nCommands:\n1. Google Search: \"google\", args: \"input\": \"<search>\"\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n4. Message GPT Agent: \"message_agent\", args: \"key",
    "chunk_id": "bf2382547acbfbef02b09520413a611f",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "506a6c5555222943aef4c5330fb2db24",
    "chunk": " a few minutes\n\nCommands:\n1. Google Search: \"google\", args: \"input\": \"<search>\"\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n5. List GPT Agents: \"list_agents\", args:\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n19. Do Nothing: \"do_nothing\", args:\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\n\nResources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n\nYou should only respond in JSON format as described below\nResponse Format:\n{\n    \"thoughts\": {\n        \"text\": \"thought\",\n        \"reasoning\": \"reasoning\",\n        \"plan\": \"- short bulleted\\n- list that conveys\\n",
    "chunk_id": "506a6c5555222943aef4c5330fb2db24",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "3e5bf829690290ac703d57c260135bee",
    "chunk": " self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n\nYou should only respond in JSON format as described below\nResponse Format:\n{\n    \"thoughts\": {\n        \"text\": \"thought\",\n        \"reasoning\": \"reasoning\",\n        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n        \"criticism\": \"constructive self-criticism\",\n        \"speak\": \"thoughts summary to say to user\"\n    },\n    \"command\": {\n        \"name\": \"command name\",\n        \"args\": {\n            \"arg name\": \"value\"\n        }\n    }\n}\nEnsure the response can be parsed by Python json.loads\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\n[\n  {\n    \"role\": \"system\",\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\"\n  },\n  {\n    \"role\": \"assistant\",\n    \"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\",
    "chunk_id": "3e5bf829690290ac703d57c260135bee",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "ed8e0fb81f2338fc1d5b69ea8c0c9624",
    "chunk": "    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\"\n  },\n  {\n    \"role\": \"assistant\",\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\n  }\n]\nThen after these clarification, the agent moved into the code writing mode with a different system message.\nSystem message:\n\nYou will get instructions for code to write.\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\nMake sure that every detail of the architecture is, in the end, implemented as code.\nThink step by step and reason yourself to the right decisions to make sure we get it right.\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\nThen you will output the content of each file including ALL code.\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\nFILENAME is the lowercase file name including the file extension,\nLANG is the markup code block language for the code\u2019s language, and CODE is the code:\nFILENAME\nCODE\nYou will start with the \u201centrypoint\u201d file, then go to the ones that are imported by that file, and so on.\nPlease note that the code should be fully functional. No placeholders.\nFollow a language and framework appropriate best practice file naming convention.\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\nEnsure to implement all code, if you are unsure, write a plausible implementation.\nInclude module dependency or package manager dependency definition file.\nBefore you finish, double check that all parts of the architecture is present in the files.\nUseful to know:\nYou almost always put different classes in different files.\nFor Python, you always create an appropriate requirements.txt file.\nFor NodeJS, you always create an appropriate package.json file.\nYou always add a comment briefly describing the purpose of the function definition.\nYou try to add comments explaining very complex bits of logic.\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\npackage\/project.\nPython toolbelt preferences:\n\npytest\ndataclasses\n\n\n",
    "chunk_id": "ed8e0fb81f2338fc1d5b69ea8c0c9624",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "45fa7fba03b681a5b8a8908f1eb46d98",
    "chunk": "Useful to know:\nYou almost always put different classes in different files.\nFor Python, you always create an appropriate requirements.txt file.\nFor NodeJS, you always create an appropriate package.json file.\nYou always add a comment briefly describing the purpose of the function definition.\nYou try to add comments explaining very complex bits of logic.\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\npackage\/project.\nPython toolbelt preferences:\n\npytest\ndataclasses\n\n\nConversatin samples:\n[\n  {\n    \"role\": \"system\",\n    \"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage\/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n  },\n #  \u2026 same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\n  {\n    \"role\": \"assistant\",\n    \"content\": \"Assumptions:\\n1. Model: The model will",
    "chunk_id": "45fa7fba03b681a5b8a8908f1eb46d98",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "956670305f6fc97c21a0bdb55f1cbd21",
    "chunk": " comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage\/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n  },\n #  \u2026 same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\n  {\n    \"role\": \"assistant\",\n    \"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\"\n  }\n]\nChallenges#\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\n\n\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n\n\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans",
    "chunk_id": "956670305f6fc97c21a0bdb55f1cbd21",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "197bbd672bd4d5c046b20fe739370c93",
    "chunk": ", API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n\n\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n\n\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\n\n\nCitation#\nCited as:\n\nWeng, Lilian. (Jun 2023). \u201cLLM-powered Autonomous Agents\u201d. Lil\u2019Log. https:\/\/lilianweng.github.io\/posts\/2023-06-23-agent\/.\n\nOr\n@article{weng2023agent,\n  title   = \"LLM-powered Autonomous Agents\",\n  author  = \"Weng, Lilian\",\n  journal = \"lilianweng.github.io\",\n  year    = \"2023\",\n  month   = \"Jun\",\n  url     = \"https:\/\/lilianweng.github.io\/posts\/2023-06-23-agent\/\"\n}\nReferences#\n[1] Wei et al. \u201cChain of thought prompting elicits reasoning in large language models.\u201d NeurIPS 2022\n[2] Yao et al. \u201cTree of Thoughts: Dliberate Problem Solving with Large Language Models.\u201d arXiv preprint arXiv:2305.10601 (2023).\n[3] Liu et al. \u201cChain of Hindsight Aligns Language Models with Feedback\n\u201c arXiv preprint arXiv:2302.02676 (2023).\n[4] Liu et al. \u201cLLM+P: Empowering Large Language Models with Optimal Planning Proficiency\u201d arXiv preprint arXiv:2304.11477 (2023).\n[5] Yao et al. \u201cReAct: Synergizing reasoning and acting in language models.\u201d ICLR 2023.\n[6] Google Blog. \u201cAnnouncing ScaNN: Efficient Vector Similarity Search\u201d July 28, 2020.\n[7] https:\/\/chat.openai.com\/share\/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n[8] Shinn & Labash. \u201cReflexion: an autonomous agent with dynamic memory and self-reflection\u201d arXiv preprint arXiv",
    "chunk_id": "197bbd672bd4d5c046b20fe739370c93",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "8e82a9387edfe0ca4030c02afaf4c456",
    "chunk": ".\u201d ICLR 2023.\n[6] Google Blog. \u201cAnnouncing ScaNN: Efficient Vector Similarity Search\u201d July 28, 2020.\n[7] https:\/\/chat.openai.com\/share\/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n[8] Shinn & Labash. \u201cReflexion: an autonomous agent with dynamic memory and self-reflection\u201d arXiv preprint arXiv:2303.11366 (2023).\n[9] Laskin et al. \u201cIn-context Reinforcement Learning with Algorithm Distillation\u201d ICLR 2023.\n[10] Karpas et al. \u201cMRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.\u201d arXiv preprint arXiv:2205.00445 (2022).\n[11] Nakano et al. \u201cWebgpt: Browser-assisted question-answering with human feedback.\u201d arXiv preprint arXiv:2112.09332 (2021).\n[12] Parisi et al. \u201cTALM: Tool Augmented Language Models\u201d\n[13] Schick et al. \u201cToolformer: Language Models Can Teach Themselves to Use Tools.\u201d arXiv preprint arXiv:2302.04761 (2023).\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n[15] Li et al. \u201cAPI-Bank: A Benchmark for Tool-Augmented LLMs\u201d arXiv preprint arXiv:2304.08244 (2023).\n[16] Shen et al. \u201cHuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace\u201d arXiv preprint arXiv:2303.17580 (2023).\n[17] Bran et al. \u201cChemCrow: Augmenting large-language models with chemistry tools.\u201d arXiv preprint arXiv:2304.05376 (2023).\n[18] Boiko et al. \u201cEmergent autonomous scientific research capabilities of large language models.\u201d arXiv preprint arXiv:2304.05332 (2023).\n[19] Joon Sung Park, et al. \u201cGenerative Agents: Interactive Simulacra of Human Behavior.\u201d arXiv preprint arXiv:2304.03442 (2023).\n[20] AutoGPT. https:\/\/github.com\/Significant-Gravitas\/Auto-GPT\n[21] GPT-Engineer. https:\/\/github.com\/AntonOsika\/gpt-engineer\n\n\n\nnlp\nlanguage-model\nagent\nsteerability\nprompting\n\n\n\n\u00ab \n\nAdversarial Attacks on LLMs\n\n\n \u00bb\n\nPrompt Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a9 2024",
    "chunk_id": "8e82a9387edfe0ca4030c02afaf4c456",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "5362446350dfc49693fb5422a6621425",
    "chunk": " Behavior.\u201d arXiv preprint arXiv:2304.03442 (2023).\n[20] AutoGPT. https:\/\/github.com\/Significant-Gravitas\/Auto-GPT\n[21] GPT-Engineer. https:\/\/github.com\/AntonOsika\/gpt-engineer\n\n\n\nnlp\nlanguage-model\nagent\nsteerability\nprompting\n\n\n\n\u00ab \n\nAdversarial Attacks on LLMs\n\n\n \u00bb\n\nPrompt Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a9 2024 Lil'Log\n\n        Powered by\n        Hugo &\n        PaperMod\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "chunk_id": "5362446350dfc49693fb5422a6621425",
    "document_ids": [
        "3fee4530342ef98ff23b43d5b7086ce8"
    ],
    "n_tokens": 115,
    "entities": [
        {
            "name": "\"ORGANIZATION\"",
            "type": "\"AUTOGPT\"",
            "description": "\"AutoGPT is an organization that developed a preprint titled 'Behavior.'\"",
            "source_id": "5362446350dfc49693fb5422a6621425"
        },
        {
            "name": "\"PERSON\"",
            "type": "\"SIGNIFICANT-GRAVITAS\"",
            "description": "\"Significant-Gravitas is the person behind the AutoGPT project.\"",
            "source_id": "5362446350dfc49693fb5422a6621425"
        },
        {
            "name": "\"GEO\"",
            "type": "\"ARXIV\"",
            "description": "\"arXiv is a location where preprints are published, including 'Behavior' by AutoGPT.\"",
            "source_id": "5362446350dfc49693fb5422a6621425"
        },
        {
            "name": "\"EVENT\"",
            "type": "\"ADVERSARIAL ATTACKS ON LLMS\"",
            "description": "\"Adversarial Attacks on LLMs is an event or topic related to the field of natural language processing.\"",
            "source_id": "5362446350dfc49693fb5422a6621425"
        },
        {
            "name": "\"AUTOGPT\"",
            "type": "",
            "description": "",
            "source_id": "5362446350dfc49693fb5422a6621425"
        },
        {
            "name": "\"SIGNIFICANT-GRAVITAS\"",
            "type": "",
            "description": "",
            "source_id": "5362446350dfc49693fb5422a6621425"
        },
        {
            "name": "\"ARXIV\"",
            "type": "",
            "description": "",
            "source_id": "5362446350dfc49693fb5422a6621425"
        }
    ],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"&quot;ORGANIZATION&quot;\">      <data key=\"d0\">\"AUTOGPT\"<\/data>      <data key=\"d1\">\"AutoGPT is an organization that developed a preprint titled 'Behavior.'\"<\/data>      <data key=\"d2\">5362446350dfc49693fb5422a6621425<\/data>    <\/node>    <node id=\"&quot;PERSON&quot;\">      <data key=\"d0\">\"SIGNIFICANT-GRAVITAS\"<\/data>      <data key=\"d1\">\"Significant-Gravitas is the person behind the AutoGPT project.\"<\/data>      <data key=\"d2\">5362446350dfc49693fb5422a6621425<\/data>    <\/node>    <node id=\"&quot;GEO&quot;\">      <data key=\"d0\">\"ARXIV\"<\/data>      <data key=\"d1\">\"arXiv is a location where preprints are published, including 'Behavior' by AutoGPT.\"<\/data>      <data key=\"d2\">5362446350dfc49693fb5422a6621425<\/data>    <\/node>    <node id=\"&quot;EVENT&quot;\">      <data key=\"d0\">\"ADVERSARIAL ATTACKS ON LLMS\"<\/data>      <data key=\"d1\">\"Adversarial Attacks on LLMs is an event or topic related to the field of natural language processing.\"<\/data>      <data key=\"d2\">5362446350dfc49693fb5422a6621425<\/data>    <\/node>    <node id=\"&quot;AUTOGPT&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5362446350dfc49693fb5422a6621425<\/data>    <\/node>    <node id=\"&quot;SIGNIFICANT-GRAVITAS&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5362446350dfc49693fb5422a6621425<\/data>    <\/node>    <node id=\"&quot;ARXIV&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">5362446350dfc49693fb5422a6621425<\/data>    <\/node>    <edge source=\"&quot;AUTOGPT&quot;\" target=\"&quot;SIGNIFICANT-GRAVITAS&quot;\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">\"AutoGPT was developed by Significant-Gravitas, indicating a direct relationship between the organization and its creator.\"<\/data>      <data key=\"d5\">5362446350dfc49693fb5422a6621425<\/data>    <\/edge>    <edge source=\"&quot;AUTOGPT&quot;\" target=\"&quot;ARXIV&quot;\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">\"The preprint 'Behavior' by AutoGPT was published on arXiv, showing a connection between the publication platform and the project.\"<\/data>      <data key=\"d5\">5362446350dfc49693fb5422a6621425<\/data>    <\/edge>  <\/graph><\/graphml>"
}
{
    "id": "cc883525c7e386d12bbe9cf68b6154f1",
    "chunk": "Blog\n\nPrompt Engineering | Lil'Log\n\n\nLil'Log\nPrompt Engineering\n    \nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\n\n\n \n\n\nTable of Contents\n\n\n\nBasic Prompting\n\nZero-Shot\n\nFew-shot\n\nTips for Example Selection\n\nTips for Example Ordering\n\n\n\nInstruction Prompting\n\nSelf-Consistency Sampling\n\nChain-of-Thought (CoT)\n\nTypes of CoT prompts\n\nTips and Extensions\n\n\nAutomatic Prompt Design\n\nAugmented Language Models\n\nRetrieval\n\nProgramming Language\n\nExternal APIs\n\n\nCitation\n\nUseful Resources\n\nReferences\n\n\n\n\n\nPrompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\n[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\nBasic Prompting#\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\nZero-Shot#\nZero-shot learning is to simply feed the task text to the model and ask for results.\n(All the sentiment analysis examples are from SST-2)\nText: i'll bet the video game is a lot more fun than the film.\nSentiment:\nFew-shot#\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\nSentiment: positive\n\nText: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to",
    "chunk_id": "cc883525c7e386d12bbe9cf68b6154f1",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "d6edcef46b4d7d0a4c65159217d602f0",
    "chunk": " token consumption and may hit the context length limit when input and output text are long.\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\nSentiment: positive\n\nText: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\nSentiment: negative\n\nText: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\nSentiment: positive\n\nText: i'll bet the video game is a lot more fun than the film.\nSentiment:\nMany studies looked into how to construct in-context examples to maximize the performance and observed that choice of prompt format, training examples, and the order of the examples can lead to dramatically different performance, from near random guess to near SoTA.\nZhao et al. (2021) investigated the case of few-shot classification and proposed that several biases with LLM (they use GPT-3 in the experiments) contribute to such high variance: (1) Majority label bias exists if distribution of labels among the examples is unbalanced; (2) Recency bias refers to the tendency where the model may repeat the label at the end; (3) Common token bias indicates that LLM tends to produce common tokens more often than rare tokens. To conquer such bias, they proposed a method to calibrate the label probabilities output by the model to be uniform when the input string is N\/A.\nTips for Example Selection#\n\n\nChoose examples that are semantically similar to the test example using $k$-NN clustering in the embedding space (Liu et al., 2021)\n\n\nTo select a diverse and representative set of examples, Su et al. (2022) proposed to use a graph-based approach: (1) First, construct a directed graph $G=(V, E)$ based on the embedding (e.g. by SBERT or other embedding models) cosine similarity between samples, where each node points to its $k$ nearest neighbors; (2) Start with a set of selected samples $\\mathcal{L}=\\emptyset$ and a set of remaining samples $\\mathcal{U}$. Each sample $u \\in \\mathcal{U}$ is scored by $$\n\\text{score}(u) = \\sum_{v \\in \\{v \\mid (u, v) \\in E, v\\in \\mathcal{U}\\}} s(v)\\quad\\text{where }s(v)=\\rho^{- \\vert \\{\\ell \\in \\mathcal{L} \\vert (v, \\ell)\\in E \\}\\vert},\\quad\\rho > 1\n$$",
    "chunk_id": "d6edcef46b4d7d0a4c65159217d602f0",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "457496d4db611db4b38b1c887e1dea28",
    "chunk": " \\mathcal{U}$ is scored by $$\n\\text{score}(u) = \\sum_{v \\in \\{v \\mid (u, v) \\in E, v\\in \\mathcal{U}\\}} s(v)\\quad\\text{where }s(v)=\\rho^{- \\vert \\{\\ell \\in \\mathcal{L} \\vert (v, \\ell)\\in E \\}\\vert},\\quad\\rho > 1\n$$ such that $s(v)$ is low if many of $v$\u2019s neighbors are selected and thus the scoring encourages to pick diverse samples.\n\n\nRubin et al. (2022) proposed to train embeddings via contrastive learning specific to one training dataset for in-context learning sample selection.  Given each training pair $(x, y)$, the quality of one example $e_i$ (formatted input-output pair) can be measured by a conditioned probability assigned by LM: $\\text{score}(e_i) = P_\\text{LM}(y \\mid e_i, x)$. We can identify other examples with top-$k$ and bottom-$k$ scores as positive and negative sets of candidates for every training pair and use that for contrastive learning.\n\n\nSome researchers tried Q-Learning to do sample selection. (Zhang et al. 2022)\n\n\nMotivated by uncertainty-based active learning, Diao et al. (2023) suggested to identify examples with high disagreement or entropy among multiple sampling trials. Then annotate these examples to be used in few-shot prompts.\n\n\nTips for Example Ordering#\n\nA general suggestion is to keep the selection of examples diverse, relevant to the test sample and in random order to avoid majority label bias and recency bias.\nIncreasing model sizes or including more training examples does not reduce variance among different permutations of in-context examples. Same order may work well for one model but badly for another. When the validation set is limited, consider choosing the order such that the model does not produce extremely unbalanced predictions or being overconfident about its predictions. (Lu et al. 2022)\n\nInstruction Prompting#\nThe purpose of presenting few-shot examples in the prompt is to explain our intent to the model; in other words, describe the task instruction to the model in the form of demonstrations. However, few-shot can be expensive in terms of token usage and restricts the input length due to limited context length. So, why not just give the instruction directly?\nInstructed LM (e.g. InstructGPT, natural instruction) finetunes a pretrained model with high-quality tuples of (task instruction, input, ground truth output) to make LM better understand user intention and follow instruction. RLHF (Reinforcement Learning from Human Feedback) is a common method to do so. The benefit of instruction following style fine-tuning improves the model to be more aligned with human intention and greatly reduces the cost of communication.\nWhen interacting with instruction models",
    "chunk_id": "457496d4db611db4b38b1c887e1dea28",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "853cfcfe6b8d07ab24514db81aad3e59",
    "chunk": "?\nInstructed LM (e.g. InstructGPT, natural instruction) finetunes a pretrained model with high-quality tuples of (task instruction, input, ground truth output) to make LM better understand user intention and follow instruction. RLHF (Reinforcement Learning from Human Feedback) is a common method to do so. The benefit of instruction following style fine-tuning improves the model to be more aligned with human intention and greatly reduces the cost of communication.\nWhen interacting with instruction models, we should describe the task requirement in details, trying to be specific and precise and avoiding say \u201cnot do something\u201d but rather specify what to do.\nPlease label the sentiment towards the movie of the given movie review. The sentiment label should be \"positive\" or \"negative\". \nText: i'll bet the video game is a lot more fun than the film. \nSentiment:\nExplaining the desired audience is another smart way to give instructions\n\nFor example to produce education materials for kids,\n\nDescribe what is quantum physics to a 6-year-old.\n\nAnd safe content,\n\n... in language that is safe for work.\nIn-context instruction learning (Ye et al. 2023) combines few-shot learning with instruction prompting. It incorporates multiple demonstration examples across different tasks in the prompt, each demonstration consisting of instruction, task input and output. Note that their experiments were only on classification tasks and the instruction prompt contains all label options.\nDefinition: Determine the speaker of the dialogue, \"agent\" or \"customer\".\nInput: I have successfully booked your tickets.\nOuput: agent\n\nDefinition: Determine which category the question asks for, \"Quantity\" or \"Location\".\nInput: What's the oldest building in US?\nOuput: Location\n\nDefinition: Classify the sentiment of the given movie review, \"positive\" or \"negative\".\nInput: i'll bet the video game is a lot more fun than the film.\nOutput:\nSelf-Consistency Sampling#\nSelf-consistency sampling (Wang et al. 2022a) is to sample multiple outputs with temperature > 0 and then selecting the best one out of these candidates.\nThe criteria for selecting the best candidate can vary from task to task. A general solution is to pick majority vote. For tasks that are easy to validate such as a programming question with unit tests, we can simply run through the interpreter and verify the correctness with unit tests.\nChain-of-Thought (CoT)#\nChain-of-thought (CoT) prompting (Wei et al. 2022) generates a sequence of short sentences to describe reasoning logics step by step, known as reasoning chains or rationales, to eventually lead to the final answer. The benefit of CoT is more pronounced for complicated reasoning tasks, while using large models (e.g. with more than 50B parameters). Simple tasks only benefit slightly from CoT prompting.\nTypes of CoT prompts#\nTwo main types of CoT prompting:\n\nFew-shot CoT",
    "chunk_id": "853cfcfe6b8d07ab24514db81aad3e59",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "c7ba1e2210636c000198e6c378947a75",
    "chunk": "T) prompting (Wei et al. 2022) generates a sequence of short sentences to describe reasoning logics step by step, known as reasoning chains or rationales, to eventually lead to the final answer. The benefit of CoT is more pronounced for complicated reasoning tasks, while using large models (e.g. with more than 50B parameters). Simple tasks only benefit slightly from CoT prompting.\nTypes of CoT prompts#\nTwo main types of CoT prompting:\n\nFew-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.\n\n(All the math reasoning examples are from GSM8k)\nQuestion: Tom and Elizabeth have a competition to climb a hill. Elizabeth takes 30 minutes to climb the hill. Tom takes four times as long as Elizabeth does to climb the hill. How many hours does it take Tom to climb up the hill?\nAnswer: It takes Tom 30*4 = <<30*4=120>>120 minutes to climb the hill.\nIt takes Tom 120\/60 = <<120\/60=2>>2 hours to climb the hill.\nSo the answer is 2.\n===\nQuestion: Jack is a soccer player. He needs to buy two pairs of socks and a pair of soccer shoes. Each pair of socks cost $9.50, and the shoes cost $92. Jack has $40. How much more money does Jack need?\nAnswer: The total cost of two pairs of socks is $9.50 x 2 = $<<9.5*2=19>>19.\nThe total cost of the socks and the shoes is $19 + $92 = $<<19+92=111>>111.\nJack need $111 - $40 = $<<111-40=71>>71 more.\nSo the answer is 71.\n===\nQuestion: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?\nAnswer:\n\nZero-shot CoT. Use natural language statement like Let's think step by step to explicitly encourage the model to first generate reasoning chains and then to prompt with Therefore, the answer is to produce answers (Kojima et al. 2022 ). Or a similar statement Let's work this out it a step by step to be sure we have the right answer (Zhou et al. 2022).\n\nQuestion: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?\nAnswer: Let's think step by step.\nTips and Extensions#\n\n\nSelf-consistency sampling can improve reasoning accuracy by sampling a number of diverse answers and then taking the majority vote. (Wang et al. 2022a)\n\n\nAnother approach for ensemble learning is",
    "chunk_id": "c7ba1e2210636c000198e6c378947a75",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "3e08ba63efabadbd93cee86aafadfb2f",
    "chunk": ". 2022).\n\nQuestion: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?\nAnswer: Let's think step by step.\nTips and Extensions#\n\n\nSelf-consistency sampling can improve reasoning accuracy by sampling a number of diverse answers and then taking the majority vote. (Wang et al. 2022a)\n\n\nAnother approach for ensemble learning is to alter the example order or use model generated rationales to replace human-written ones to introduce randomness during multiple sample trials. Then aggregate model outputs with a majority vote to get final answer. (Wang et al. 2022b)\n\n\nIf training examples are only associated with true answers (easy to verify!) but no rationales, we can follow the STaR (Self-Taught Reasoner; Zelikman et al. 2022) method : (1) Ask LLM to generate reasoning chains and only keep those leading to correct answers; (2) Then fine-tune the model with generated rationales and repeat the process until convergence. Note that higher temperature is more likely to generate incorrect rationales with correct answers. If training examples do not have ground truth answers, maybe consider using majority votes as the \u201ccorrect\u201d answers.\n\n\nPrompts with demonstrations of higher reasoning complexity can achieve better performance, where complexity is measured by the number of reasoning steps in the chains. When separating reasoning steps, newline \\n symbol works better than step i, period . or semicolon ;. (Fu et al. 2023)\n\n\nComplexity-based consistency is to explicitly prefer complex chains among all the generations by taking majority vote among only top $k$ complex chains. (Fu et al. 2023)\n\n\nLater, Shum et al. (2023) found that in their experiments CoT prompts with only complex examples can improve the accuracy of complex questions, but perform poorly in simple questions; evidence shown on GSM8k.\n\n\nChanging Q: to Question: is found to be helpful. (Fu et al. 2023)\n\n\nYe & Durrett (2022) found that the benefit of including explanations in the prompt is small to moderate for NLP tasks that involve reasoning over text (i.e. QA and NLI) and the effects vary by models. They observed that explanations are more likely to be nonfactual than be inconsistent (i.e. whether explanation entails prediction). Nonfactual explanations most likely lead to incorrect predictions.\n\n\nSelf-Ask (Press et al. 2022) is a method to repeatedly prompt the model to ask following-up questions to construct the thought process iteratively. Follow-up questions can be answered by search engine results. Similarly, IRCoT (Interleaving Retrieval CoT; Trivedi et al. 2022) and ReAct (Reason + Act; Yao et al. 2023) combines iterative",
    "chunk_id": "3e08ba63efabadbd93cee86aafadfb2f",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "e559704eff72a598c60098aa74f5088c",
    "chunk": " entails prediction). Nonfactual explanations most likely lead to incorrect predictions.\n\n\nSelf-Ask (Press et al. 2022) is a method to repeatedly prompt the model to ask following-up questions to construct the thought process iteratively. Follow-up questions can be answered by search engine results. Similarly, IRCoT (Interleaving Retrieval CoT; Trivedi et al. 2022) and ReAct (Reason + Act; Yao et al. 2023) combines iterative CoT prompting with queries to Wikipedia APIs to search for relevant entities and content and then add it back into the context.\n\n\n\nFig. 1. How Self-Ask works with external search queries.(Image source: Press et al. 2022).\n\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, essentially creating a tree structure. The search process can be BFS or DFS while each state is evaluated by a classifier (via a prompt) or majority vote.\n\n\nFig. 2. How Self-Ask works with external search queries.(Image source: Yao et al. 2022).\nAutomatic Prompt Design#\nPrompt is a sequence of prefix tokens that increase the probability of getting  desired output given input. Therefore we can treat them as trainable parameters and optimize them directly on the embedding space via gradient descent, such as AutoPrompt (Shin et al., 2020, Prefix-Tuning (Li & Liang (2021)), P-tuning (Liu et al. 2021) and Prompt-Tuning (Lester et al. 2021). This section in my \u201cControllable Neural Text Generation\u201d post has a good coverage of them. The trend from AutoPrompt to Prompt-Tuning is that the setup gets gradually simplified.\nAPE (Automatic Prompt Engineer; Zhou et al. 2022) is a method to search over a pool of model-generated instruction candidates and then filters the candidate set according to a chosen score function to ultimately choose the best candidate with highest score.\n\n\nPrompt LLM to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. E.g. {{Given desired input-output pairs}}\\n\\nThe instruction is.\n\n\nGiven a dataset of $\\mathcal{D}_\\text{train} = \\{(x, y)\\}$, we would like to find an instruction $\\rho$ such that $\\rho^* = \\arg\\max_\\rho \\mathbb{E}_{(x, y) \\in \\mathcal{D}_\\text{train}} [f(\\rho, x, y)]$, where $f(.)$ is a per-sample score function, such as execution accuracy $\\mathbb{1}[\\text{LM}(.\\vert \\rho, x)=y]$ or log probability: $p_\\text{LM}(y \\mid",
    "chunk_id": "e559704eff72a598c60098aa74f5088c",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "bf45511ad2976ddc2305d165c60375e3",
    "chunk": " $\\rho$ such that $\\rho^* = \\arg\\max_\\rho \\mathbb{E}_{(x, y) \\in \\mathcal{D}_\\text{train}} [f(\\rho, x, y)]$, where $f(.)$ is a per-sample score function, such as execution accuracy $\\mathbb{1}[\\text{LM}(.\\vert \\rho, x)=y]$ or log probability: $p_\\text{LM}(y \\mid \\rho, x)$.\n\n\nUse an iterative Monte Carlo search method to improve the best candidates by proposing semantically similar variants via prompts like Generate a variation of the following instruction while keeping the semantic meaning.\\n\\nInput: ...\\n\\nOutput:...\n\n\nTo construct chain-of-thought prompts automatically, Shum et al. (2023) suggested augment-prune-select, a three-step process:\n\nAugment: Generate multiple pseudo-chains of thought given question using few-shot or zero-shot CoT prompts;\nPrune: Prune pseudo chains based on whether generated answers match ground truths.\nSelect: Apply a variance-reduced policy gradient strategy to learn the probability distribution over selected examples, while considering the probability distribution over examples as policy and the validation set accuracy as reward.\n\nZhang et al. (2023) instead adopted clustering techniques to sample questions and then generates chains. They observed that LLMs tend to make certain types of mistakes. One type of errors can be similar in the emebedding space and thus get grouped together. By only sampling one or a few from frequent-error clusters, we can prevent too many wrong demonstrations of one error type and collect a diverse set of examples.\n\nQuestion clustering: Embed questions and run $k$-means for clustering.\nDemonstration selection: Select a set of representative questions from each cluster; i.e. one demonstration from one cluster. Samples in each cluster are sorted by distance to the cluster centroid and those closer to the centroid are selected first.\nRationale generation: Use zero-shot CoT to generate reasoning chains for selected questions and construct few-shot prompt to run inference.\n\nAugmented Language Models#\nA survey on augmented language models by Mialon et al. (2023) has great coverage over multiple categories of language models augmented with reasoning skills and the ability of using external tools. Recommend it.\nRetrieval#\nOften we need to complete tasks that require latest knowledge after the model pretraining time cutoff or internal\/private knowledge base. In that case, the model would not know the context if we don\u2019t explicitly provide it in the prompt. Many methods for Open Domain Question Answering depend on first doing retrieval over a knowledge base and then incorporating the retrieved content as part of the prompt. The accuracy of such a process depends on the quality of both retrieval and generation steps.\nLazaridou et al. (2022) studied how to use Google Search for document retrieval to augment LLMs. Given a question $q$, clean text is",
    "chunk_id": "bf45511ad2976ddc2305d165c60375e3",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "cb53f7fd6843147be82ab418a3886892",
    "chunk": ", the model would not know the context if we don\u2019t explicitly provide it in the prompt. Many methods for Open Domain Question Answering depend on first doing retrieval over a knowledge base and then incorporating the retrieved content as part of the prompt. The accuracy of such a process depends on the quality of both retrieval and generation steps.\nLazaridou et al. (2022) studied how to use Google Search for document retrieval to augment LLMs. Given a question $q$, clean text is extracted out of 20 URLs returned by Google, resulting in a set of documents. Because these documents are long, each document is split into paragraphs of 6 sentences, $\\{p\\}$. Paragraphs are ranked by TF-IDF based cosine similarity between evidence paragraphs and the query. Only the most relevant paragraph is used in the prompt to produce an answer $a$.\nFor closed-book QA, each demonstration is formatted as follows to construct few-shot prompts. Swapping the question with the evidence (longer distance between questions and answers) is found to consistently yield lower results across all datasets.\nEvidence: ...\nQuestion: ...\nAnswer: ...\nThe answer probability is computed in three ways:\n\nRAG style, $p(a_i \\mid q) = \\sum_{i=1}^n p_\\text{tf-idf} (p_i \\mid q) \\cdot p_\\text{LM}(a_i \\mid q, p_i)$, where $p_\\text{tf-idf} (p_i \\mid q)$ is the normalized cosine similarities between the TF-IDF passage and question representations.\nNoisy channel inference, $p(a_i\\mid q) = \\frac{p_\\text{LM}(q \\mid a_i, p_i) \\cdot p_\\text{LM}(a_i \\mid p_i)}{p_\\text{LM}(q \\mid p_i)}$\nProduct-of-Experts (PoE), combines all probabilities used above in addition to $p_\\text{LM}(p_i \\mid q)$.\n\nAccording to their experiments on generation and classification tasks, among three answer reranking scores - PoE > Noisy channel > RAG. Among individual probabilities, $p_\\text{LM}(a \\mid q, p_i)$ and $p_\\text{LM}(q \\mid p_i, a)$ are found to be most informative. $p_\\text{LM}(q \\mid p_i, a)$ captures how well the question can be explained by LM given evidence paragraph and answer and can reliably be used for reranking answer candidates.\nOne observation with SituatedQA dataset for questions grounded in different dates is that despite LM (pretraining cutoff is year 2020) has access to latest information via Google Search, its performance on post-2020 questions are still a lot worse than on pre-2020 questions. This suggests the existence of some discrepencies or conflicting parametric between contextual information and",
    "chunk_id": "cb53f7fd6843147be82ab418a3886892",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "0a408c9dd7328713d57c536ae35e5c47",
    "chunk": " captures how well the question can be explained by LM given evidence paragraph and answer and can reliably be used for reranking answer candidates.\nOne observation with SituatedQA dataset for questions grounded in different dates is that despite LM (pretraining cutoff is year 2020) has access to latest information via Google Search, its performance on post-2020 questions are still a lot worse than on pre-2020 questions. This suggests the existence of some discrepencies or conflicting parametric between contextual information and model internal knowledge.\nInterestingly it is found to be beneficial even with only \u201cinternal retrieval\u201d, that is, to generate knowledge about a topic before answering the question (Liu et al. 2022). First we can use  the following template to extract knowledge:\nGenerate some knowledge about the input. Examples:\n\nInput: What type of water formation is formed by clouds?\nKnowledge: Clouds are made of water vapor.\n\nInput: {question}\nKnowledge:\nAnd then with model-generated knowledge, prompt the LM further to get the answer.\nProgramming Language#\nBoth PAL (Program-aided language models); Gao et al. 2022) and PoT (Program of Thoughts prompting; Chen et al. 2022) ask LLM to generate programming language statements to resolve natural language reasoning problems, hence offloading the solution step to a runtime such as a Python interpreter. Such setup decouples complex computation and reasoning. It relies on a LM with good enough coding skills.\n\nFig. 3. Comparing CoT and PoT. (Image source: Chen et al. 2022).\nExternal APIs#\nTALM (Tool Augmented Language Models; Parisi et al. 2022) is a language model augmented with text-to-text API calls. LM is guided to generate |tool-call and tool input text conditioned on task input text to construct API call requests. When |result shows up, the specified tool API is called and the returned result gets appended to the text sequence. The final output is generated following |output token.\n\nFig. 4. The format of API calls in TALM. (Image source: Parisi et al. 2022).\nTALM adopts a self-play approach to iteratively bootstrap the dataset of tool use examples and finetune LM with it. This self-play, defined as a model interacting with a tool API, iteratively expands the dataset based on whether a newly added tool API can improve the model outputs. Same idea is adopted in Toolformer too, described in more details below. The pipeline loosely mimics a RL process where LM is the policy network and it is trained by policy gradient with a binary reward signal.\n\nFig. 5. Self-play iterations help boost the model performance.(Image source: Parisi et al. 2022).\nToolformer (Schick et al. 2023) is a LM that can use external tools via simple APIs, which is built in a self-supervised manner and",
    "chunk_id": "0a408c9dd7328713d57c536ae35e5c47",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "df74f5b4317acd824f65aa48ac6dec6c",
    "chunk": " is adopted in Toolformer too, described in more details below. The pipeline loosely mimics a RL process where LM is the policy network and it is trained by policy gradient with a binary reward signal.\n\nFig. 5. Self-play iterations help boost the model performance.(Image source: Parisi et al. 2022).\nToolformer (Schick et al. 2023) is a LM that can use external tools via simple APIs, which is built in a self-supervised manner and only requires a handful of demonstrations for each API. The toolbox of Toolformer includes:\n\nCalculator to help LM with the lack of precise math skills;\nQ&A system to help with unfaithful content and hallucination;\nSearch engine to provide up-to-date information after pretraining cut off time;\nTranslation system to improve performance on low resource language;\nCalendar to make LM be aware of time progression.\n\n\nFig. 6. Illustration of how to build Toolformer.(Image source: Schick et al. 2023).\nToolformer is trained as follows:\n\n\nPrompting to annotate potential API calls. Ask a pre-trained LM to annotate a dataset via few-shot learning with API call usage examples. Formatting example:\n\nFig. 7. How dataset is annotated to do API calls.(Image source: Schick et al. 2023).\n\n\nEach API call is represented as a tuple of (API name, corresponding input), $c=(a_c, i_c)$ and its corresponding result is denoted as $r$. The API call sequences with and without results are labeled as follows, respectively:\n\n  $$\n  \\begin{aligned}\n  e(c) &= \\langle\\texttt{API}\\rangle a_c(i_c) \\langle\\texttt{\/API}\\rangle \\\\\n  e(c, r) &= \\langle\\texttt{API}\\rangle a_c(i_c) \\to r \\langle\\texttt{\/API}\\rangle\n  \\end{aligned}\n  $$\n  \n\n\nSample API calls based on the probabilities $p_\\text{LM}(\\langle\\texttt{API}\\rangle \\mid \\text{prompt}(\\mathbf{x}), \\mathbf{x}_{1:i})$ and select top $k$ candidate positions for doing API calls at position $i$ if the probability is larger than a threshold.\n\n\nThen we sample potential API calls from the LM given the sequence $[\\text{prompt}(\\mathbf{x}), x_1, \\dots, x_{i-1}, \\langle\\texttt{API}\\rangle]$ as prefix and $\\langle\\texttt{\/API}\\rangle$ as suffix.\n\n\n\n\nFilter annotations based on whether API calls help model predict future tokens. Use a self-supervised loss to decide which API calls are actually helpful.\n\n\nExecute each API call $c_i$ to get corresponding result $r_i$.\n\n\nCompute weighted cross entropy loss for the LM over tokens $x_i, \\dots, x_n$ when the model",
    "chunk_id": "df74f5b4317acd824f65aa48ac6dec6c",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "0018ca45eb04fe86a428cc09b712e175",
    "chunk": " x_{i-1}, \\langle\\texttt{API}\\rangle]$ as prefix and $\\langle\\texttt{\/API}\\rangle$ as suffix.\n\n\n\n\nFilter annotations based on whether API calls help model predict future tokens. Use a self-supervised loss to decide which API calls are actually helpful.\n\n\nExecute each API call $c_i$ to get corresponding result $r_i$.\n\n\nCompute weighted cross entropy loss for the LM over tokens $x_i, \\dots, x_n$ when the model is prefixed with the prompt. Two versions are computed, one with API result and the other with empty sequence $\\varepsilon$.\n\n  $$\n  \\begin{aligned}\n  L^+_i &= L_i(e(c_i, r_i)) \\\\\n  L^-_i &= \\min(L_i(\\varepsilon), L_i(e(c_i, \\varepsilon))) \\\\\n  \\end{aligned}\n  $$\n  \nOnly API calls with $L^-_i - L^+_i$ larger than a threshold are kept, meaning that adding this API call and its results help the model predict future tokens.\n\n\n\n\nFine-tune LM on this annotated dataset. The new training sequences are constructed as $\\mathbf{x}^* = x_{1:i-1}, e(c_i, r_i), x_{i:n}$ . The training data is a combination of the original dataset (e.g. a subset of CCNet, as in the paper) and its augmented version.\n\n\nAt inference time, decoding runs until the model produces \u201c$\\to$ \" token, indicating that it is expecting response from an API call next.\nToolformer currently does not support tool use in a chain (i.e. using the output of one tool as an input for another tool) or in an interactive way (i.e. adopt API response after human selection). Both are interesting future directions to expand the model for.\nCitation#\nCited as:\n\nWeng, Lilian. (Mar 2023). Prompt Engineering. Lil\u2019Log. https:\/\/lilianweng.github.io\/posts\/2023-03-15-prompt-engineering\/.\n\nOr\n@article{weng2023prompt,\n  title   = \"Prompt Engineering\",\n  author  = \"Weng, Lilian\",\n  journal = \"lilianweng.github.io\",\n  year    = \"2023\",\n  month   = \"Mar\",\n  url     = \"https:\/\/lilianweng.github.io\/posts\/2023-03-15-prompt-engineering\/\"\n}\nUseful Resources#\n\nOpenAI Cookbook has many in-depth examples for how to utilize LLM efficiently.\nLangChain, a library for combining language models with other components to build applications.\nPrompt Engineering Guide repo contains a pretty comprehensive collection of education materials on prompt engineering.\nlearnprompting.org\nPromptPerfect\nSemantic Kernel\n\nReferences#\n[1] Zhao et al. \u201cCalibrate Before Use: Improving Few-shot Performance of",
    "chunk_id": "0018ca45eb04fe86a428cc09b712e175",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "cbf63ca5de3a188fdfae1db1de81dcb4",
    "chunk": ".io\/posts\/2023-03-15-prompt-engineering\/\"\n}\nUseful Resources#\n\nOpenAI Cookbook has many in-depth examples for how to utilize LLM efficiently.\nLangChain, a library for combining language models with other components to build applications.\nPrompt Engineering Guide repo contains a pretty comprehensive collection of education materials on prompt engineering.\nlearnprompting.org\nPromptPerfect\nSemantic Kernel\n\nReferences#\n[1] Zhao et al. \u201cCalibrate Before Use: Improving Few-shot Performance of Language Models.\u201d ICML 2021\n[2] Liu et al. \u201cWhat Makes Good In-Context Examples for GPT-3?\u201d arXiv preprint arXiv:2101.06804 (2021).\n[3] Lu et al. \u201cFantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity.\u201d ACL 2022\n[4] Ye et al. \u201cIn-Context Instruction Learning.\u201d arXiv preprint arXiv:2302.14691 (2023).\n[5] Su et al. \u201cSelective annotation makes language models better few-shot learners.\u201d arXiv preprint arXiv:2209.01975 (2022).\n[6] Rubin et al. \u201cLearning to retrieve prompts for in-context learning.\u201d NAACL-HLT 2022\n[7] Wei et al. \u201cChain of thought prompting elicits reasoning in large language models.\u201d NeurIPS 2022\n[8] Wang et al. \u201cSelf-Consistency Improves Chain of Thought Reasoning in Language Models.\u201d ICLR 2023.\n[9] Diao et al. \u201cActive Prompting with Chain-of-Thought for Large Language Models.\u201d arXiv preprint arXiv:2302.12246 (2023).\n[10] Zelikman et al. \u201cSTaR: Bootstrapping Reasoning With Reasoning.\u201d arXiv preprint arXiv:2203.14465 (2022).\n[11] Ye & Durrett. \u201cThe unreliability of explanations in few-shot in-context learning.\u201d arXiv preprint arXiv:2205.03401 (2022).\n[12] Trivedi et al. \u201cInterleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.\u201d arXiv preprint arXiv:2212.10509 (2022).\n[13] Press et al. \u201cMeasuring and narrowing the compositionality gap in language models.\u201d arXiv preprint arXiv:2210.03350 (2022).\n[14] Yao et al. \u201cReAct: Synergizing reasoning and acting in language models.\u201d ICLR 2023.\n[15] Fu et al. \u201cComplexity-based prompting for multi-step reasoning.\u201d arXiv preprint arXiv:2210.00720 (2022).\n[",
    "chunk_id": "cbf63ca5de3a188fdfae1db1de81dcb4",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "ec2a7e8c0f40062164805673b3735ef0",
    "chunk": "13] Press et al. \u201cMeasuring and narrowing the compositionality gap in language models.\u201d arXiv preprint arXiv:2210.03350 (2022).\n[14] Yao et al. \u201cReAct: Synergizing reasoning and acting in language models.\u201d ICLR 2023.\n[15] Fu et al. \u201cComplexity-based prompting for multi-step reasoning.\u201d arXiv preprint arXiv:2210.00720 (2022).\n[16] Wang et al. \u201cRationale-augmented ensembles in language models.\u201d arXiv preprint arXiv:2207.00747 (2022).\n[17] Zhang et al. \u201cAutomatic chain of thought prompting in large language models.\u201d arXiv preprint arXiv:2210.03493 (2022).\n[18] Shum et al. \u201cAutomatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data.\u201d arXiv preprint arXiv:2302.12822 (2023).\n[19] Zhou et al. \u201cLarge Language Models Are Human-Level Prompt Engineers.\u201d ICLR 2023.\n[20] Lazaridou et al. \u201cInternet augmented language models through few-shot prompting for open-domain question answering.\u201d arXiv preprint arXiv:2203.05115 (2022).\n[21] Chen et al. \u201cProgram of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks.\u201d arXiv preprint arXiv:2211.12588 (2022).\n[22] Gao et al. \u201cPAL: Program-aided language models.\u201d arXiv preprint arXiv:2211.10435 (2022).\n[23] Parisi et al. \u201cTALM: Tool Augmented Language Models\u201d arXiv preprint arXiv:2205.12255 (2022).\n[24] Schick et al. \u201cToolformer: Language Models Can Teach Themselves to Use Tools.\u201d arXiv preprint arXiv:2302.04761 (2023).\n[25] Mialon et al. \u201cAugmented Language Models: a Survey\u201d arXiv preprint arXiv:2302.07842 (2023).\n[26] Yao et al. \u201cTree of Thoughts: Deliberate Problem Solving with Large Language Models.\u201d arXiv preprint arXiv:2305.10601 (2023).\n\n\n\nnlp\nlanguage-model\nalignment\nsteerability\nprompting\n\n\n\n\u00ab \n\nLLM Powered Autonomous Agents\n\n\n \u00bb\n\nThe Transformer Family Version 2.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a9 2024 Lil'Log\n\n        Powered by\n        Hugo &\n        PaperMod\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "chunk_id": "ec2a7e8c0f40062164805673b3735ef0",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 571,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "66dc1c9cf334acf2ea1e9788d749e760",
    "chunk": " arXiv preprint arXiv:2305.10601 (2023).\n\n\n\nnlp\nlanguage-model\nalignment\nsteerability\nprompting\n\n\n\n\u00ab \n\nLLM Powered Autonomous Agents\n\n\n \u00bb\n\nThe Transformer Family Version 2.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a9 2024 Lil'Log\n\n        Powered by\n        Hugo &\n        PaperMod\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "chunk_id": "66dc1c9cf334acf2ea1e9788d749e760",
    "document_ids": [
        "4338fde7857098aa6e2cec3d233d1946"
    ],
    "n_tokens": 71,
    "entities": [
        {
            "name": "\"ORGANIZATION\"",
            "type": "\"ARXIV\"",
            "description": "\"arXiv is an organization that hosts preprints in the fields of physics, mathematics, computer science, and related disciplines.\"",
            "source_id": "66dc1c9cf334acf2ea1e9788d749e760"
        },
        {
            "name": "\"PERSON\"",
            "type": "\"AUTHOR\"",
            "description": "\"The author of the arXiv preprint arXiv:2305.10601 (2023) is not specified.\"",
            "source_id": "66dc1c9cf334acf2ea1e9788d749e760"
        },
        {
            "name": "\"GEO\"",
            "type": "\"LOCATION\"",
            "description": "\"No specific location is mentioned in relation to the arXiv preprint arXiv:2305.10601 (2023).\"",
            "source_id": "66dc1c9cf334acf2ea1e9788d749e760"
        },
        {
            "name": "\"EVENT\"",
            "type": "\"PREPRINT\"",
            "description": "\"The event of interest is the publication of a preprint on arXiv, specifically arXiv:2305.10601 (2023).\"",
            "source_id": "66dc1c9cf334acf2ea1e9788d749e760"
        }
    ],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"&quot;ORGANIZATION&quot;\">      <data key=\"d0\">\"ARXIV\"<\/data>      <data key=\"d1\">\"arXiv is an organization that hosts preprints in the fields of physics, mathematics, computer science, and related disciplines.\"<\/data>      <data key=\"d2\">66dc1c9cf334acf2ea1e9788d749e760<\/data>    <\/node>    <node id=\"&quot;PERSON&quot;\">      <data key=\"d0\">\"AUTHOR\"<\/data>      <data key=\"d1\">\"The author of the arXiv preprint arXiv:2305.10601 (2023) is not specified.\"<\/data>      <data key=\"d2\">66dc1c9cf334acf2ea1e9788d749e760<\/data>    <\/node>    <node id=\"&quot;GEO&quot;\">      <data key=\"d0\">\"LOCATION\"<\/data>      <data key=\"d1\">\"No specific location is mentioned in relation to the arXiv preprint arXiv:2305.10601 (2023).\"<\/data>      <data key=\"d2\">66dc1c9cf334acf2ea1e9788d749e760<\/data>    <\/node>    <node id=\"&quot;EVENT&quot;\">      <data key=\"d0\">\"PREPRINT\"<\/data>      <data key=\"d1\">\"The event of interest is the publication of a preprint on arXiv, specifically arXiv:2305.10601 (2023).\"<\/data>      <data key=\"d2\">66dc1c9cf334acf2ea1e9788d749e760<\/data>    <\/node>  <\/graph><\/graphml>"
}
{
    "id": "1c507763316720048a2b68e598bbfac3",
    "chunk": "Blog\n\nAttention? Attention! | Lil'Log\n\nLil'Log\n\n    \nDate: June 24, 2018  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\n\nTable of Contents\n\n\n\nWhat\u2019s Wrong with Seq2Seq Model?\n\nBorn for Translation\n\nDefinition\n\n\nA Family of Attention Mechanisms\n\nSummary\n\nSelf-Attention\n\nSoft vs Hard Attention\n\nGlobal vs Local Attention\n\n\nNeural Turing Machines\n\nReading and Writing\n\nAttention Mechanisms\n\n\nPointer Network\n\nTransformer\n\nKey, Value and Query\n\nMulti-Head Self-Attention\n\nEncoder\n\nDecoder\n\nFull Architecture\n\n\nSNAIL\n\nSelf-Attention GAN\n\nReferences\n\n\n\n\n\n\n[Updated on 2018-10-28: Add Pointer Network and the link to my implementation of Transformer.]\n[Updated on 2018-11-06: Add a link to the implementation of Transformer model.]\n[Updated on 2018-11-18: Add Neural Turing Machines.]\n[Updated on 2019-07-18: Correct the mistake on using the term \u201cself-attention\u201d when introducing the show-attention-tell paper; moved it to Self-Attention section.]\n[Updated on 2020-04-07: A follow-up post on improved Transformer models is here.]\nAttention is, to some extent, motivated by how we pay visual attention to different regions of an image or correlate words in one sentence. Take the picture of a Shiba Inu in Fig. 1 as an example.\n\nFig. 1. A Shiba Inu in a men\u2019s outfit. The credit of the original photo goes to Instagram @mensweardog.\nHuman visual attention allows us to focus on a certain region with \u201chigh resolution\u201d (i.e. look at the pointy ear in the yellow box) while perceiving the surrounding image in \u201clow resolution\u201d (i.e. now how about the snowy background and the outfit?), and then adjust the focal point or do the inference accordingly. Given a small patch of an image, pixels in the rest provide clues what should be displayed there. We expect to see a pointy ear in the yellow box because we have seen a dog\u2019s nose, another pointy ear on the right, and Shiba\u2019s mystery eyes (stuff in the red boxes). However, the sweater and blanket at the bottom would not be as helpful as those doggy features.\nSimilarly, we can explain the relationship between words in one sentence or close context. When we see \u201ceating\u201d, we expect to encounter a food word very soon. The color term describes the food, but probably not so much with \u201ceating\u201d directly.\n\nFig. 2. One word \"attends\" to other words in the same sentence differently.\nIn a nutshell, attention in deep learning can be broadly interpreted as a vector of importance weights: in order to predict or infer one element, such as a pixel in an image or a word in a",
    "chunk_id": "1c507763316720048a2b68e598bbfac3",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "117d07084be79471f3c3e709340d0bd7",
    "chunk": " one sentence or close context. When we see \u201ceating\u201d, we expect to encounter a food word very soon. The color term describes the food, but probably not so much with \u201ceating\u201d directly.\n\nFig. 2. One word \"attends\" to other words in the same sentence differently.\nIn a nutshell, attention in deep learning can be broadly interpreted as a vector of importance weights: in order to predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate using the attention vector how strongly it is correlated with (or \u201cattends to\u201d as you may have read in many papers) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.\nWhat\u2019s Wrong with Seq2Seq Model?#\nThe seq2seq model was born in the field of language modeling (Sutskever, et al. 2014). Broadly speaking, it aims to transform an input sequence (source) to a new one (target) and both sequences can be of arbitrary lengths. Examples of transformation tasks include machine translation between multiple languages in either text or audio, question-answer dialog generation, or even parsing sentences into grammar trees.\nThe seq2seq model normally has an encoder-decoder architecture, composed of:\n\nAn encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding or \u201cthought\u201d vector) of a fixed length. This representation is expected to be a good summary of the meaning of the whole source sequence.\nA decoder is initialized with the context vector to emit the transformed output. The early work only used the last state of the encoder network as the decoder initial state.\n\nBoth the encoder and decoder are recurrent neural networks, i.e. using LSTM or GRU units.\n\nFig. 3. The encoder-decoder model, translating the sentence \"she is eating a green apple\" to Chinese. The visualization of both encoder and decoder is unrolled in time.\nA critical and apparent disadvantage of this fixed-length context vector design is incapability of remembering long sentences. Often it has forgotten the first part once it completes processing the whole input. The attention mechanism was born (Bahdanau et al., 2015) to resolve this problem.\nBorn for Translation#\nThe attention mechanism was born to help memorize long source sentences in neural machine translation (NMT). Rather than building a single context vector out of the encoder\u2019s last hidden state, the secret sauce invented by attention is to create shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element.\nWhile the context vector has access to the entire input sequence, we don\u2019t need to worry about forgetting. The alignment between the source and target is learned and controlled by the context vector. Essentially the context vector consumes three pieces of information:\n\nencoder hidden states;\ndecoder hidden states;\nalignment between source and target.\n\n\nFig. 4. The encoder-de",
    "chunk_id": "117d07084be79471f3c3e709340d0bd7",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "ced47e3d35326d24f631ff458a11e04d",
    "chunk": " sauce invented by attention is to create shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element.\nWhile the context vector has access to the entire input sequence, we don\u2019t need to worry about forgetting. The alignment between the source and target is learned and controlled by the context vector. Essentially the context vector consumes three pieces of information:\n\nencoder hidden states;\ndecoder hidden states;\nalignment between source and target.\n\n\nFig. 4. The encoder-decoder model with additive attention mechanism in Bahdanau et al., 2015.\nDefinition#\nNow let\u2019s define the attention mechanism introduced in NMT in a scientific way. Say, we have a source sequence $\\mathbf{x}$ of length $n$ and try to output a target sequence $\\mathbf{y}$ of length $m$:\n\n$$\n\\begin{aligned}\n\\mathbf{x} &= [x_1, x_2, \\dots, x_n] \\\\\n\\mathbf{y} &= [y_1, y_2, \\dots, y_m]\n\\end{aligned}\n$$\n\n(Variables in bold indicate that they are vectors; same for everything else in this post.)\nThe encoder is a bidirectional RNN (or other recurrent network setting of your choice) with a forward hidden state $\\overrightarrow{\\boldsymbol{h}}_i$ and a backward one $\\overleftarrow{\\boldsymbol{h}}_i$. A simple concatenation of two represents the encoder state. The motivation is to include both the preceding and following words in the annotation of one word.\n\n$$\n\\boldsymbol{h}_i = [\\overrightarrow{\\boldsymbol{h}}_i^\\top; \\overleftarrow{\\boldsymbol{h}}_i^\\top]^\\top, i=1,\\dots,n\n$$\n\nThe decoder network has hidden state $\\boldsymbol{s}_t=f(\\boldsymbol{s}_{t-1}, y_{t-1}, \\mathbf{c}_t)$ for the output word at position t, $t=1,\\dots,m$, where the context vector $\\mathbf{c}_t$ is a sum of hidden states of the input sequence, weighted by alignment scores:\n\n$$\n\\begin{aligned}\n\\mathbf{c}_t &= \\sum_{i=1}^n \\alpha_{t,i} \\boldsymbol{h}_i & \\small{\\text{; Context vector for output }y_t}\\\\\n\\alpha_{t,i} &= \\text{align}(y_t, x_i) & \\small{\\text{; How well two words }y_t\\text{ and }x_i\\text{ are aligned.}}\\\\\n&= \\frac{\\exp(\\text{score}(\\boldsymbol{s}_{t-1}, \\boldsymbol{h}_i))}{\\sum_{i'=1}^n \\exp",
    "chunk_id": "ced47e3d35326d24f631ff458a11e04d",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "48887d6cc13a549361f87277ee1c82ab",
    "chunk": "text{; Context vector for output }y_t}\\\\\n\\alpha_{t,i} &= \\text{align}(y_t, x_i) & \\small{\\text{; How well two words }y_t\\text{ and }x_i\\text{ are aligned.}}\\\\\n&= \\frac{\\exp(\\text{score}(\\boldsymbol{s}_{t-1}, \\boldsymbol{h}_i))}{\\sum_{i'=1}^n \\exp(\\text{score}(\\boldsymbol{s}_{t-1}, \\boldsymbol{h}_{i'}))} & \\small{\\text{; Softmax of some predefined alignment score.}}.\n\\end{aligned}\n$$\n\nThe alignment model assigns a score $\\alpha_{t,i}$ to the pair of input at position i and output at position t, $(y_t, x_i)$, based on how well they match. The set of $\\{\\alpha_{t, i}\\}$ are weights defining how much of each source hidden state should be considered for each output. In Bahdanau\u2019s paper, the alignment score $\\alpha$ is parametrized by a feed-forward network with a single hidden layer and this network is jointly trained with other parts of the model. The score function is therefore in the following form, given that tanh is used as the non-linear activation function:\n\n$$\n\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a[\\boldsymbol{s}_t; \\boldsymbol{h}_i])\n$$\n\nwhere both $\\mathbf{v}_a$ and $\\mathbf{W}_a$ are weight matrices to be learned in the alignment model.\nThe matrix of alignment scores is a nice byproduct to explicitly show the correlation between source and target words.\n\nFig. 5. Alignment matrix of \"L'accord sur l'Espace \u00e9conomique europ\u00e9en a \u00e9t\u00e9 sign\u00e9 en ao\u00fbt 1992\" (French) and its English translation \"The agreement on the European Economic Area was signed in August 1992\". (Image source: Fig 3 in Bahdanau et al., 2015)\nCheck out this nice tutorial by Tensorflow team for more implementation instructions.\nA Family of Attention Mechanisms#\nWith the help of the attention, the dependencies between source and target sequences are not restricted by the in-between distance anymore! Given the big improvement by attention in machine translation, it soon got extended into the computer vision field (Xu et al. 2015) and people started exploring various other forms of attention mechanisms (Luong, et al., 2015; Britz et al., 2017; Vaswani, et al., 2017).\nSummary#\nBelow is a summary table of several popular attention mechanisms and corresponding alignment score functions:\n\n\n\nName\nAlignment score",
    "chunk_id": "48887d6cc13a549361f87277ee1c82ab",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "2da73aaaf304094f41e7641504f8fddc",
    "chunk": " restricted by the in-between distance anymore! Given the big improvement by attention in machine translation, it soon got extended into the computer vision field (Xu et al. 2015) and people started exploring various other forms of attention mechanisms (Luong, et al., 2015; Britz et al., 2017; Vaswani, et al., 2017).\nSummary#\nBelow is a summary table of several popular attention mechanisms and corresponding alignment score functions:\n\n\n\nName\nAlignment score function\nCitation\n\n\n\n\nContent-base attention\n$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\text{cosine}[\\boldsymbol{s}_t, \\boldsymbol{h}_i]$\nGraves2014\n\n\nAdditive(*)\n$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a[\\boldsymbol{s}_{t-1}; \\boldsymbol{h}_i])$\nBahdanau2015\n\n\nLocation-Base\n$\\alpha_{t,i} = \\text{softmax}(\\mathbf{W}_a \\boldsymbol{s}_t)$Note: This simplifies the softmax alignment to only depend on the target position.\nLuong2015\n\n\nGeneral\n$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\boldsymbol{s}_t^\\top\\mathbf{W}_a\\boldsymbol{h}_i$where $\\mathbf{W}_a$ is a trainable weight matrix in the attention layer.\nLuong2015\n\n\nDot-Product\n$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\boldsymbol{s}_t^\\top\\boldsymbol{h}_i$\nLuong2015\n\n\nScaled Dot-Product(^)\n$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\frac{\\boldsymbol{s}_t^\\top\\boldsymbol{h}_i}{\\sqrt{n}}$Note: very similar to the dot-product attention except for a scaling factor; where n is the dimension of the source hidden state.\nVaswani2017\n\n\n\n(*) Referred to as \u201cconcat\u201d in Luong, et al., 2015 and as \u201cadditive attention\u201d in Vaswani, et al., 2017.\n(^) It adds a scaling factor $1\/\\sqrt{n}$, motivated by the concern when the input is large, the softmax function may have an extremely small gradient, hard for efficient learning.\nHere are a summary of broader categories of attention mechanisms:\n\n\n\nName\nDefinition\nCitation\n\n\n\n\nSelf-Attention(&)\nRelating different positions of the same input sequence. Theoretically the self-attention can adopt any score functions above, but just replace",
    "chunk_id": "2da73aaaf304094f41e7641504f8fddc",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "8c20602c3abc407b783bc24716fda2db",
    "chunk": " Vaswani, et al., 2017.\n(^) It adds a scaling factor $1\/\\sqrt{n}$, motivated by the concern when the input is large, the softmax function may have an extremely small gradient, hard for efficient learning.\nHere are a summary of broader categories of attention mechanisms:\n\n\n\nName\nDefinition\nCitation\n\n\n\n\nSelf-Attention(&)\nRelating different positions of the same input sequence. Theoretically the self-attention can adopt any score functions above, but just replace the target sequence with the same input sequence.\nCheng2016\n\n\nGlobal\/Soft\nAttending to the entire input state space.\nXu2015\n\n\nLocal\/Hard\nAttending to the part of input state space; i.e. a patch of the input image.\nXu2015; Luong2015\n\n\n\n(&) Also, referred to as \u201cintra-attention\u201d in Cheng et al., 2016 and some other papers.\nSelf-Attention#\nSelf-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.\nThe long short-term memory network paper used self-attention to do machine reading. In the example below, the self-attention mechanism enables us to learn the correlation between the current words and the previous part of the sentence.\n\nFig. 6. The current word is in red and the size of the blue shade indicates the activation level. (Image source: Cheng et al., 2016)\nSoft vs Hard Attention#\nIn the show, attend and tell paper, attention mechanism is applied to images to generate captions. The image is first encoded by a CNN to extract features. Then a LSTM decoder consumes the convolution features to produce descriptive words one by one, where the weights are learned through attention. The visualization of the attention weights clearly demonstrates which regions of the image the model is paying attention to so as to output a certain word.\n\nFig. 7. \"A woman is throwing a frisbee in a park.\" (Image source: Fig. 6(b) in Xu et al. 2015)\nThis paper first proposed the distinction between \u201csoft\u201d vs \u201chard\u201d attention, based on whether the attention has access to the entire image or only a patch:\n\nSoft Attention: the alignment weights are learned and placed \u201csoftly\u201d over all patches in the source image; essentially the same type of attention as in Bahdanau et al., 2015.\n\nPro: the model is smooth and differentiable.\nCon: expensive when the source input is large.\n\n\nHard Attention: only selects one patch of the image to attend to at a time.\n\nPro: less calculation at the inference time.\nCon: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (Luong",
    "chunk_id": "8c20602c3abc407b783bc24716fda2db",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "09091150bcffda82d722fcc6ac324ee6",
    "chunk": "\u201d over all patches in the source image; essentially the same type of attention as in Bahdanau et al., 2015.\n\nPro: the model is smooth and differentiable.\nCon: expensive when the source input is large.\n\n\nHard Attention: only selects one patch of the image to attend to at a time.\n\nPro: less calculation at the inference time.\nCon: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (Luong, et al., 2015)\n\n\n\nGlobal vs Local Attention#\nLuong, et al., 2015 proposed the \u201cglobal\u201d and \u201clocal\u201d attention. The global attention is similar to the soft attention, while the local one is an interesting blend between hard and soft, an improvement over the hard attention to make it differentiable: the model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector.\n\nFig. 8. Global vs local attention (Image source: Fig 2 & 3 in Luong, et al., 2015)\nNeural Turing Machines#\nAlan Turing in 1936 proposed a minimalistic model of computation. It is composed of a infinitely long tape and a head to interact with the tape. The tape has countless cells on it, each filled with a symbol: 0, 1 or blank (\" \u201c). The operation head can read symbols, edit symbols and move left\/right on the tape. Theoretically a Turing machine can simulate any computer algorithm, irrespective of how complex or expensive the procedure might be. The infinite memory gives a Turing machine an edge to be mathematically limitless. However, infinite memory is not feasible in real modern computers and then we only consider Turing machine as a mathematical model of computation.\n\nFig. 9. How a Turing machine looks like: a tape + a head that handles the tape. (Image source: http:\/\/aturingmachine.com\/)\nNeural Turing Machine (NTM, Graves, Wayne & Danihelka, 2014) is a model architecture for coupling a neural network with external memory storage. The memory mimics the Turing machine tape and the neural network controls the operation heads to read from or write to the tape. However, the memory in NTM is finite, and thus it probably looks more like a \u201cNeural von Neumann Machine\u201d.\nNTM contains two major components, a controller neural network and a memory bank.\nController: is in charge of executing operations on the memory. It can be any type of neural network, feed-forward or recurrent.\nMemory: stores processed information. It is a matrix of size $N \\times M$, containing N vector rows and each has $M$ dimensions.\nIn one update iteration, the controller processes the input and interacts with the memory bank accordingly to generate output. The interaction is handled by a set of parallel read and write heads. Both read and write operations are",
    "chunk_id": "09091150bcffda82d722fcc6ac324ee6",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "3c1e785b314e1e8eb32b20d82cafecb4",
    "chunk": " memory bank.\nController: is in charge of executing operations on the memory. It can be any type of neural network, feed-forward or recurrent.\nMemory: stores processed information. It is a matrix of size $N \\times M$, containing N vector rows and each has $M$ dimensions.\nIn one update iteration, the controller processes the input and interacts with the memory bank accordingly to generate output. The interaction is handled by a set of parallel read and write heads. Both read and write operations are \u201cblurry\u201d by softly attending to all the memory addresses.\n\nFig 10. Neural Turing Machine Architecture.\nReading and Writing#\nWhen reading from the memory at time t, an attention vector of size $N$, $\\mathbf{w}_t$ controls how much attention to assign to different memory locations (matrix rows). The read vector $\\mathbf{r}_t$ is a sum weighted by attention intensity:\n\n$$\n\\mathbf{r}_t = \\sum_{i=1}^N w_t(i)\\mathbf{M}_t(i)\\text{, where }\\sum_{i=1}^N w_t(i)=1, \\forall i: 0 \\leq w_t(i) \\leq 1\n$$\n\nwhere $w_t(i)$ is the $i$-th element in $\\mathbf{w}_t$ and $\\mathbf{M}_t(i)$ is the $i$-th row vector in the memory.\nWhen writing into the memory at time t, as inspired by the input and forget gates in LSTM, a write head first wipes off some old content according to an erase vector $\\mathbf{e}_t$ and then adds new information by an add vector $\\mathbf{a}_t$.\n\n$$\n\\begin{aligned}\n\\tilde{\\mathbf{M}}_t(i) &= \\mathbf{M}_{t-1}(i) [\\mathbf{1} - w_t(i)\\mathbf{e}_t] &\\scriptstyle{\\text{; erase}}\\\\\n\\mathbf{M}_t(i) &= \\tilde{\\mathbf{M}}_t(i) + w_t(i) \\mathbf{a}_t &\\scriptstyle{\\text{; add}}\n\\end{aligned}\n$$\n\nAttention Mechanisms#\nIn Neural Turing Machine, how to generate the attention distribution $\\mathbf{w}_t$ depends on the addressing mechanisms: NTM uses a mixture of content-based and location-based addressings.\nContent-based addressing\nThe content-addressing creates attention vectors based on the similarity between the key vector $\\mathbf{k}_t$ extracted by the controller from the input and memory rows. The content-based attention scores are computed as cosine similarity and then normalized by softmax. In addition, NTM adds a strength multiplier $\\beta_t$ to amplify or attenuate the focus of the distribution.\n\n$$\nw_t",
    "chunk_id": "3c1e785b314e1e8eb32b20d82cafecb4",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "842c01fa36de36db4769f0fd81c71b21",
    "chunk": " on the addressing mechanisms: NTM uses a mixture of content-based and location-based addressings.\nContent-based addressing\nThe content-addressing creates attention vectors based on the similarity between the key vector $\\mathbf{k}_t$ extracted by the controller from the input and memory rows. The content-based attention scores are computed as cosine similarity and then normalized by softmax. In addition, NTM adds a strength multiplier $\\beta_t$ to amplify or attenuate the focus of the distribution.\n\n$$\nw_t^c(i) \n= \\text{softmax}(\\beta_t \\cdot \\text{cosine}[\\mathbf{k}_t, \\mathbf{M}_t(i)])\n= \\frac{\\exp(\\beta_t \\frac{\\mathbf{k}_t \\cdot \\mathbf{M}_t(i)}{\\|\\mathbf{k}_t\\| \\cdot \\|\\mathbf{M}_t(i)\\|})}{\\sum_{j=1}^N \\exp(\\beta_t \\frac{\\mathbf{k}_t \\cdot \\mathbf{M}_t(j)}{\\|\\mathbf{k}_t\\| \\cdot \\|\\mathbf{M}_t(j)\\|})}\n$$\n\nInterpolation\nThen an interpolation gate scalar $g_t$ is used to blend the newly generated content-based attention vector with the attention weights in the last time step:\n\n$$\n\\mathbf{w}_t^g = g_t \\mathbf{w}_t^c + (1 - g_t) \\mathbf{w}_{t-1} \n$$\n\nLocation-based addressing\nThe location-based addressing sums up the values at different positions in the attention vector, weighted by a weighting distribution over allowable integer shifts. It is equivalent to a 1-d convolution with a kernel $\\mathbf{s}_t(.)$, a function of the position offset. There are multiple ways to define this distribution. See Fig. 11. for inspiration.\n\nFig. 11. Two ways to represent the shift weighting distribution $\\mathbf{s}\\_t$.\nFinally the attention distribution is enhanced by a sharpening scalar $\\gamma_t \\geq 1$.\n\n$$\n\\begin{aligned}\n\\tilde{w}_t(i) &= \\sum_{j=1}^N w_t^g(j) s_t(i-j) & \\scriptstyle{\\text{; circular convolution}}\\\\\nw_t(i) &= \\frac{\\tilde{w}_t(i)^{\\gamma_t}}{\\sum_{j=1}^N \\tilde{w}_t(j)^{\\gamma_t}} & \\scriptstyle{\\text{; sharpen}}\n\\end{aligned}\n$$\n\nThe complete process of generating the attention vector $\\mathbf{w}_t$ at time step t is illustrated in Fig. 12. All the parameters produced by the controller are unique for each head. If there are multiple read and write heads in",
    "chunk_id": "842c01fa36de36db4769f0fd81c71b21",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "712eb5880ce06b7cafefb28435166b09",
    "chunk": "de{w}_t(i)^{\\gamma_t}}{\\sum_{j=1}^N \\tilde{w}_t(j)^{\\gamma_t}} & \\scriptstyle{\\text{; sharpen}}\n\\end{aligned}\n$$\n\nThe complete process of generating the attention vector $\\mathbf{w}_t$ at time step t is illustrated in Fig. 12. All the parameters produced by the controller are unique for each head. If there are multiple read and write heads in parallel, the controller would output multiple sets.\n\nFig. 12. Flow diagram of the addressing mechanisms in Neural Turing Machine. (Image source: Graves, Wayne & Danihelka, 2014)\nPointer Network#\nIn problems like sorting or travelling salesman, both input and output are sequential data. Unfortunately, they cannot be easily solved by classic seq-2-seq or NMT models, given that the discrete categories of output elements are not determined in advance, but depends on the variable input size. The Pointer Net (Ptr-Net; Vinyals, et al. 2015) is proposed to resolve this type of problems: When the output elements correspond to positions in an input sequence. Rather than using attention to blend hidden units of an encoder into a context vector (See Fig. 8), the Pointer Net applies attention over the input elements to pick one as the output at each decoder step.\n\nFig. 13. The architecture of a Pointer Network model. (Image source: Vinyals, et al. 2015)\nThe Ptr-Net outputs a sequence of integer indices, $\\boldsymbol{c} = (c_1, \\dots, c_m)$ given a sequence of input vectors $\\boldsymbol{x} = (x_1, \\dots, x_n)$ and $1 \\leq c_i \\leq n$. The model still embraces an encoder-decoder framework. The encoder and decoder hidden states are denoted as $(\\boldsymbol{h}_1, \\dots, \\boldsymbol{h}_n)$ and $(\\boldsymbol{s}_1, \\dots, \\boldsymbol{s}_m)$, respectively. Note that $\\mathbf{s}_i$ is the output gate after cell activation in the decoder. The Ptr-Net applies additive attention between states and then normalizes it by softmax to model the output conditional probability:\n\n$$\n\\begin{aligned}\ny_i &= p(c_i \\vert c_1, \\dots, c_{i-1}, \\boldsymbol{x}) \\\\\n    &= \\text{softmax}(\\text{score}(\\boldsymbol{s}_t; \\boldsymbol{h}_i)) = \\text{softmax}(\\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a[\\boldsymbol{s}_t; \\boldsymbol{h}_i]))\n\\end{aligned}\n$$\n\nThe attention mechanism is simplified, as Ptr-Net does",
    "chunk_id": "712eb5880ce06b7cafefb28435166b09",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "d2eea7a19812fac9f658d6f9fff87d95",
    "chunk": " \\dots, c_{i-1}, \\boldsymbol{x}) \\\\\n    &= \\text{softmax}(\\text{score}(\\boldsymbol{s}_t; \\boldsymbol{h}_i)) = \\text{softmax}(\\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a[\\boldsymbol{s}_t; \\boldsymbol{h}_i]))\n\\end{aligned}\n$$\n\nThe attention mechanism is simplified, as Ptr-Net does not blend the encoder states into the output with attention weights. In this way, the output only responds to the positions but not the input content.\nTransformer#\n\u201cAttention is All you Need\u201d\n(Vaswani, et al., 2017), without a doubt, is one of the most impactful and interesting paper in 2017. It presented a lot of improvements to the soft attention and make it possible to do seq2seq modeling without recurrent network units. The proposed \u201ctransformer\u201d model is entirely built on the self-attention mechanisms without using sequence-aligned recurrent architecture.\nThe secret recipe is carried in its model architecture.\nKey, Value and Query#\nThe major component in the transformer is the unit of multi-head self-attention mechanism. The transformer views the encoded representation of the input as a set of key-value pairs, $(\\mathbf{K}, \\mathbf{V})$, both of dimension $n$ (input sequence length); in the context of NMT, both the keys and values are the encoder hidden states. In the decoder, the previous output is compressed into a query ($\\mathbf{Q}$ of dimension $m$) and the next output is produced by mapping this query and the set of keys and values.\nThe transformer adopts the scaled dot-product attention: the output is a weighted sum of the values, where the weight assigned to each value is determined by the dot-product of the query with all the keys:\n\n$$\n\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{n}})\\mathbf{V}\n$$\n\nMulti-Head Self-Attention#\n\nFig. 14. Multi-head scaled dot-product attention mechanism. (Image source: Fig 2 in Vaswani, et al., 2017)\nRather than only computing the attention once, the multi-head mechanism runs through the scaled dot-product attention multiple times in parallel. The independent attention outputs are simply concatenated and linearly transformed into the expected dimensions. I assume the motivation is because ensembling always helps? ;) According to the paper, \u201cmulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\u201d\n\n$$\n\\begin{aligned}\n\\text{MultiHead}(\\mathbf{Q}, \\math",
    "chunk_id": "d2eea7a19812fac9f658d6f9fff87d95",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "b055fcfe607a7e6d4b8442df7d598f6e",
    "chunk": "-head mechanism runs through the scaled dot-product attention multiple times in parallel. The independent attention outputs are simply concatenated and linearly transformed into the expected dimensions. I assume the motivation is because ensembling always helps? ;) According to the paper, \u201cmulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\u201d\n\n$$\n\\begin{aligned}\n\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= [\\text{head}_1; \\dots; \\text{head}_h]\\mathbf{W}^O \\\\\n\\text{where head}_i &= \\text{Attention}(\\mathbf{Q}\\mathbf{W}^Q_i, \\mathbf{K}\\mathbf{W}^K_i, \\mathbf{V}\\mathbf{W}^V_i)\n\\end{aligned}\n$$\n\nwhere $\\mathbf{W}^Q_i$, $\\mathbf{W}^K_i$, $\\mathbf{W}^V_i$, and $\\mathbf{W}^O$ are parameter matrices to be learned.\nEncoder#\n\nFig. 15. The transformer\u2019s encoder. (Image source: Vaswani, et al., 2017)\nThe encoder generates an attention-based representation with capability to locate a specific piece of information from a potentially infinitely-large context.\n\nA stack of N=6 identical layers.\nEach layer has a multi-head self-attention layer and a simple position-wise fully connected feed-forward network.\nEach sub-layer adopts a residual connection and a layer normalization.\nAll the sub-layers output data of the same dimension $d_\\text{model} = 512$.\n\nDecoder#\n\nFig. 16. The transformer\u2019s decoder. (Image source: Vaswani, et al., 2017)\nThe decoder is able to retrieval from the encoded representation.\n\nA stack of N = 6 identical layers\nEach layer has two sub-layers of multi-head attention mechanisms and one sub-layer of fully-connected feed-forward network.\nSimilar to the encoder, each sub-layer adopts a residual connection and a layer normalization.\nThe first multi-head attention sub-layer is modified to prevent positions from attending to subsequent positions, as we don\u2019t want to look into the future of the target sequence when predicting the current position.\n\nFull Architecture#\nFinally here is the complete view of the transformer\u2019s architecture:\n\nBoth the source and target sequences first go through embedding layers to produce data of the same dimension $d_\\text{model} =512$.\nTo preserve the position information, a sinusoid-wave-based positional encoding is applied and summed with the embedding output.\nA softmax and linear layer are added to the final decoder output.\n\n\nFig. 17. The full model architecture of the transformer. (Image source: Fig 1 & 2 in Vaswani, et al.,",
    "chunk_id": "b055fcfe607a7e6d4b8442df7d598f6e",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "0257021b4dad429bebecabe24e76f06e",
    "chunk": " the transformer\u2019s architecture:\n\nBoth the source and target sequences first go through embedding layers to produce data of the same dimension $d_\\text{model} =512$.\nTo preserve the position information, a sinusoid-wave-based positional encoding is applied and summed with the embedding output.\nA softmax and linear layer are added to the final decoder output.\n\n\nFig. 17. The full model architecture of the transformer. (Image source: Fig 1 & 2 in Vaswani, et al., 2017.)\nTry to implement the transformer model is an interesting experience, here is mine: lilianweng\/transformer-tensorflow. Read the comments in the code if you are interested.\nSNAIL#\nThe transformer has no recurrent or convolutional structure, even with the positional encoding added to the embedding vector, the sequential order is only weakly incorporated. For problems sensitive to the positional dependency like reinforcement learning, this can be a big problem.\nThe Simple Neural Attention Meta-Learner (SNAIL) (Mishra et al., 2017) was developed partially to resolve the problem with positioning in the transformer model by combining the self-attention mechanism in transformer with temporal convolutions. It has been demonstrated to be good at both supervised learning and reinforcement learning tasks.\n\nFig. 18. SNAIL model architecture (Image source: Mishra et al., 2017)\nSNAIL was born in the field of meta-learning, which is another big topic worthy of a post by itself. But in simple words, the meta-learning model is expected to be generalizable to novel, unseen tasks in the similar distribution. Read this nice introduction if interested.\nSelf-Attention GAN#\nSelf-Attention GAN (SAGAN; Zhang et al., 2018) adds self-attention layers into GAN to enable both the generator and the discriminator to better model relationships between spatial regions.\nThe classic DCGAN (Deep Convolutional GAN) represents both discriminator and generator as multi-layer convolutional networks. However, the representation capacity of the network is restrained by the filter size, as the feature of one pixel is limited to a small local region. In order to connect regions far apart, the features have to be dilute through layers of convolutional operations and the dependencies are not guaranteed to be maintained.\nAs the (soft) self-attention in the vision context is designed to explicitly learn the relationship between one pixel and all other positions, even regions far apart, it can easily capture global dependencies. Hence GAN equipped with self-attention is expected to handle details better, hooray!\n\nFig. 19. Convolution operation and self-attention have access to regions of very different sizes.\nThe SAGAN adopts the non-local neural network to apply the attention computation. The convolutional image feature maps $\\mathbf{x}$ is branched out into three copies, corresponding to the concepts of key, value, and query in the transformer:\n\nKey: $",
    "chunk_id": "0257021b4dad429bebecabe24e76f06e",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "f8645fbf7cf181e83ed90908332852b4",
    "chunk": " it can easily capture global dependencies. Hence GAN equipped with self-attention is expected to handle details better, hooray!\n\nFig. 19. Convolution operation and self-attention have access to regions of very different sizes.\nThe SAGAN adopts the non-local neural network to apply the attention computation. The convolutional image feature maps $\\mathbf{x}$ is branched out into three copies, corresponding to the concepts of key, value, and query in the transformer:\n\nKey: $f(\\mathbf{x}) = \\mathbf{W}_f \\mathbf{x}$\nQuery: $g(\\mathbf{x}) = \\mathbf{W}_g \\mathbf{x}$\nValue: $h(\\mathbf{x}) = \\mathbf{W}_h \\mathbf{x}$\n\nThen we apply the dot-product attention to output the self-attention feature maps:\n\n$$\n\\begin{aligned}\n\\alpha_{i,j} &= \\text{softmax}(f(\\mathbf{x}_i)^\\top g(\\mathbf{x}_j)) \\\\\n\\mathbf{o}_j &= \\mathbf{W}_v \\Big( \\sum_{i=1}^N \\alpha_{i,j} h(\\mathbf{x}_i) \\Big)\n\\end{aligned}\n$$\n\n\nFig. 20. The self-attention mechanism in SAGAN. (Image source: Fig. 2 in Zhang et al., 2018)\nNote that $\\alpha_{i,j}$ is one entry in the attention map, indicating how much attention the model should pay to the $i$-th position when synthesizing the $j$-th location. $\\mathbf{W}_f$, $\\mathbf{W}_g$, and $\\mathbf{W}_h$ are all 1x1 convolution filters. If you feel that 1x1 conv sounds like a weird concept (i.e., isn\u2019t it just to multiply the whole feature map with one number?), watch this short tutorial by Andrew Ng. The output $\\mathbf{o}_j$ is a column vector of the final output $\\mathbf{o}= (\\mathbf{o}_1, \\mathbf{o}_2, \\dots, \\mathbf{o}_j, \\dots, \\mathbf{o}_N)$.\nFurthermore, the output of the attention layer is multiplied by a scale parameter and added back to the original input feature map:\n\n$$\n\\mathbf{y} = \\mathbf{x}_i + \\gamma \\mathbf{o}_i\n$$\n\nWhile the scaling parameter $\\gamma$ is increased gradually from 0 during the training, the network is configured to first rely on the cues in the local regions and then gradually learn to assign more weight to the regions that are further away.\n\nFig. 21. 128\u00d7128 example images generated by SAGAN for different classes. (Image source: Partial Fig",
    "chunk_id": "f8645fbf7cf181e83ed90908332852b4",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "4bafa4b7a99509b7f2085620eb44451a",
    "chunk": "$$\n\\mathbf{y} = \\mathbf{x}_i + \\gamma \\mathbf{o}_i\n$$\n\nWhile the scaling parameter $\\gamma$ is increased gradually from 0 during the training, the network is configured to first rely on the cues in the local regions and then gradually learn to assign more weight to the regions that are further away.\n\nFig. 21. 128\u00d7128 example images generated by SAGAN for different classes. (Image source: Partial Fig. 6 in Zhang et al., 2018)\n\nCited as:\n@article{weng2018attention,\n  title   = \"Attention? Attention!\",\n  author  = \"Weng, Lilian\",\n  journal = \"lilianweng.github.io\",\n  year    = \"2018\",\n  url     = \"https:\/\/lilianweng.github.io\/posts\/2018-06-24-attention\/\"\n}\nReferences#\n[1] \u201cAttention and Memory in Deep Learning and NLP.\u201d - Jan 3, 2016 by Denny Britz\n[2] \u201cNeural Machine Translation (seq2seq) Tutorial\u201d\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \u201cNeural machine translation by jointly learning to align and translate.\u201d ICLR 2015.\n[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. \u201cShow, attend and tell: Neural image caption generation with visual attention.\u201d ICML, 2015.\n[5] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. \u201cSequence to sequence learning with neural networks.\u201d NIPS 2014.\n[6] Thang Luong, Hieu Pham, Christopher D. Manning. \u201cEffective Approaches to Attention-based Neural Machine Translation.\u201d EMNLP 2015.\n[7] Denny Britz, Anna Goldie, Thang Luong, and Quoc Le. \u201cMassive exploration of neural machine translation architectures.\u201d ACL 2017.\n[8] Ashish Vaswani, et al. \u201cAttention is all you need.\u201d NIPS 2017.\n[9] Jianpeng Cheng, Li Dong, and Mirella Lapata. \u201cLong short-term memory-networks for machine reading.\u201d EMNLP 2016.\n[10] Xiaolong Wang, et al. \u201cNon-local Neural Networks.\u201d CVPR 2018\n[11] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. \u201cSelf-Attention Generative Adversarial Networks.\u201d arXiv preprint arXiv:1805.08318 (2018).\n[12] Nikhil Mishra, Mostafa Rohaninejad",
    "chunk_id": "4bafa4b7a99509b7f2085620eb44451a",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "3968d8353035865b06a7675872403655",
    "chunk": " for machine reading.\u201d EMNLP 2016.\n[10] Xiaolong Wang, et al. \u201cNon-local Neural Networks.\u201d CVPR 2018\n[11] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. \u201cSelf-Attention Generative Adversarial Networks.\u201d arXiv preprint arXiv:1805.08318 (2018).\n[12] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. \u201cA simple neural attentive meta-learner.\u201d ICLR 2018.\n[13] \u201cWaveNet: A Generative Model for Raw Audio\u201d - Sep 8, 2016 by DeepMind.\n[14]  Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. \u201cPointer networks.\u201d NIPS 2015.\n[15] Alex Graves, Greg Wayne, and Ivo Danihelka. \u201cNeural turing machines.\u201d arXiv preprint arXiv:1410.5401 (2014).\n\n\n\narchitecture\nattention\ntransformer\nrnn\n\n\n\n\u00ab \n\nFrom Autoencoder to Beta-VAE\n\n\n \u00bb\n\nImplementing Deep Reinforcement Learning Models with Tensorflow + OpenAI Gym\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a9 2024 Lil'Log\n\n        Powered by\n        Hugo &\n        PaperMod\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "chunk_id": "3968d8353035865b06a7675872403655",
    "document_ids": [
        "6b84d9f8ee14ff7f6f4c5f608ca4d741"
    ],
    "n_tokens": 284,
    "entities": [
        {
            "name": "\"XIAOLONG WANG ET AL.\"",
            "type": "\"PERSON\"",
            "description": "\"Xiaolong Wang and his team published a paper on Non-local Neural Networks at CVPR 2018.\"",
            "source_id": "3968d8353035865b06a7675872403655"
        },
        {
            "name": "\"HAN ZHANG ET AL.\"",
            "type": "\"PERSON\"",
            "description": "\"Han Zhang and his co-authors developed the Self-Attention Generative Adversarial Networks model.\"",
            "source_id": "3968d8353035865b06a7675872403655"
        },
        {
            "name": "\"DEEPMIND\"",
            "type": "",
            "description": "",
            "source_id": "3968d8353035865b06a7675872403655"
        },
        {
            "name": "\"WAVENET\"",
            "type": "",
            "description": "",
            "source_id": "3968d8353035865b06a7675872403655"
        },
        {
            "name": "\"NON-LOCAL NEURAL NETWORKS\"",
            "type": "",
            "description": "",
            "source_id": "3968d8353035865b06a7675872403655"
        },
        {
            "name": "\"SELF-ATTENTION GENERATIVE ADVERSARIAL NETWORKS\"",
            "type": "",
            "description": "",
            "source_id": "3968d8353035865b06a7675872403655"
        }
    ],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"&quot;XIAOLONG WANG ET AL.&quot;\">      <data key=\"d0\">\"PERSON\"<\/data>      <data key=\"d1\">\"Xiaolong Wang and his team published a paper on Non-local Neural Networks at CVPR 2018.\"<\/data>      <data key=\"d2\">3968d8353035865b06a7675872403655<\/data>    <\/node>    <node id=\"&quot;HAN ZHANG ET AL.&quot;\">      <data key=\"d0\">\"PERSON\"<\/data>      <data key=\"d1\">\"Han Zhang and his co-authors developed the Self-Attention Generative Adversarial Networks model.\"<\/data>      <data key=\"d2\">3968d8353035865b06a7675872403655<\/data>    <\/node>    <node id=\"&quot;DEEPMIND&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">3968d8353035865b06a7675872403655<\/data>    <\/node>    <node id=\"&quot;WAVENET&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">3968d8353035865b06a7675872403655<\/data>    <\/node>    <node id=\"&quot;NON-LOCAL NEURAL NETWORKS&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">3968d8353035865b06a7675872403655<\/data>    <\/node>    <node id=\"&quot;SELF-ATTENTION GENERATIVE ADVERSARIAL NETWORKS&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">3968d8353035865b06a7675872403655<\/data>    <\/node>    <edge source=\"&quot;XIAOLONG WANG ET AL.&quot;\" target=\"&quot;NON-LOCAL NEURAL NETWORKS&quot;\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">\"Xiaolong Wang and his team published a paper on Non-local Neural Networks.\"<\/data>      <data key=\"d5\">3968d8353035865b06a7675872403655<\/data>    <\/edge>    <edge source=\"&quot;HAN ZHANG ET AL.&quot;\" target=\"&quot;SELF-ATTENTION GENERATIVE ADVERSARIAL NETWORKS&quot;\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">\"Han Zhang and his co-authors developed the Self-Attention Generative Adversarial Networks model.\"<\/data>      <data key=\"d5\">3968d8353035865b06a7675872403655<\/data>    <\/edge>    <edge source=\"&quot;DEEPMIND&quot;\" target=\"&quot;WAVENET&quot;\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">\"DeepMind developed the WaveNet model for raw audio generation.\"<\/data>      <data key=\"d5\">3968d8353035865b06a7675872403655<\/data>    <\/edge>  <\/graph><\/graphml>"
}
{
    "id": "60de0e0c8ae33d5b469523a636ab7c6e",
    "chunk": "Article\nGreat Wall of China - Wikipedia\n\n1\nNames\n\n\n\n\n\n\n\n\n2\nHistory\n\n\n\n\nToggle History subsection\n\n\n\n\n\n2.1\nEarly walls\n\n\n\n\n\n\n\n\n2.2\nMing era\n\n\n\n\n\n\n\n\n2.3\nForeign accounts\n\n\n\n\n\n\n\n\n\n\n3\nCourse\n\n\n\n\nToggle Course subsection\n\n\n\n\n\n3.1\nHan Great Wall\n\n\n\n\n\n\n\n\n3.2\nMing Great Wall\n\n\n\n\n\n\n\n\n\n\n4\nCharacteristics\n\n\n\n\n\n\n\n\n5\nCondition\n\n\n\n\n\n\n\n\n6\nVisibility from space\n\n\n\n\nToggle Visibility from space subsection\n\n\n\n\n\n6.1\nFrom the Moon\n\n\n\n\n\n\n\n\n6.2\nFrom low Earth orbit\n\n\n\n\n\n\n\n\n\n\n7\nGallery\n\n\n\n\n\n\n\n\n8\nSee also\n\n\n\n\n\n\n\n\n9\nNotes\n\n\n\n\n\n\n\n\n10\nReferences\n\n\n\n\n\n\n\n\n11\nFurther reading\n\n\n\n\n\n\n\n\n12\nExternal links\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nToggle the table of contents\n\n\n\n\n\n\n\nGreat Wall of China\n\n\n\n162 languages\n\n\n\n\nAfrikaansAlemannisch\u12a0\u121b\u122d\u129b\u0627\u0644\u0639\u0631\u0628\u064a\u0629Aragon\u00e9s\u0531\u0580\u0565\u0582\u0574\u057f\u0561\u0570\u0561\u0575\u0565\u0580\u0567\u0576\u0985\u09b8\u09ae\u09c0\u09af\u09bc\u09beAsturianuAva\u00f1e'\u1ebdAz\u0259rbaycanca\u062a\u06c6\u0631\u06a9\u062c\u0647\u09ac\u09be\u0982\u09b2\u09be\u95a9\u5357\u8a9e \/ B\u00e2n-l\u00e2m-g\u00fa\u0411\u0430\u0448\u04a1\u043e\u0440\u0442\u0441\u0430\u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f\u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f (\u0442\u0430\u0440\u0430\u0448\u043a\u0435\u0432\u0456\u0446\u0430)\u092d\u094b\u091c\u092a\u0941\u0930\u0940Bikol Central\u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438\u0f56\u0f7c\u0f51\u0f0b\u0f61\u0f72\u0f42BosanskiBrezhonegCatal\u00e0\u0427\u04d1\u0432\u0430\u0448\u043b\u0430\u010ce\u0161tinaCorsuCymraegDansk\u0627\u0644\u062f\u0627\u0631\u062c\u0629Deutsch\u078b\u07a8\u0788\u07ac\u0780\u07a8\u0784\u07a6\u0790\u07b0\u0921\u094b\u091f\u0947\u0932\u0940Eesti\u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03acEspa\u00f1olEsperantoEuskara\u0641\u0627\u0631\u0633\u06ccFiji HindiF\u00f8roysktFran\u00e7aisFryskGaeilgeG\u00e0idhligGalego\u8d1b\u8a9e\u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\u5ba2\u5bb6\u8a9e \/ Hak-k\u00e2-ng\u00ee\ud55c\uad6d\uc5b4Hausa\u0540\u0561\u0575\u0565\u0580\u0565\u0576\u0939\u093f\u0928\u094d\u0926\u0940HrvatskiIdoBahasa Indonesia\u00cdslenskaItaliano\u05e2\u05d1\u05e8\u05d9\u05eaJawaKab\u0269y\u025b\u0c95\u0ca8\u0ccd\u0ca8\u0ca1\u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8\u049a\u0430\u0437\u0430\u049b\u0448\u0430KernowekKiswahiliKriy\u00f2l gwiyannenKurd\u00ee\u041a\u044b\u0440\u0433\u044b\u0437\u0447\u0430Ladin\u0ea5\u0eb2\u0ea7LatinaLatvie\u0161uL\u00ebtzebuergeschL",
    "chunk_id": "60de0e0c8ae33d5b469523a636ab7c6e",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "f7be333973349215edf826f2a68a1e38",
    "chunk": "enskaItaliano\u05e2\u05d1\u05e8\u05d9\u05eaJawaKab\u0269y\u025b\u0c95\u0ca8\u0ccd\u0ca8\u0ca1\u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8\u049a\u0430\u0437\u0430\u049b\u0448\u0430KernowekKiswahiliKriy\u00f2l gwiyannenKurd\u00ee\u041a\u044b\u0440\u0433\u044b\u0437\u0447\u0430Ladin\u0ea5\u0eb2\u0ea7LatinaLatvie\u0161uL\u00ebtzebuergeschLietuvi\u0173LigureLimburgsLingua Franca NovaLa .lojban.Magyar\u092e\u0948\u0925\u093f\u0932\u0940\u041c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438\u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02Malti\u092e\u0930\u093e\u0920\u0940\u10db\u10d0\u10e0\u10d2\u10d0\u10da\u10e3\u10e0\u10d8\u0645\u0635\u0631\u0649\u0645\u0627\u0632\u0650\u0631\u0648\u0646\u06ccBahasa Melayu\uabc3\uabe4\uabc7\uabe9 \uabc2\uabe3\uabdf\u95a9\u6771\u8a9e \/ M\u00ecng-d\u0115\u0324ng-ng\u1e73\u0304\u041c\u043e\u043d\u0433\u043e\u043b\u1019\u103c\u1014\u103a\u1019\u102c\u1018\u102c\u101e\u102cNederlandsNedersaksies\u0928\u0947\u092a\u093e\u0932\u0940\u0928\u0947\u092a\u093e\u0932 \u092d\u093e\u0937\u093e\u65e5\u672c\u8a9e\u041d\u043e\u0445\u0447\u0438\u0439\u043dNordfriiskNorsk bokm\u00e5lNorsk nynorskNouormandOccitan\u0b13\u0b21\u0b3c\u0b3f\u0b06O\u02bbzbekcha \/ \u045e\u0437\u0431\u0435\u043a\u0447\u0430\u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40\u067e\u0646\u062c\u0627\u0628\u06ccPapiamentu\u067e\u069a\u062a\u0648Patois\u1797\u17b6\u179f\u17b6\u1781\u17d2\u1798\u17c2\u179aPlattd\u00fc\u00fctschPolskiPortugu\u00easQ\u0131r\u0131mtatarcaRom\u00e2n\u0103Runa Simi\u0420\u0443\u0441\u0441\u043a\u0438\u0439\u0421\u0430\u0445\u0430 \u0442\u044b\u043b\u0430\u0938\u0902\u0938\u094d\u0915\u0943\u0924\u092e\u094d\u0633\u0631\u0627\u0626\u06cc\u06a9\u06ccScotsSeelterskShqipSicilianu\u0dc3\u0dd2\u0d82\u0dc4\u0dbdSimple English\u0633\u0646\u068c\u064aSloven\u010dinaSloven\u0161\u010dinaSoomaaliga\u06a9\u0648\u0631\u062f\u06cc\u0421\u0440\u043f\u0441\u043a\u0438 \/ srpskiSrpskohrvatski \/ \u0441\u0440\u043f\u0441\u043a\u043e\u0445\u0440\u0432\u0430\u0442\u0441\u043a\u0438SundaSuomiSvenskaTagalog\u0ba4\u0bae\u0bbf\u0bb4\u0bcdTaqbaylit\u0422\u0430\u0442\u0430\u0440\u0447\u0430 \/ tatar\u00e7a\u0c24\u0c46\u0c32\u0c41\u0c17\u0c41\u0e44\u0e17\u0e22\u0422\u043e\u04b7\u0438\u043a\u04e3T\u00fcrk\u00e7eTwi\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430\u0627\u0631\u062f\u0648\u0626\u06c7\u064a\u063a\u06c7\u0631\u0686\u06d5 \/ Uyghurche",
    "chunk_id": "f7be333973349215edf826f2a68a1e38",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "77023dbe683179e6d4350a377b653d9a",
    "chunk": " \/ \u0441\u0440\u043f\u0441\u043a\u043e\u0445\u0440\u0432\u0430\u0442\u0441\u043a\u0438SundaSuomiSvenskaTagalog\u0ba4\u0bae\u0bbf\u0bb4\u0bcdTaqbaylit\u0422\u0430\u0442\u0430\u0440\u0447\u0430 \/ tatar\u00e7a\u0c24\u0c46\u0c32\u0c41\u0c17\u0c41\u0e44\u0e17\u0e22\u0422\u043e\u04b7\u0438\u043a\u04e3T\u00fcrk\u00e7eTwi\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430\u0627\u0631\u062f\u0648\u0626\u06c7\u064a\u063a\u06c7\u0631\u0686\u06d5 \/ UyghurcheVahcuenghV\u00e8netoVeps\u00e4n kel\u2019Ti\u1ebfng Vi\u1ec7tV\u00f5ro\u6587\u8a00Winaray\u5434\u8bed\u05d9\u05d9\u05b4\u05d3\u05d9\u05e9\u7cb5\u8a9eZazaki\u017demait\u0117\u0161ka\u4e2d\u6587\n\nEdit links\n\n\n\n\n\n\n\n\n\n\n\nArticleTalk\n\n\n\n\n\nEnglish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReadView sourceView history\n\n\n\n\n\n\n\nTools\n\n\n\n\n\nTools\nmove to sidebar\nhide\n\n\n\n\t\tActions\n\t\n\n\nReadView sourceView history\n\n\n\n\n\n\t\tGeneral\n\t\n\n\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata item\n\n\n\n\n\n\t\tPrint\/export\n\t\n\n\nDownload as PDFPrintable version\n\n\n\n\n\n\t\tIn other projects\n\t\n\n\nWikimedia CommonsWikivoyage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppearance\nmove to sidebar\nhide\n\n\n\n\n\n\n\n\n\nCoordinates: 40\u00b041\u2032N 117\u00b014\u2032E\ufeff \/ \ufeff40.68\u00b0N 117.23\u00b0E\ufeff \/ 40.68; 117.23\n\n\nFrom Wikipedia, the free encyclopedia\n\n\nSeries of defensive walls along the historical northern borders of China\n\"Great Wall\" redirects here. For other uses, see Great Wall (disambiguation).\n\n\nGreat Wall of China\u842c\u91cc\u9577\u57ce \/ \u4e07\u91cc\u957f\u57ceThe Ming dynasty Great Wall at JinshanlingMap of all the wall constructionsGeneral informationTypeFortificationCountryChinaCoordinates40\u00b041\u2032N 117\u00b014\u2032E\ufeff \/ \ufeff40.68\u00b0N 117.23\u00b0E\ufeff \/ 40.68; 117.23\n\n\nUNESCO World Heritage SiteOfficial\u00a0nameThe Great WallLocationAsia-PacificCriteriaCultural: i, ii, iii, iv, viReference438Inscription1987 (11th Session)Area2,151.55 haBuffer\u00a0zone4,800.8 ha\nTechnical detailsSize21,196.18\u00a0km (13,170.70\u00a0mi)[1][2][3]\nGreat Wall of ChinaTraditional\u00a0Chinese\u9577\u57ceSimplified\u00a0Chinese\u957f\u57ceLiteral meaning\"The Long Wall\"TranscriptionsStandard MandarinHanyu PinyinCh\u00e1ngch\u00e9ngWade\u2013GilesCh'ang2-ch'eng2IPA[\u0288\u0282\u02b0a\u030c\u014b.\u0288\u0282\u02b0\u0259\u030c\u014b]WuRomanizationzan\u5e73 zen\u5e73Yue: CantoneseYale RomanizationCheung4 sing4JyutpingCoeng4sing4",
    "chunk_id": "77023dbe683179e6d4350a377b653d9a",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "8984422f0f5373043e4534b575ec016e",
    "chunk": "Chinese\u9577\u57ceSimplified\u00a0Chinese\u957f\u57ceLiteral meaning\"The Long Wall\"TranscriptionsStandard MandarinHanyu PinyinCh\u00e1ngch\u00e9ngWade\u2013GilesCh'ang2-ch'eng2IPA[\u0288\u0282\u02b0a\u030c\u014b.\u0288\u0282\u02b0\u0259\u030c\u014b]WuRomanizationzan\u5e73 zen\u5e73Yue: CantoneseYale RomanizationCheung4 sing4JyutpingCoeng4sing4IPA[ts\u02b0\u0153\u014b\u02e9.s\u026a\u014b\u02e9]Southern MinHokkien POJTn\u0302g-si\u00e2\u207fT\u00e2i-l\u00f4Tn\u0302g-si\u00e2nnAlternative Chinese nameTraditional\u00a0Chinese\u842c\u91cc\u9577\u57ceSimplified\u00a0Chinese\u4e07\u91cc\u957f\u57ceLiteral meaning\"The 10,000-li Long Wall\"TranscriptionsStandard MandarinHanyu PinyinW\u00e0nl\u01d0 Ch\u00e1ngch\u00e9ngWuRomanizationVae\u53bb-li\u4e0a zan\u5e73-zen\u5e73Yue: CantoneseYale RomanizationMaan6lei5 Cheung4sing4JyutpingMaan6-lei5 coeng4-sing4IPA[man\u02e8.lej\u02e9\u02e7\u00a0ts\u02b0\u0153\u014b\u02e9.s\u026a\u014b\u02e9]Southern MinT\u00e2i-l\u00f4B\u0101n-l\u00ed tn\u0302g-si\u00e2nn\n\nThe Great Wall of China (traditional Chinese: \u842c\u91cc\u9577\u57ce; simplified Chinese: \u4e07\u91cc\u957f\u57ce; pinyin: W\u00e0nl\u01d0 Ch\u00e1ngch\u00e9ng, literally \"ten thousand li long wall\") is a series of fortifications that were built across the historical northern borders of ancient Chinese states and Imperial China as protection against various nomadic groups from the Eurasian Steppe. Several walls were built from as early as the 7th century\u00a0BC,[4] with selective stretches later joined by Qin Shi Huang (220\u2013206\u00a0BC), the first emperor of China. Little of the Qin wall remains.[5] Later on, many successive dynasties built and maintained multiple stretches of border walls. The best-known sections of the wall were built by the Ming dynasty (1368\u20131644).\nApart from defense, other purposes of the Great Wall have included border controls, allowing the imposition of duties on goods transported along the Silk Road, regulation or encouragement of trade and the control of immigration and emigration.[6] Furthermore, the defensive characteristics of the Great Wall were enhanced by the construction of watchtowers, troop barracks, garrison stations, signaling capabilities through the means of smoke or fire, and the fact that the path of the Great Wall also served as a transportation corridor.\nThe frontier walls built by different dynasties have multiple courses. Collectively, they stretch from Liaodong in the east to Lop Lake in the west,",
    "chunk_id": "8984422f0f5373043e4534b575ec016e",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "f9d540e7e3481155012e4e0214ec2576",
    "chunk": " encouragement of trade and the control of immigration and emigration.[6] Furthermore, the defensive characteristics of the Great Wall were enhanced by the construction of watchtowers, troop barracks, garrison stations, signaling capabilities through the means of smoke or fire, and the fact that the path of the Great Wall also served as a transportation corridor.\nThe frontier walls built by different dynasties have multiple courses. Collectively, they stretch from Liaodong in the east to Lop Lake in the west, from the present-day Sino\u2013Russian border in the north to Tao River (Taohe) in the south; along an arc that roughly delineates the edge of the Mongolian steppe; spanning 21,196.18\u00a0km (13,170.70\u00a0mi) in total.[7][3] Today, the defensive system of the Great Wall is generally recognized as one of the most impressive architectural feats in history.[8]\n\n\nNames\nHuayi tu, an 1136 map of China with the Great Wall depicted on the northern edge of the country\nThe collection of fortifications known as the Great Wall of China has historically had a number of different names in both Chinese and English.\nIn Chinese histories, the term \"Long Wall(s)\" (t \u9577\u57ce, s \u957f\u57ce, Ch\u00e1ngch\u00e9ng) appears in Sima Qian's Records of the Grand Historian, where it referred both to the separate great walls built between and north of the Warring States and to the more unified construction of the First Emperor.[9] The Chinese character \u57ce, meaning city or fortress, is a phono-semantic compound of the \"earth\" radical \u571f and phonetic \u6210, whose Old Chinese pronunciation has been reconstructed as *de\u014b.[10] It originally referred to the rampart which surrounded traditional Chinese cities and was used by extension for these walls around their respective states; today, however, it is much more often the Chinese word for \"city\".[11]\nThe longer Chinese name \"Ten-Thousand Mile Long Wall\" (t \u842c\u91cc\u9577\u57ce, s \u4e07\u91cc\u957f\u57ce, W\u00e0nl\u01d0 Ch\u00e1ngch\u00e9ng) came from Sima Qian's description of it in the Records, though he did not name the walls as such. The AD\u00a0493 Book of Song quotes the frontier general Tan Daoji referring to \"the long wall of 10,000 miles\", closer to the modern name, but the name rarely features in pre-modern times otherwise.[12] The traditional Chinese mile (\u91cc, l\u01d0) was an often irregular distance that was intended to show the length of a standard village and varied with terrain but was usually standardized at distances around a third of an English mile (540\u00a0m).[13] However, this use of \"ten-thousand\" (w\u00e0n) is figurative in a similar manner to the Greek and English myriad and simply",
    "chunk_id": "f9d540e7e3481155012e4e0214ec2576",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "582bae37868475654da1ca9014d6db68",
    "chunk": " closer to the modern name, but the name rarely features in pre-modern times otherwise.[12] The traditional Chinese mile (\u91cc, l\u01d0) was an often irregular distance that was intended to show the length of a standard village and varied with terrain but was usually standardized at distances around a third of an English mile (540\u00a0m).[13] However, this use of \"ten-thousand\" (w\u00e0n) is figurative in a similar manner to the Greek and English myriad and simply means \"innumerable\" or \"immeasurable\".[14]\nBecause of the wall's association with the First Emperor's supposed tyranny, the Chinese dynasties after Qin usually avoided referring to their own additions to the wall by the name \"Long Wall\".[15] Instead, various terms were used in medieval records, including \"frontier(s)\" (\u585e, S\u00e0i),[16] \"rampart(s)\" (\u57a3, Yu\u00e1n),[16] \"barrier(s)\" (\u969c, Zh\u00e0ng),[16] \"the outer fortresses\" (\u5916\u5821, W\u00e0ib\u01ceo),[17] and \"the border wall(s)\" (t \u908a\u7246, s \u8fb9\u5899, Bi\u0101nqi\u00e1ng).[15] Poetic and informal names for the wall included \"the Purple Frontier\" (\u7d2b\u585e, Z\u01d0s\u00e0i)[18] and \"the Earth Dragon\" (t \u571f\u9f8d, s \u571f\u9f99, T\u01d4l\u00f3ng).[19] Only during the Qing period did \"Long Wall\" become the catch-all term to refer to the many border walls regardless of their location or dynastic origin, equivalent to the English \"Great Wall\".[20]\nSections of the wall in south Gobi Desert and Mongolian steppe are sometimes referred to as \"Wall of Genghis Khan\", even though Genghis Khan did not construct any walls or permanent defense lines himself.[21]\nThe current English name evolved from accounts of \"the Chinese wall\" from early modern European travelers.[20] By the nineteenth century,[20] \"the Great Wall of China\" had become standard in English and French, although other European languages such as German continue to refer to it as \"the Chinese wall\".[14]\n\nHistory\nMain article: History of the Great Wall of China\nEarly walls\nFurther information: Great Wall of Qi\nThe Great Wall of the Qin stretches from Lintao to Liaodong.\nThe Chinese were already familiar with the techniques of wall-building by the time of the Spring and Autumn period between the 8th and 5th centuries\u00a0BC.[22] During this time and the subsequent Warring States period, the states of Qin, Wei, Zhao, Qi, Han, Yan, and Zhongshan[23][24] all constructed extensive fortifications to defend their own borders. Built to withstand the attack of",
    "chunk_id": "582bae37868475654da1ca9014d6db68",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "4a56ee3901ee978bece030df769e1ff9",
    "chunk": " of the Qin stretches from Lintao to Liaodong.\nThe Chinese were already familiar with the techniques of wall-building by the time of the Spring and Autumn period between the 8th and 5th centuries\u00a0BC.[22] During this time and the subsequent Warring States period, the states of Qin, Wei, Zhao, Qi, Han, Yan, and Zhongshan[23][24] all constructed extensive fortifications to defend their own borders. Built to withstand the attack of small arms such as swords and spears, these walls were made mostly of stone or by stamping earth and gravel between board frames.\n\nThe Great Wall of the Han is the longest of all walls, from Mamitu, near Yumenguan, to Liaodong.\nKing Zheng of Qin conquered the last of his opponents and unified China as the First Emperor of the Qin dynasty (\"Qin Shi Huang\") in 221\u00a0BC.  Intending to impose centralized rule and prevent the resurgence of feudal lords, he ordered the destruction of the sections of the walls that divided his empire among the former states. To position the empire against the Xiongnu people from the north, however, he ordered the building of new walls to connect the remaining fortifications along the empire's northern frontier. \"Build and move on\" was a central guiding principle in constructing the wall, implying that the Chinese were not erecting a permanently fixed border.[25]\nTransporting the large quantity of materials required for construction was difficult, so builders always tried to use local resources. Stones from the mountains were used over mountain ranges, while rammed earth was used for construction in the plains. There are no surviving historical records indicating the exact length and course of the Qin walls. Most of the ancient walls have eroded away over the centuries, and very few sections remain today. The human cost of the construction is unknown, but it has been estimated by some authors that hundreds of thousands[26] of workers died building the Qin wall. Later, the Han,[27] the Northern dynasties and the Sui all repaired, rebuilt, or expanded sections of the Great Wall at great cost to defend themselves against northern invaders.[28] The Tang and Song dynasties did not undertake any significant effort in the region.[28] Dynasties founded by non-Han ethnic groups also built their border walls: the Xianbei-ruled Northern Wei, the Khitan-ruled Liao, Jurchen-led Jin and the Tangut-established Western Xia, who ruled vast territories over Northern China throughout centuries, all constructed defensive walls but those were located much to the north of the other Great Walls as we know it, within China's autonomous region of Inner Mongolia and in modern-day Mongolia itself.[29]\n\nMing era\nMain article: Ming Great Wall\nThe extent of the Ming Empire and its walls\nThe Great Wall concept was revived again under the Ming in the 14th century,[30] and following",
    "chunk_id": "4a56ee3901ee978bece030df769e1ff9",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "13913f97931616b65d066601e428309b",
    "chunk": " Jin and the Tangut-established Western Xia, who ruled vast territories over Northern China throughout centuries, all constructed defensive walls but those were located much to the north of the other Great Walls as we know it, within China's autonomous region of Inner Mongolia and in modern-day Mongolia itself.[29]\n\nMing era\nMain article: Ming Great Wall\nThe extent of the Ming Empire and its walls\nThe Great Wall concept was revived again under the Ming in the 14th century,[30] and following the Ming army's defeat by the Oirats in the Battle of Tumu. The Ming had failed to gain a clear upper hand over the Mongol tribes after successive battles, and the long-drawn conflict was taking a toll on the empire. The Ming adopted a new strategy to keep the nomadic tribes out by constructing walls along the northern border of China. Acknowledging the Mongol control established in the Ordos Desert, the wall followed the desert's southern edge instead of incorporating the bend of the Yellow River.\nUnlike the earlier fortifications, the Ming construction was stronger and more elaborate due to the use of bricks and stone instead of rammed earth. Up to 25,000 watchtowers are estimated to have been constructed on the wall.[31] As Mongol raids continued periodically over the years, the Ming devoted considerable resources to repair and reinforce the walls. Sections near the Ming capital of Beijing were especially strong.[32] Qi Jiguang between 1567 and 1570 also repaired and reinforced the wall, faced sections of the ram-earth wall with bricks and constructed 1,200 watchtowers from Shanhaiguan Pass to Changping to warn of approaching Mongol raiders.[33] During the 1440s\u20131460s, the Ming also built a so-called \"Liaodong Wall\". Similar in function to the Great Wall (whose extension, in a sense, it was), but more basic in construction, the Liaodong Wall enclosed the agricultural heartland of the Liaodong province, protecting it against potential incursions by Jurchen-Mongol Oriyanghan from the northwest and the Jianzhou Jurchens from the north. While stones and tiles were used in some parts of the Liaodong Wall, most of it was in fact simply an earth dike with moats on both sides.[34]\nTowards the end of the Ming, the Great Wall helped defend the empire against the Manchu invasions that began around 1600. Even after the loss of all of Liaodong, the Ming army held the heavily fortified Shanhai Pass, preventing the Manchus from conquering the Chinese heartland. The Manchus were finally able to cross the Great Wall in 1644, after Beijing had already fallen to Li Zicheng's short-lived Shun dynasty. Before this time, the Manchus had crossed the Great Wall multiple times to raid, but this time it was for conquest.",
    "chunk_id": "13913f97931616b65d066601e428309b",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "1badb4cf5e7251e0f334c2e504fca0b7",
    "chunk": " around 1600. Even after the loss of all of Liaodong, the Ming army held the heavily fortified Shanhai Pass, preventing the Manchus from conquering the Chinese heartland. The Manchus were finally able to cross the Great Wall in 1644, after Beijing had already fallen to Li Zicheng's short-lived Shun dynasty. Before this time, the Manchus had crossed the Great Wall multiple times to raid, but this time it was for conquest. The gates at Shanhai Pass were opened on May 25 by the commanding Ming general, Wu Sangui, who formed an alliance with the Manchus, hoping to use the Manchus to expel the rebels from Beijing.[35] The Manchus quickly seized Beijing, and eventually defeated both the Shun dynasty and the remaining Ming resistance, consolidating the rule of the Qing dynasty over all of China proper.[36]\nUnder Qing rule, China's borders extended beyond the walls and Mongolia was annexed into the empire, so constructions on the Great Wall were discontinued. On the other hand, the so-called Willow Palisade, following a line similar to that of the Ming Liaodong Wall, was constructed by the Qing rulers in Manchuria. Its purpose, however, was not defense but rather to prevent Han Chinese migration into Manchuria.[37]\n\nForeign accounts\nPart of the Great Wall of China (April 1853, X, p. 41)[38]\nThe Great Wall in 1907\nNone of the Europeans who visited China or Mongolia in the 13th and 14th centuries, such as Giovanni da Pian del Carpine, William of Rubruck, Marco Polo, Odoric of Pordenone and Giovanni de' Marignolli, mentioned the Great Wall.[39][40]\nThe North African traveler Ibn Battuta, who also visited China during the Yuan dynasty c.\u20091346, had heard about China's Great Wall, possibly before he had arrived in China.[41] He wrote that the wall is \"sixty days' travel\" from Zeitun (modern Quanzhou) in his travelogue Gift to Those Who Contemplate the Wonders of Cities and the Marvels of Travelling. He associated it with the legend of the wall mentioned in the Qur'an,[42] which Dhul-Qarnayn (commonly associated with Alexander the Great) was said to have erected to protect people near the land of the rising sun from the savages of Gog and Magog. However, Ibn Battuta could find no one who had either seen it or knew of anyone who had seen it, suggesting that although there were remnants of the wall at that time, they were not significant.[43]\nSoon after Europeans reached Ming China by ship in the early 16th century, accounts of the Great Wall started to circulate in Europe, even though no European was to see it for another century.",
    "chunk_id": "1badb4cf5e7251e0f334c2e504fca0b7",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "c1b9b6246d8889ded0159b26ecee738c",
    "chunk": " the land of the rising sun from the savages of Gog and Magog. However, Ibn Battuta could find no one who had either seen it or knew of anyone who had seen it, suggesting that although there were remnants of the wall at that time, they were not significant.[43]\nSoon after Europeans reached Ming China by ship in the early 16th century, accounts of the Great Wall started to circulate in Europe, even though no European was to see it for another century. Possibly one of the earliest European descriptions of the wall and of its significance for the defense of the country against the \"Tartars\" (i.e. Mongols) may be the one contained in Jo\u00e3o de Barros's 1563 Asia.[44] Other early accounts in Western sources include those of Gaspar da Cruz, Bento de Goes, Matteo Ricci, and Bishop Juan Gonz\u00e1lez de Mendoza,[45] the latter in 1585 describing it as a \"superbious and mightie work\" of architecture, though he had not seen it.[46] In 1559, in his work \"A Treatise of China and the Adjoyning Regions\", Gaspar da Cruz offers an early discussion of the Great Wall.[45] Perhaps the first recorded instance of a European actually entering China via the Great Wall came in 1605, when the Portuguese Jesuit brother Bento de G\u00f3is reached the northwestern Jiayu Pass from India.[47] Early European accounts were mostly modest and empirical, closely mirroring contemporary Chinese understanding of the Wall,[48] although later they slid into hyperbole,[49] including the erroneous but ubiquitous claim that the Ming walls were the same ones that were built by the first emperor in the 3rd century\u00a0BC.[49]\nWhen China opened its borders to foreign merchants and visitors after its defeat in the First and Second Opium Wars, the Great Wall became a main attraction for tourists. The travelogues of the later 19th century further enhanced the reputation and the mythology of the Great Wall.[50]\n\n\nCourse\nA formal definition of what constitutes a \"Great Wall\" has not been agreed upon, making the full course of the Great Wall difficult to describe in its entirety.[51] The defensive lines contain multiple stretches of ramparts, trenches and ditches, as well as individual fortresses.\nIn 2012, based on existing research and the results of a comprehensive mapping survey, the National Cultural Heritage Administration of China concluded that the remaining Great Wall associated sites include 10,051 wall sections, 1,764 ramparts or trenches, 29,510 individual buildings, and 2,211 fortifications or passes, with the walls and trenches spanning a total length of 21,196.18\u00a0km (13,170.70\u00a0mi).[3] Incorporating advanced technologies, the study has concluded that the Ming Great Wall measures 8,850\u00a0km (5,500",
    "chunk_id": "c1b9b6246d8889ded0159b26ecee738c",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "66d7dc1ac9ec1e4e7fd67955cae6e8d9",
    "chunk": " Heritage Administration of China concluded that the remaining Great Wall associated sites include 10,051 wall sections, 1,764 ramparts or trenches, 29,510 individual buildings, and 2,211 fortifications or passes, with the walls and trenches spanning a total length of 21,196.18\u00a0km (13,170.70\u00a0mi).[3] Incorporating advanced technologies, the study has concluded that the Ming Great Wall measures 8,850\u00a0km (5,500\u00a0mi).[52] This consists of 6,259\u00a0km (3,889\u00a0mi) of wall sections, 359\u00a0km (223\u00a0mi) of trenches and 2,232\u00a0km (1,387\u00a0mi) of natural defensive barriers such as hills and rivers.[52] In addition, Qin, Han and earlier Great Wall sites are 3,080\u00a0km (1,914\u00a0mi) long in total; Jin dynasty (1115\u20131234) border fortifications are 4,010\u00a0km (2,492\u00a0mi) in length; the remainder date back to Northern Wei, Northern Qi, Sui, Tang, the Five Dynasties, Song, Liao and Xixia.[3] About half of the sites are located in Inner Mongolia (31%) and Hebei (19%).[3]\n\nHan Great Wall\nGreat Wall of Han dynasty near Yumenguan\nHan fortifications starts from Yumen Pass and Yang Pass, southwest of Dunhuang, in Gansu province. Ruins of the remotest Han border posts are found in Mamitu (t \u99ac\u8ff7\u9014, s \u9a6c\u8ff7\u9014, M\u01cem\u00edt\u00fa, l \"horses losing their way\") near Yumen Pass.\n\nMing Great Wall\nJiayuguan, the westmost point of Ming Great Wall\nThe Jiayu Pass, located in Gansu province, is the western terminus of the Ming Great Wall. From Jiayu Pass the wall travels discontinuously down the Hexi Corridor and into the deserts of Ningxia, where it enters the western edge of the Yellow River loop at Yinchuan. Here the first major walls erected during the Ming dynasty cut through the Ordos Desert to the eastern edge of the Yellow River loop. There at Piantou Pass (t \u504f\u982d\u95dc, s \u504f\u5934\u5173, Pi\u0101nt\u00f3ugu\u0101n) in Xinzhou, Shanxi province, the Great Wall splits in two with the \"Outer Great Wall\" (t \u5916\u9577\u57ce, s \u5916\u957f\u57ce, W\u00e0i Ch\u01cengch\u00e9ng) extending along the Inner Mongolia border with Shanxi into Hebei province, and the \"Inner Great Wall\" (t \u5167\u9577\u57ce, s \u5167\u957f\u57ce, N\u00e8i Ch\u01cengch\u00e9ng) running southeast",
    "chunk_id": "66d7dc1ac9ec1e4e7fd67955cae6e8d9",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "85ab2e93be34d4c8007d712d356b564d",
    "chunk": " Pi\u0101nt\u00f3ugu\u0101n) in Xinzhou, Shanxi province, the Great Wall splits in two with the \"Outer Great Wall\" (t \u5916\u9577\u57ce, s \u5916\u957f\u57ce, W\u00e0i Ch\u01cengch\u00e9ng) extending along the Inner Mongolia border with Shanxi into Hebei province, and the \"Inner Great Wall\" (t \u5167\u9577\u57ce, s \u5167\u957f\u57ce, N\u00e8i Ch\u01cengch\u00e9ng) running southeast from Piantou Pass for some 400\u00a0km (250\u00a0mi), passing through important passes like the Pingxing Pass and Yanmen Pass before joining the Outer Great Wall at Sihaiye (\u56db\u6d77\u51b6, S\u00ech\u01ceiy\u011b), in Beijing's Yanqing County.\nThe sections of the Great Wall around Beijing municipality are especially famous: they were frequently renovated and are regularly visited by tourists today. The Badaling Great Wall near Zhangjiakou is the most famous stretch of the wall, for this was the first section to be opened to the public in the People's Republic of China, as well as the showpiece stretch for foreign dignitaries.[53] The Badaling Great Wall saw nearly 10 million visitors in 2018, and in 2019, a daily limit of 65,000 visitors was instated.[54] South of Badaling is the Juyong Pass; when it was used by the Chinese to protect their land, this section of the wall had many guards to defend the capital Beijing. Made of stone and bricks from the hills, this portion of the Great Wall is 7.8\u00a0m (25\u00a0ft 7\u00a0in) high and 5\u00a0m (16\u00a0ft 5\u00a0in) wide.\n\nMing dynasty Great Wall at Jinshanling\nOne of the most striking sections of the Ming Great Wall is where it climbs extremely steep slopes in Jinshanling. There it runs 11\u00a0km (7\u00a0mi) long, ranges from 5 to 8\u00a0m (16\u00a0ft 5\u00a0in to 26\u00a0ft 3\u00a0in) in height, and 6\u00a0m (19\u00a0ft 8\u00a0in) across the bottom, narrowing up to 5\u00a0m (16\u00a0ft 5\u00a0in) across the top. Wangjing Lou (t \u671b\u4eac\u6a13, s \u671b\u4eac\u697c, W\u00e0ngj\u012bng L\u00f3u) is one of Jinshanling's 67 watchtowers, 980\u00a0m (3,220\u00a0ft) above sea level. Southeast of Jinshanling is the Mutianyu Great Wall which winds along lofty, cragged mountains from the southeast to the northwest for 2.25\u00a0km (1.40\u00a0mi). It is connected with Juyongguan Pass to the west and Gubeikou to the east. This section was",
    "chunk_id": "85ab2e93be34d4c8007d712d356b564d",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "3e43120f7f351002f7f3ba71f9ae5906",
    "chunk": "j\u012bng L\u00f3u) is one of Jinshanling's 67 watchtowers, 980\u00a0m (3,220\u00a0ft) above sea level. Southeast of Jinshanling is the Mutianyu Great Wall which winds along lofty, cragged mountains from the southeast to the northwest for 2.25\u00a0km (1.40\u00a0mi). It is connected with Juyongguan Pass to the west and Gubeikou to the east. This section was one of the first to be renovated following the turmoil of the Cultural Revolution.[55]\nAt the edge of the Bohai Gulf is Shanhai Pass, considered the traditional end of the Great Wall and the \"First Pass Under Heaven\". The part of the wall inside Shanhai Pass that meets the sea is named the \"Old Dragon Head\". 3\u00a0km (2\u00a0mi) north of Shanhai Pass is Jiaoshan Great Wall (t \u7126\u5c71\u9577\u57ce, s \u7126\u5c71\u957f\u57ce, Ji\u0101osh\u0101n Ch\u00e1ngch\u00e9ng), the site of the first mountain of the Great Wall.[56] 15\u00a0km (9\u00a0mi) northeast from Shanhaiguan is Jiumenkou (t \u4e5d\u9580\u53e3, s \u4e5d\u95e8\u53e3, Ji\u01d4m\u00e9nk\u01d2u), which is the only portion of the wall that was built as a bridge.\nIn 2009, 180\u00a0km of previously unknown sections of the Ming wall concealed by hills, trenches and rivers were discovered with the help of infrared range finders and GPS devices.[57] In March and April 2015, nine sections with a total length of more than 10\u00a0km (6\u00a0mi), believed to be part of the Great Wall, were discovered along the border of Ningxia autonomous region and Gansu province.[58]\n\nCharacteristics\nBefore the use of bricks, the Great Wall was mainly built from rammed earth, stones, and wood. During the Ming, however, bricks were heavily used in many areas of the wall, as were materials such as tiles, lime, and stone. The size and weight of the bricks made them easier to work with than earth and stone, so construction quickened. Additionally, bricks could bear more weight and endure better than rammed earth. Stone can hold under its own weight better than brick, but is more difficult to use. Consequently, stones cut into rectangular shapes were used for the foundation, inner and outer brims, and gateways of the wall. Battlements line the uppermost portion of the vast majority of the wall, with defensive gaps a little over 30\u00a0cm (12\u00a0in) tall, and about 23\u00a0cm (9.1\u00a0in) wide. From the parapets, guards could survey the surrounding land.[59]\nSticky rice mortar, consisting of sticky rice soup mixed with slaked lime, was extensively",
    "chunk_id": "3e43120f7f351002f7f3ba71f9ae5906",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "288c066ce2f139cb25f300ffb971fb45",
    "chunk": " shapes were used for the foundation, inner and outer brims, and gateways of the wall. Battlements line the uppermost portion of the vast majority of the wall, with defensive gaps a little over 30\u00a0cm (12\u00a0in) tall, and about 23\u00a0cm (9.1\u00a0in) wide. From the parapets, guards could survey the surrounding land.[59]\nSticky rice mortar, consisting of sticky rice soup mixed with slaked lime, was extensively used to hold bricks together;[60][61] no human bones or body parts were ever incorporated into the mortar or any part of the wall, contrary to what a legend states.[62][63] Communication between the army units along the length of the Great Wall, including the ability to call reinforcements and warn garrisons of enemy movements, was of high importance. Signal towers were built upon hill tops or other high points along the wall for their visibility. Wooden gates could be used as a trap against those going through. Barracks, stables, and armories were built near the wall's inner surface.[59]\n\nCondition\nA more rural portion of the Great Wall that stretches through the mountains, here seen in slight disrepair\nWhile portions north of Beijing and near tourist centers have been preserved and even extensively renovated, in many other locations the wall is in disrepair. The wall sometimes provided a source of stones to build houses and roads.[64] Sections of the wall are also prone to graffiti and vandalism, while inscribed bricks were pilfered and sold on the market for up to 50 renminbi.[65] Parts have been destroyed to make way for construction or mining.[66]\nA 2012 report by the National Cultural Heritage Administration states that 22% of the Ming Great Wall has disappeared, while 1,961\u00a0km (1,219\u00a0mi) of wall have vanished.[65] In 2007 it was estimated that more than 60\u00a0km (37\u00a0mi) of the wall in Gansu province may disappear in the next 20 years, due to erosion from sandstorms. In some places, the height of the wall has been reduced from more than 5\u00a0m (16\u00a0ft 5\u00a0in) to less than 2\u00a0m (6\u00a0ft 7\u00a0in). Various square lookout towers that characterize the most famous images of the wall have disappeared. Many western sections of the wall are constructed from mud, rather than brick and stone, and thus are more susceptible to erosion.[67] In 2014 a portion of the wall near the border of Liaoning and Hebei province was repaired with concrete. The work has been much criticized.[68]\nA section of the wall in Shanxi province was severely damaged in 2023 by construction workers, who widened an existing gap in the wall to make a shortcut for an excavator to pass through. Police described the act as causing \"irreversible",
    "chunk_id": "288c066ce2f139cb25f300ffb971fb45",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "6e69cd0b5b252867b27c8630604beeb6",
    "chunk": ", rather than brick and stone, and thus are more susceptible to erosion.[67] In 2014 a portion of the wall near the border of Liaoning and Hebei province was repaired with concrete. The work has been much criticized.[68]\nA section of the wall in Shanxi province was severely damaged in 2023 by construction workers, who widened an existing gap in the wall to make a shortcut for an excavator to pass through. Police described the act as causing \"irreversible damage to the integrity of the Ming Great Wall and to the safety of the cultural relics\".[69]\n\nVisibility from space\nVarious factoids in popular culture claim that the Great Wall can be seen (with the naked eye) from space, with questionable degrees of veracity.\n\nFrom the Moon\nThe Great Wall of China cannot be seen by the naked human eye from the Moon.[70] Even though the myth is thoroughly debunked,[71] it is still ingrained in popular culture.[72] The apparent width of the Great Wall from the Moon would be the same as that of a human hair viewed from 3\u00a0km (2\u00a0mi) away.[73]\nOne of the earliest known references to the myth that the Great Wall can be seen from the moon appears in a letter written in 1754 by the English antiquary William Stukeley. Stukeley wrote that, \"This mighty wall [Hadrian's wall] of four score miles [130\u00a0km] in length is only exceeded by the Chinese Wall, which makes a considerable figure upon the terrestrial globe, and may be discerned at the Moon.\"[74] The claim was also mentioned by Henry Norman in 1895 where he states \"besides its age it enjoys the reputation of being the only work of human hands on the globe visible from the Moon.\"[75] The issue of \"canals\" on Mars was prominent in the late 19th century and may have led to the belief that long, thin objects were visible from space. The claim that the Great Wall is visible from the moon also appears in 1932's Ripley's Believe It or Not! strip.[76]\n\nFrom low Earth orbit\nIdentical satellite images of a section of the Great Wall in northern Shanxi, running diagonally from lower left to upper right and not to be confused with the more prominent river running from upper left to lower right. In the image on the right, the Great Wall has been outlined in red. The region pictured is 12\u00a0km \u00d7\u00a012\u00a0km (7\u00a0mi \u00d7\u00a07\u00a0mi).\nA more controversial question is whether the wall is visible from low Earth orbit (an altitude of as little as 160\u00a0km (100\u00a0mi)). NASA claims that it is barely visible, and only under nearly perfect conditions; it is no more conspicuous than many other human-made objects.[77]\nVeteran US astronaut Gene Cernan has stated: \"At Earth orbit of 100",
    "chunk_id": "6e69cd0b5b252867b27c8630604beeb6",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "23cb603e53694dcbc8b01841f0e14320",
    "chunk": " The region pictured is 12\u00a0km \u00d7\u00a012\u00a0km (7\u00a0mi \u00d7\u00a07\u00a0mi).\nA more controversial question is whether the wall is visible from low Earth orbit (an altitude of as little as 160\u00a0km (100\u00a0mi)). NASA claims that it is barely visible, and only under nearly perfect conditions; it is no more conspicuous than many other human-made objects.[77]\nVeteran US astronaut Gene Cernan has stated: \"At Earth orbit of 100 to 200 miles [160 to 320\u00a0km] high, the Great Wall of China is, indeed, visible to the naked eye.\" Ed Lu, Expedition 7 Science Officer aboard the International Space Station, adds that, \"It's less visible than a lot of other objects. And you have to know where to look.\"\nIn October 2003, Chinese astronaut Yang Liwei stated that he had not been able to see the Great Wall of China. In response, the European Space Agency (ESA) issued a press release reporting that from an orbit between 160 and 320\u00a0km (100 and 200\u00a0mi), the Great Wall is visible to the naked eye.[73] The image was actually a river in Beijing.[78]\nLeroy Chiao, a Chinese-American astronaut, took a photograph from the International Space Station that shows the wall. It was so indistinct that the photographer was not certain he had actually captured it. Based on the photograph, the China Daily later reported that the Great Wall can be seen from 'space' with the naked eye, under favorable viewing conditions, if one knows exactly where to look.[79][73]\n\nGallery\n\n\n\nThe Great Wall at Badaling\n\n\n\nThe Great Wall at dawn\n\n\n\nThe Juyongguan area of the Great Wall accepts numerous tourists each day.\n\n\n\nRemains of Beacon tower, near Yumenguan, 2011\n\n\n\n\"The First Mound\" \u2013 at Jiayu Pass, the western terminus of the Ming wall\n\n\n\nThe Great Wall near Jiayu Pass, Qilian Mountains in behind\n\n\n\nMing Great Wall remnant, near Yinchuan\n\n\n\nThe Great Wall remnant at Yulin\n\n\n\nGateway of Gubeikou Fortress\n\n\n\nEnvironmental protection sign, near Great Wall, 2011\n\n\n\nMing Great Wall at Simatai, overlooking the gorge\n\n\n\nMutianyu Great Wall. This is atop the wall on a section that has not been restored.\n\n\n\nThe Old Dragon Head, the Great Wall where it meets the sea in the vicinity of Shanhai Pass\n\n\n\nInside the watchtower\n\n\n\nInside a watchtower\n\n\n\nBadaling Great Wall during winter\n\n\n\nThe Great Wall during autumn\/winter\n\n\n\nTourists at The Great Wall\n\n\n\nTourists at The Great Wall\n\n\nSee also\n\nCheolli Jangseong\nChinese city wall\nDefense of the Great Wall\nGates of Alexander\nGrand Canal (China)\nGreat Wall of China hoax\nGreat Wall Marathon\nGreat Wall of Gorgan",
    "chunk_id": "23cb603e53694dcbc8b01841f0e14320",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "d0c3bd559c1cfed3ab23390c6b551e2b",
    "chunk": " Great Wall where it meets the sea in the vicinity of Shanhai Pass\n\n\n\nInside the watchtower\n\n\n\nInside a watchtower\n\n\n\nBadaling Great Wall during winter\n\n\n\nThe Great Wall during autumn\/winter\n\n\n\nTourists at The Great Wall\n\n\n\nTourists at The Great Wall\n\n\nSee also\n\nCheolli Jangseong\nChinese city wall\nDefense of the Great Wall\nGates of Alexander\nGrand Canal (China)\nGreat Wall of China hoax\nGreat Wall Marathon\nGreat Wall of Gorgan\nGreat Wall of India\nList of World Heritage Sites in China\nMiaojiang Great Wall\nOffa's Dyke\nRoman military frontiers and fortifications\nZasechnaya cherta\n\nNotes\n\n\n^ \"China's Great Wall Found To Measure More Than 20,000 Kilometers\". Bloomberg. June 5, 2012. Retrieved June 6, 2012.\n\n^ \"China's Great Wall is 'longer than previously thought'\". BBC News. June 6, 2012. Archived from the original on December 5, 2021. Retrieved December 28, 2021.\n\n^ a b c d e \u4e2d\u56fd\u957f\u57ce\u4fdd\u62a4\u62a5\u544a [Protection Report of the Great Wall of China]. National Cultural Heritage Administration.\n\n^ The New York Times with introduction by Sam Tanenhaus (2011). The New York Times Guide to Essential Knowledge: A Desk Reference for the Curious Mind. St. Martin's Press of Macmillan Publishers. p.\u00a01131. ISBN\u00a0978-0-312-64302-7. Beginning as separate sections of fortification around the 7th century B.C.E and unified during the Qin Dynasty in the 3rd century B.C.E, this wall, built of earth and rubble with a facing of brick or stone, runs from east to west across China for over 4,000 miles.\n\n^ \"Great Wall of China\". Encyclop\u00e6dia Britannica. October 21, 2023. Large parts of the fortification system date from the 7th through the 4th century BC. In the 3rd century BC Shihuangdi (Qin Shi Huang), the first emperor of a united China (under the Qin dynasty), connected a number of existing defensive walls into a single system. Traditionally, the eastern terminus of the wall was considered to be Shanhai Pass (Shanhaiguan) on the coast of the Bohai (Gulf of Zhili), and the wall's length\u00a0\u2013 without its branches and other secondary sections\u00a0\u2013 was thought to extend for some 6,690\u00a0km (4,160\u00a0mi).\n\n^ Shelach-Lavi, Gideon; Wachtel, Ido; Golan, Dan; Batzorig, Otgonjargal; Amartuvshin, Chunag; Ellenblum, Ronnie; Honeychurch, William (June 2020).",
    "chunk_id": "d0c3bd559c1cfed3ab23390c6b551e2b",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "18a46b7a2ec33b3d7d99512fbfecadd2",
    "chunk": " Bohai (Gulf of Zhili), and the wall's length\u00a0\u2013 without its branches and other secondary sections\u00a0\u2013 was thought to extend for some 6,690\u00a0km (4,160\u00a0mi).\n\n^ Shelach-Lavi, Gideon; Wachtel, Ido; Golan, Dan; Batzorig, Otgonjargal; Amartuvshin, Chunag; Ellenblum, Ronnie; Honeychurch, William (June 2020). \"Medieval long-wall construction on the Mongolian Steppe during the eleventh to thirteenth centuries AD\". Antiquity. 94 (375): 724\u2013741. doi:10.15184\/aqy.2020.51. ISSN\u00a00003-598X.\n\n^ \"Great Wall of China even longer than previously thought\". Canadian Broadcasting Corporation. June 6, 2012. Retrieved June 6, 2012.\n\n^ \"Great Wall of China\". History. April 20, 2009.\n\n^ Waldron 1983, p.\u00a0650.\n\n^ Baxter, William H. &\u00a0al. (September 20, 2014). \"Baxter\u2013Sagart Old Chinese Reconstruction, Version 1.1\" (PDF). Ann Arbor: University of Michigan. Retrieved January 22, 2015.\n\n^ See Lovell 2006, p. 25\n\n^ Waldron 1990, p.\u00a0202. Tan Daoji's exact quote: \"So you would destroy your Great Wall of Ten Thousand Li!\" (\u4e43\u5fa9\u58de\u6c5d\u842c\u91cc\u4e4b\u9577\u57ce) Note the use of the particle \u4e4b zhi that differentiates the quote from the modern name.\n\n^ Byron R. Winborn (1994). Wen Bon: a Naval Air Intelligence Officer behind Japanese lines in China. University of North Texas Press. p.\u00a063. ISBN\u00a0978-0-929398-77-8.\n\n^ a b Lindesay, William (2007). The Great Wall Revisited: From the Jade Gate to Old Dragon's Head. Beijing: Wuzhou Publishing. p.\u00a021. ISBN\u00a0978-7-5085-1032-3.\n\n^ a b Waldron 1983, p.\u00a0651.\n\n^ a b c Lovell 2006, p.\u00a015.\n\n^ Waldron 1990, p.\u00a049.\n\n^ Waldron 1990, p.\u00a021.\n\n^ Waldron 1988, p.\u00a069.\n\n^ a b c Hessler 2007, p.\u00a059.\n\n^ Man, John (2008). \"6. WALL-HUNT IN THE GOBI\". The Great Wall: The extraordinary history of China's wonder of the world. TRANSWORLD PUBLISHERS LTD. pp.\u00a0132\u2013148. ISBN\u00a09780553817683.\n\n",
    "chunk_id": "18a46b7a2ec33b3d7d99512fbfecadd2",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "f3293d470518b86144cb0072fa588b63",
    "chunk": "49.\n\n^ Waldron 1990, p.\u00a021.\n\n^ Waldron 1988, p.\u00a069.\n\n^ a b c Hessler 2007, p.\u00a059.\n\n^ Man, John (2008). \"6. WALL-HUNT IN THE GOBI\". The Great Wall: The extraordinary history of China's wonder of the world. TRANSWORLD PUBLISHERS LTD. pp.\u00a0132\u2013148. ISBN\u00a09780553817683.\n\n^ \u6b77\u4ee3\u738b\u671d\u4fee\u9577\u57ce (in Chinese). Chiculture.net. Retrieved October 24, 2010.\n\n^ \u53e4\u4ee3\u957f\u57ce \u2013 \u6218\u4e89\u4e0e\u548c\u5e73\u7684\u7ebd\u5e26 (in Chinese). Newsmth.net. Retrieved October 24, 2010.\n\n^ \u4e07\u91cc\u957f\u57ce (in Chinese). Newsmth.net. Retrieved October 24, 2010.\n\n^ Burbank, Jane; Cooper, Frederick (2010). Empires in World History: Power and the Politics of Difference. Princeton, New Jersey: Princeton University Press. p.\u00a045.\n\n^ Slavicek, Mitchell & Matray 2005, p.\u00a035.\n\n^ Coonan, Clifford (February 27, 2012). \"British researcher discovers piece of Great Wall 'marooned outside China'\". The Irish Times. Retrieved February 28, 2012.\n\n^ a b Waldron 1983, p.\u00a0653.\n\n^ Waldron 1983, p.\u00a0654; Haw 2006, pp.\u00a052\u201354.\n\n^ Karnow & Mooney 2008, p.\u00a0192.\n\n^ Szab\u00f3, D\u00e1vid & Loczy 2010, p.\u00a0220.\n\n^ Evans 2006, p.\u00a0177.\n\n^ \"Great Wall at Mutianyu\". Great Wall of China. Archived from the original on March 9, 2013.\n\n^ Edmonds 1985, pp.\u00a038\u201340.\n\n^ Lovell 2006, p.\u00a0254.\n\n^ Elliott 2001, pp.\u00a01\u20132.\n\n^ Elliott, Mark C. \"The Limits of Tartary: Manchuria in Imperial and National Geographies\". Journal of Asian Studies 59, no. 3 (2000): 603\u2013646.\n\n^ \"Part of the Great Wall of China\". The Wesleyan Juvenile Offering: A Miscellany of Missionary Information for Young Persons. X: 41. April 1853. Retrieved February 29, 2016.\n\n^ Ruysbroek, Willem van (1900) [1255]. The Journey of William of Rubruck to the Eastern Parts of the World, 1253\u201355, as Narrated by Himself, with Two Accounts of the Earlier Journey of John of Pian de Carpine. Translated from the Latin",
    "chunk_id": "f3293d470518b86144cb0072fa588b63",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "45e9c4a8746ef74104e1c163f507ae88",
    "chunk": "venile Offering: A Miscellany of Missionary Information for Young Persons. X: 41. April 1853. Retrieved February 29, 2016.\n\n^ Ruysbroek, Willem van (1900) [1255]. The Journey of William of Rubruck to the Eastern Parts of the World, 1253\u201355, as Narrated by Himself, with Two Accounts of the Earlier Journey of John of Pian de Carpine. Translated from the Latin by William Woodville Rockhill. London: The Hakluyt Society.\n\n^ Haw 2006, pp.\u00a053\u201354.\n\n^ Haw 2006, pp.\u00a054\u201355.\n\n^ Qur'an, XVIII: \"The Cave\". English translations hosted at Wikisource include Maulana Muhammad Ali's, E.H. Palmer's, and the Progressive Muslims Organization's.\n\n^ Haw 2006, pp.\u00a053\u201355.\n\n^ Barros, Jo\u00e3o de (1777) [1563]. \u00c1sia de Jo\u00e3o de Barros: Dos feitos que os portugueses fizeram no descobrimento dos mares e terras do Oriente. Vol.\u00a0V. Lisbon: Lisboa. 3a D\u00e9cada, pp.\u00a0186\u2013204 (originally Vol. II, Ch. vii).\n\n^ a b Waldron 1990, pp.\u00a0204\u201305.\n\n^ Lach, Donald F (1965). Asia in the Making of Europe. Vol.\u00a0I. The University of Chicago Press. p.\u00a0769.\n\n^ Yule 1866, p.\u00a0579This section is the report of G\u00f3is's travel, as reported by Matteo Ricci in De Christiana expeditione apud Sinas (published 1615), annotated by Henry Yule).\n\n^ Waldron 1990, pp.\u00a02\u20134.\n\n^ a b Waldron 1990, p.\u00a0206.\n\n^ Waldron 1990, p.\u00a0209.\n\n^ Hessler 2007, p.\u00a060.\n\n^ a b \"Great Wall of China 'even longer'\". BBC. April 20, 2009. Retrieved April 20, 2009.\n\n^ Rojas 2010, p.\u00a0140.\n\n^ Askhar, Aybek. \"Limit placed on number of visitors to Great Wall\". China Daily. Retrieved August 10, 2020.\n\n^ Lindesay 2008, p.\u00a0212.\n\n^ \"Jiaoshan Great Wall\". TravelChinaGuide.com. Retrieved September 15, 2010. Jiaoshan Great Wall is located about 3\u00a0km (2\u00a0mi) from Shanhaiguan ancient city. It is named after Jiaoshan Mountain, which is the highest peak to the north of Shanhai Pass and also the first mountain the Great Wall climbs",
    "chunk_id": "45e9c4a8746ef74104e1c163f507ae88",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "76b1071055a40b3f857eafefdebd59aa",
    "chunk": " 10, 2020.\n\n^ Lindesay 2008, p.\u00a0212.\n\n^ \"Jiaoshan Great Wall\". TravelChinaGuide.com. Retrieved September 15, 2010. Jiaoshan Great Wall is located about 3\u00a0km (2\u00a0mi) from Shanhaiguan ancient city. It is named after Jiaoshan Mountain, which is the highest peak to the north of Shanhai Pass and also the first mountain the Great Wall climbs up after Shanhai Pass. Therefore Jiaoshan Mountain is noted as \"The first mountain of the Great Wall\".\n\n^ \"Great Wall of China longer than believed as 180 missing miles found\". The Guardian. Associated Press. April 20, 2009. Retrieved April 18, 2015.\n\n^ \"Newly-discovered remains redraw path of Great Wall\". China Daily. April 15, 2015. Archived from the original on April 18, 2015. Retrieved April 18, 2015.\n\n^ a b Turnbull 2007, p.\u00a029.\n\n^ \"Sticky rice porridge and the Great Wall of China\". World Archaeology. July 6, 2010. Retrieved July 6, 2022.\n\n^ Boissoneault, Lorraine (February 16, 2017). \"Sticky Rice Mortar, the View From Space, and More Fun Facts About China's Great Wall\". Smithsonian. Retrieved July 6, 2022.\n\n^ Nanos, Janelle (November 12, 2010). \"Slide Down the Great Wall of China\". National Geographic. Retrieved July 6, 2022. [...] (in fact, there have been no bones, human or otherwise, found in the Wall, though a great number of workers did die while toiling to build it).\n\n^ Horsford, Simon (February 17, 2017). \"Five myths about the Great Wall of China\". The Telegraph. ISSN\u00a00307-1235. Archived from the original on February 20, 2017. Retrieved July 6, 2022. No bones or indeed other indication of human remains have been found in the Wall.\n\n^ Ford, Peter (November 30, 2006). New law to keep China's Wall looking great. Christian Science Monitor, Asia Pacific section. Retrieved March 17, 2007.\n\n^ a b Wong, Edward (June 29, 2015). \"China Fears Loss of Great Wall, Brick by Brick\". The New York Times. Retrieved July 1, 2015.\n\n^ Bruce G. Doar: The Great Wall of China: Tangible, Intangible and Destructible. China Heritage Newsletter, China Heritage Project, Australian National University\n\n^ \"China's Wall becoming less and less Great\". Reuters. August 29, 2007. Retrieved August 30, 2007.\n\n",
    "chunk_id": "76b1071055a40b3f857eafefdebd59aa",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "1a82e565d2e93603410f831e27603113",
    "chunk": "June 29, 2015). \"China Fears Loss of Great Wall, Brick by Brick\". The New York Times. Retrieved July 1, 2015.\n\n^ Bruce G. Doar: The Great Wall of China: Tangible, Intangible and Destructible. China Heritage Newsletter, China Heritage Project, Australian National University\n\n^ \"China's Wall becoming less and less Great\". Reuters. August 29, 2007. Retrieved August 30, 2007.\n\n^ Ben Westcott; Serenitie Wang (September 21, 2016). \"China's Great Wall covered in cement\". CNN.\n\n^ \"China's Great Wall damaged by workers looking for shortcut\". September 5, 2023 \u2013 via www.bbc.co.uk.\n\n^ \"NASA - China's Wall Less Great in View from Space\". www.nasa.gov. Retrieved June 7, 2021.\n\n^ Urban Legends.com website Archived March 3, 2012, at the Wayback Machine. Accessed May 12, 2010.\"Can you see the Great Wall of China from the moon or outer space? Archived May 11, 2013, at the Wayback Machine\", Answers.com. Accessed May 12, 2010.Cecil Adams, \"Is the Great wall of China the only manmade object byou can see from space?\", The Straight Dope. Accessed May 12, 2010.Snopes, \"Great wall from space\", last updated July 21, 2007. Accessed May 12, 2010.\"Is China's Great Wall Visible from Space?\", Scientific American, February 21, 2008. \"...\u00a0the wall is only visible from low orbit under a specific set of weather and lighting conditions. And many other structures that are less spectacular from an earthly vantage point\u2014desert roads, for example\u2014appear more prominent from an orbital perspective.\"\n\n^ \"Metro Tescos\", The Times (London), April 26, 2010. Found at The Times website. Accessed May 12, 2010.\n\n^ a b c L\u00f3pez-Gil 2008, pp.\u00a03\u20134.\n\n^ The Family Memoirs of the Rev. William Stukeley (1887) Vol. 3, p. 142. (1754).\n\n^ Norman, Henry, The Peoples and Politics of the Far East, p. 215. (1895).\n\n^ \"\"The Great Wall of China\", Ripley's Believe It or Not!, 1932.\n\n^ \"NASA \u2013 Great Wall of China\". Nasa.gov. Retrieved July 31, 2010.\n\n^ \"People's Daily Online -- ESA admits \"Great Wal\" on satellite photo a mistake\". Archived from the original on September 4, 2022. Retrieved September 4, 2022.\n\n^ Markus, Francis. (April 19, 2005). Great Wall visible in space photo",
    "chunk_id": "1a82e565d2e93603410f831e27603113",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "eefa6c0fba0dd613d0d86189900825b4",
    "chunk": " Great Wall of China\", Ripley's Believe It or Not!, 1932.\n\n^ \"NASA \u2013 Great Wall of China\". Nasa.gov. Retrieved July 31, 2010.\n\n^ \"People's Daily Online -- ESA admits \"Great Wal\" on satellite photo a mistake\". Archived from the original on September 4, 2022. Retrieved September 4, 2022.\n\n^ Markus, Francis. (April 19, 2005). Great Wall visible in space photo. BBC News, Asia-Pacific section. Retrieved March 17, 2007.\n\n\nReferences\n\nEdmonds, Richard Louis (1985). Northern Frontiers of Qing China and Tokugawa Japan: A Comparative Study of Frontier Policy. University of Chicago, Department of Geography; Research Paper No. 213. ISBN\u00a0978-0-89065-118-6.\nElliott, Mark C. (2001). The Manchu Way: The Eight Banners and Ethnic Identity in Late Imperial China. Stanford University Press. ISBN\u00a0978-0-8047-4684-7.\nEvans, Thammy (2006). Great Wall of China: Beijing & Northern China. Bradt Travel Guide. Bradt Travel Guides. p.\u00a03. ISBN\u00a0978-1-84162-158-6.\nHaw, Stephen G. (2006). Marco Polo's China: a Venetian in the realm of Khubilai Khan. Volume 3 of Routledge studies in the early history of Asia. Psychology Press. ISBN\u00a0978-0-415-34850-8.\nHessler, Peter (2007). \"Letter from China: Walking the Wall\". The New Yorker. No.\u00a0May 21, 2007. pp.\u00a058\u201367.\nKarnow, Catherine; Mooney, Paul (2008). National Geographic Traveler: Beijing. National Geographic Books. p.\u00a0192. ISBN\u00a0978-1-4262-0231-5.\nLindesay, William (2008). The Great Wall Revisited: From the Jade Gate to Old Dragon's Head. Harvard University Press. ISBN\u00a0978-0-674-03149-4.\nL\u00f3pez-Gil, Norberto (2008). \"Is it Really Possible to See the Great Wall of China from Space with a Naked Eye?\" (PDF). Journal of Optometry. 1 (1): 3\u20134. doi:10.3921\/joptom.2008.3. ISSN\u00a01989-1342. PMC\u00a03972694. Archived from the original (PDF) on September 10, 2008.\nLovell, Julia (2006). The Great Wall\u00a0: China against the world 1000 BC \u2013 AD 2000. Sydney: Picador Pan Macmillan. ISBN\u00a0978-0-330-42241-3.\nRojas",
    "chunk_id": "eefa6c0fba0dd613d0d86189900825b4",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "94ed9de059c73d08af5ea3b0737ac798",
    "chunk": "\u20134. doi:10.3921\/joptom.2008.3. ISSN\u00a01989-1342. PMC\u00a03972694. Archived from the original (PDF) on September 10, 2008.\nLovell, Julia (2006). The Great Wall\u00a0: China against the world 1000 BC \u2013 AD 2000. Sydney: Picador Pan Macmillan. ISBN\u00a0978-0-330-42241-3.\nRojas, Carlos (2010). The Great Wall\u00a0: a cultural history. Cambridge, Massachusetts: Harvard University Press. ISBN\u00a0978-0-674-04787-7.\nSlavicek, Louise Chipley; Mitchell, George J.; Matray, James I. (2005). The Great Wall of China. Infobase Publishing. p.\u00a035. ISBN\u00a0978-0-7910-8019-1.\nSzab\u00f3, J\u00f3zsef; D\u00e1vid, L\u00f3r\u00e1nt; Loczy, Denes, eds. (2010). Anthropogenic Geomorphology: A Guide to Man-made Landforms. Springer. ISBN\u00a0978-90-481-3057-3.\nTurnbull, Stephen R (January 2007). The Great Wall of China 221 BC\u2013AD 1644. Osprey Publishing. ISBN\u00a0978-1-84603-004-8.\nWaldron, Arthur (1983). \"The Problem of The Great Wall of China\". Harvard Journal of Asiatic Studies. 43 (2): 643\u2013663. doi:10.2307\/2719110. JSTOR\u00a02719110.\nWaldron, Arthur (1988). \"The Great Wall Myth: Its Origins and Role in Modern China\". The Yale Journal of Criticism. 2 (1): 67\u2013104.\nWaldron, Arthur (1990). The Great Wall of China: from history to myth. Cambridge England New York: Cambridge University Press. ISBN\u00a0978-0-521-42707-4.\nYule, Sir Henry, ed. (1866). Cathay and the way thither: being a collection of medieval notices of China. Issues 36\u201337 of Works issued by the Hakluyt Society. Printed for the Hakluyt society.\n\nFurther reading\n\nArnold, H. J. P., \"The Great Wall: Is It or Isn't It?\" Astronomy Now, 1995.\nBeckwith, Christopher I. (2009): Empires of the Silk Road: A History of Central Eurasia from the Bronze Age to the Present. Princeton: Princeton University Press. ISBN\u00a0978-0-691-13589-2.\nLuo, Zewen, et al. and Baker, David, ed. (1981). The Great Wall. Maidenhead: McGraw-Hill Book",
    "chunk_id": "94ed9de059c73d08af5ea3b0737ac798",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "1ff2a8a84299df08b19fa4373cb4bd8a",
    "chunk": " Wall: Is It or Isn't It?\" Astronomy Now, 1995.\nBeckwith, Christopher I. (2009): Empires of the Silk Road: A History of Central Eurasia from the Bronze Age to the Present. Princeton: Princeton University Press. ISBN\u00a0978-0-691-13589-2.\nLuo, Zewen, et al. and Baker, David, ed. (1981). The Great Wall. Maidenhead: McGraw-Hill Book Company (UK). ISBN\u00a00-07-070745-6\nMan, John. (2008). The Great Wall. London: Bantam Press. 335 pages. ISBN\u00a0978-0-593-05574-8.\nMichaud, Roland and Sabrina (photographers), & Michel Jan, The Great Wall of China. Abbeville Press, 2001. ISBN\u00a00-7892-0736-2\nSchafer, Edward H. (1985). The Golden Peaches of Samarkand. Berkeley: University of California Press. ISBN\u00a0978-0-520-05462-2.\nYamashita, Michael; Lindesay, William (2007). The Great Wall\u00a0\u2013 From Beginning to End. New York: Sterling. 160 pages. ISBN\u00a0978-1-4027-3160-0.\n\nExternal links\n\n\nGreat Wall of China  at Wikipedia's sister projects\n\nDefinitions from WiktionaryMedia from CommonsNews from WikinewsQuotations from WikiquoteTexts from WikisourceTextbooks from WikibooksResources from WikiversityTravel information from Wikivoyage\n\nInternational Friends of the Great Wall Archived February 17, 2009, at the Wayback Machine\u00a0\u2013 organization focused on conservation\nUNESCO World Heritage Centre profile\nEnthusiast\/scholar website (in Chinese)\nGreat Wall of China on In Our Time at the BBC\nPhotoset of lesser visited areas of the Great Wall\n Geographic data related to Great Wall of China at OpenStreetMap\nvteGreat Wall of ChinaHistoryMing Great WallOuter Wall (W to E)\nJiayu Pass\nJumenbu\nBadaling\nHuanghuacheng\nJiankou (Beijing Knot)\nMutianyu\nBaimaguan\nGubeikou\nJinshanling\nSimatai\nHuangya Pass\nShanhai Pass\nHushan\nInner Wall (W to E)\nGuangwu (New Guangwu)\nYanmen Pass\nPingxing Pass\nJuyong Pass\nOther\nNiangzi Pass (Hebei\/Shanxi border spur)\nMiaojiang Great Wall (\"Southern Great Wall\")\nNine Garrisons\nLiaodong\nJi\nXuanfu\nDatong\nTaiyuan\nYansui\nNingxia\nGuyuan\nGansu\nOther time periodsW",
    "chunk_id": "1ff2a8a84299df08b19fa4373cb4bd8a",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "e72a340d7101a18981fe9e7f77ba3452",
    "chunk": "Hushan\nInner Wall (W to E)\nGuangwu (New Guangwu)\nYanmen Pass\nPingxing Pass\nJuyong Pass\nOther\nNiangzi Pass (Hebei\/Shanxi border spur)\nMiaojiang Great Wall (\"Southern Great Wall\")\nNine Garrisons\nLiaodong\nJi\nXuanfu\nDatong\nTaiyuan\nYansui\nNingxia\nGuyuan\nGansu\nOther time periodsWarring States (475\u2013221 BC)\nGreat Wall of Qi\nWestern Han dynasty (202 BC\u20139 AD)\nYumen Pass\nYang Pass\nYuan dynasty (1271-1368)\nCloud Platform\nQing dynasty (1644-1912)\nDajingmen\n\nvteNew 7 Wonders of the World\nGreat Wall of China\nPetra\nColosseum\nChichen Itza\nMachu Picchu\nTaj Mahal\nChrist the Redeemer\nHonorary status: Great Pyramid of Giza\nvteMountain passes of ChinaGeography of ChinaOn the border\u00a0Kazakhstan\nDzungarian Gate\n\u00a0Kyrgyzstan\nBedel Pass\nIrkeshtam Pass\nTorugart Pass\n\u00a0Tajikistan\nBeyik Pass\nKulma Pass\nNezatash Pass\n\u00a0Afghanistan\nWakhjir Pass\nTegermansu Pass\n\u00a0Pakistan\nKhunjerab Pass\nKilik Pass\nMintaka Pass\nMustagh Pass\n\u00a0India\nBum La Pass\nCho La\nDiphu Pass\nDongkha La\nIndira Col\nJelep La\nKarakoram Pass\nKepang La\nKongka Pass\nLipulekh Pass\nMana Pass\nNathu La\nRezang La\nShipki La\nSpanggur Gap\nTulung La\n\u00a0\u00a0\u00a0Nepal\nKorala\nLho La\nNangpa La\nSouth Col\n\u00a0Myanmar\nDiphu Pass\nIsu Razi Pass\n\u00a0Vietnam\nFriendship Pass\nIn the interiorOn the Great Wall\nDajingmen\nGubeikou\nJiayu Pass\nHuangya Pass\nJingxing Pass\nJuyong Pass\nNiangzi Pass\nPingxing Pass (Pingxingguan)\nShanhai Pass (Shanhaiguan)\nYang Pass\nYanmen Pass\nYumen Pass\nGuanzhong area\nDasan Pass (Sanguan)\nTong Pass (Tongguan)\nWu Pass\nXinjiang area\nDehra La\nHindutash\nIron Gate Pass\nKirgizjangal Pass (Heiqia)\nMazar Pass\nMuzart Pass\nSanju Pass\nTibet area\nCherko la\nLaken La\n",
    "chunk_id": "e72a340d7101a18981fe9e7f77ba3452",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "aa4d7c89cb959dd44e625273dc2b7739",
    "chunk": "anhaiguan)\nYang Pass\nYanmen Pass\nYumen Pass\nGuanzhong area\nDasan Pass (Sanguan)\nTong Pass (Tongguan)\nWu Pass\nXinjiang area\nDehra La\nHindutash\nIron Gate Pass\nKirgizjangal Pass (Heiqia)\nMazar Pass\nMuzart Pass\nSanju Pass\nTibet area\nCherko la\nLaken La\nLanak La\nLhagba La\nMayum La\nNorth Col\nSemo La\nTanggula Pass\nOther\nBiandukou\nCho La, Sichuan\nHangu Pass\nHulao Pass\nJianmen Pass\nKunlun Pass\nLazikou Pass\nMei Pass\n\nLandforms of China\nList of gaps in Hong Kong\n\nvteWorld Heritage Sites in ChinaEast\nArchaeological Ruins of Liangzhu City\nClassical Gardens of Suzhou\nFujian tulou\nLushan\nHuangshan\nMigratory Bird Sanctuaries along the Coast of Yellow Sea\u2013Bohai Sea of China (Phase I)\nMount Sanqing\nMount Tai\nWuyi Mountains\nTemple and Cemetery of Confucius and Kong Family Mansion in Qufu\nXidi, and Hongcun\nWest Lake\nKulangsu\nSouth Central\nAncient Building Complex in the Wudang Mountains\nHistoric Centre of Macau\nShennongjia\nKaiping Diaolou and Villages\nLongmen Grottoes\nHistoric Monuments of Dengfeng, including the Shaolin Monastery and Gaocheng Observatory\nZuojiang Huashan Rock Art\nWulingyuan\nYinxu\nSouthwestern\nChengjiang Fossil Site\nDazu Rock Carvings\nFanjingshan\nHonghe Hani Rice Terraces\nHuanglong\nJiuzhaigou\nLijiang\nMount Emei and Leshan Giant Buddha\nMount Qingcheng and Dujiangyan\nPotala Palace, including the Jokhang and Norbulingka\nSichuan Giant Panda Sanctuaries\nThree Parallel Rivers\nNorth\nMount Wutai\nChengde Mountain Resort, including Putuo Zongcheng Temple, Xumi Fushou Temple and Puning Temple\nForbidden City\nZhoukoudian\nAncient City of Pingyao\nSummer Palace\nTemple of Heaven\nXanadu\nYungang Grottoes\nBeijing Central Axis\nNortheast\nKoguryo sites\nMukden Palace\nNorthwestern\nMausoleum of the First Qin Emperor\nMogao Caves\nXinjiang Tianshan\nQinghai Hoh Xil\nMultiple regions\nGreat Wall\nChina Dan",
    "chunk_id": "aa4d7c89cb959dd44e625273dc2b7739",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "2a1acab394b99bfbcd70e6f1176fa6a2",
    "chunk": "\nForbidden City\nZhoukoudian\nAncient City of Pingyao\nSummer Palace\nTemple of Heaven\nXanadu\nYungang Grottoes\nBeijing Central Axis\nNortheast\nKoguryo sites\nMukden Palace\nNorthwestern\nMausoleum of the First Qin Emperor\nMogao Caves\nXinjiang Tianshan\nQinghai Hoh Xil\nMultiple regions\nGreat Wall\nChina Danxia\nImperial Tombs of the Ming and Qing Dynasties\nSouth China Karst\nGrand Canal\nSilk Roads\nTusi Sites\nMigratory Bird Sanctuaries along the Coast of Yellow Sea\u2013Bohai Sea of China (Phase II)\nBadain Jaran Desert\n\nPortals: China History\nAuthority control databases InternationalVIAF2NationalGermanyUnited StatesFranceBnF dataJapanCzech RepublicIsraelGeographicStructuraeOtherIdRef\n\n\n\n\nRetrieved from \"https:\/\/en.wikipedia.org\/w\/index.php?title=Great_Wall_of_China&oldid=1243739333\"\nCategories: Great Wall of China7th-century BC establishments in ChinaBorder barriersChinese architectural historyFortification linesQin Shi HuangWallsWorld Heritage Sites in ChinaNational symbols of ChinaHidden categories: Pages using gadget WikiMiniAtlasCS1 uses Chinese-language script (zh)CS1 Chinese-language sources (zh)Webarchive template wayback linksArticles with short descriptionShort description is different from WikidataWikipedia indefinitely move-protected pagesWikipedia pages semi-protected against vandalismUse mdy dates from September 2022Coordinates on WikidataArticles containing Chinese-language textArticles containing traditional Chinese-language textArticles containing simplified Chinese-language textPages using Sister project links with hidden wikidataPages using Sister project links with default searchArticles with Chinese-language sources (zh)\n\n\n\n\n\n\n This page was last edited on 3 September 2024, at 04:24\u00a0(UTC).\nText is available under the Creative Commons Attribution-ShareAlike License 4.0;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n\n\nPrivacy policy\nAbout Wikipedia\nDisclaimers\nContact Wikipedia\nCode of Conduct\nDevelopers\nStatistics\nCookie statement\nMobile view\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "chunk_id": "2a1acab394b99bfbcd70e6f1176fa6a2",
    "document_ids": [
        "e033f2c8df36e5e0b799e41549ea0b6c"
    ],
    "n_tokens": 485,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "9741c3b3496aa897eeba6790660895b6",
    "chunk": "Article\nHa Long Bay - Wikipedia\n\nJump to content\n\nMain menu\n1 Etymology\n\n2 Overview\n\n2.1 Location\n\n2.2 Climate\n\n2.3 Population\n\n3 History\n\n3.1 Soi Nhu culture (16,000\u20135000 BC)\n\n3.2 Cai Beo culture (5000\u20133000 BC)\n\n3.3 Ha Long culture (2500\u20131500 BC)\n\n3.4 Classical period\n\n3.5 Modern period\n\n4 Geology and geomorphology\n\n4.1 Karst geomorphology value\n\n4.2 Timeline of geologic evolution\n\n5 Ecology\n\n5.1 Environmental damage\n\n6 Awards and designations\n\n7 Popular culture\n\n7.1 Literature\n\n7.2 Ancient tales\n\n8 Conservation Issues\n\n8.1 Impacts of Human and Natural Factors on the Bay Area\n\n8.2 Conservation Efforts\n\nHa Long Bay\n\n\n\n65 languages\n\nArticleTalk\n\nCoordinates: 20\u00b054\u2032N 107\u00b012\u2032E\ufeff \/ \ufeff20.9\u00b0N 107.2\u00b0E\ufeff \/ 20.9; 107.2\n\nFrom Wikipedia, the free encyclopedia\n\n\nUNESCO World Heritage Site in Vietnam\n\n\nHa Long Bay and Cat Ba IslandsUNESCO World Heritage SiteLocationVietnamCriteriaNatural: vii, viii, ix, xReference672Inscription1994 (18th Session)Extensions2000, 2023Coordinates20\u00b054\u2032N 107\u00b012\u2032E\ufeff \/ \ufeff20.9\u00b0N 107.2\u00b0E\ufeff \/ 20.9; 107.2Location of Ha Long Bay in Vietnam\n\n\n\nThis article contains Vietnamese text. Without proper rendering support, you may see question marks, boxes, or other symbols instead of ch\u1eef N\u00f4m, ch\u1eef H\u00e1n and ch\u1eef Qu\u1ed1c ng\u1eef.\n\nHa Long Bay or Halong Bay (Vietnamese: V\u1ecbnh Ha Long, IPA: [v\u00een\u02c0 h\u00e2\u02d0\u02c0 law\u014bm] \u24d8) is a UNESCO World Heritage Site and popular travel destination in Qu\u1ea3ng Ninh province, Vietnam. The name Ha Long means \"descending dragon\". Administratively, the bay belongs to Ha Long city, C\u1ea9m Ph\u1ea3 city, and is a part of V\u00e2n \u0110\u1ed3n district. The bay features thousands of limestone karsts and islets in various shapes and sizes. Ha Long Bay is a center of a larger zone that includes Bai Tu Long Bay to the northeast, and Cat Ba Island to the southwest. These larger zones share a similar geological, geographical, geomorphological, climate, and cultural characters.\nHa Long Bay has an area of around 1,553\u00a0km2 (600\u00a0sq\u00a0mi), including 1,969 islets, most of which are limestone. The core of the bay has an area of 334\u00a0km2 (129\u00a0sq\u00a0mi) with a high",
    "chunk_id": "9741c3b3496aa897eeba6790660895b6",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "e528ce85710450643e9173031a7e0fd6",
    "chunk": " larger zone that includes Bai Tu Long Bay to the northeast, and Cat Ba Island to the southwest. These larger zones share a similar geological, geographical, geomorphological, climate, and cultural characters.\nHa Long Bay has an area of around 1,553\u00a0km2 (600\u00a0sq\u00a0mi), including 1,969 islets, most of which are limestone. The core of the bay has an area of 334\u00a0km2 (129\u00a0sq\u00a0mi) with a high density of 775 islets.[1] The limestone in this bay has gone through 500 million years of formation in different conditions and environments. The evolution of the karst in this bay has taken 20 million years under the impact of the tropical wet climate.[2] The geo-diversity of the environment in the area has created biodiversity, including a tropical evergreen biosystem and a seashore biosystem.[3] Ha Long Bay is home to 14 endemic floral species[4] and 60 endemic faunal species.[5]\nHistorical research surveys have shown the presence of prehistoric human beings in this area tens of thousands years ago. The successive ancient cultures are the Soi Nhu culture around 18,000\u20137,000 BC, the Cai Beo culture 7,000\u20135,000 BC[6] and the Ha Long culture 5,000\u20133,500 years ago.[7] Ha Long Bay also marked some important events in Vietnamese history, with many artifacts found in B\u00e0i Th\u01a1 mountain, \u0110\u1ea7u G\u1ed7 cave, and B\u00e3i Ch\u00e1y.[7]\nNguy\u1ec5n Tr\u00e3i praised the beauty of Ha Long Bay 500 years ago in his verse L\u1ed9 nh\u1eadp V\u00e2n \u0110\u1ed3n, in which he called it \"a rock wonder in the sky\".[8] In 1962, the Ministry of Culture, Sports and Tourism of North Vietnam listed Ha Long Bay in the National Relics and Landscapes publication.[9] In 1994, the core zone of Ha Long Bay was listed as a World Heritage Site under Criterion VII, and was listed for a second time under Criterion VIII.[10]\n\n\nEtymology[edit]\nThe name Ha Long (ch\u1eef H\u00e1n: \u4e0b\u9f8d) means \"descending dragon\".\nBefore the 19th century, the name Ha Long Bay had not been recorded in the old books of the country. It has been called other names such as An Bang, L\u1ee5c Th\u1ee7y, and V\u00e2n \u0110\u1ed3n. In the late 19th century, the name Ha Long Bay appeared on the Maritime Map of France. The French-language Hai Phong News reported \"Dragon appears on Ha Long Bay\".\n\nDrone shot of limestone cliffs\nAccording to local legend, when Vietnam had just started to develop into a country, they had to fight against invaders. To assist the Vietnamese in defending their country, the gods sent a family of",
    "chunk_id": "e528ce85710450643e9173031a7e0fd6",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "b49d3abf54e7a19ec53d758cf3b7bc99",
    "chunk": " such as An Bang, L\u1ee5c Th\u1ee7y, and V\u00e2n \u0110\u1ed3n. In the late 19th century, the name Ha Long Bay appeared on the Maritime Map of France. The French-language Hai Phong News reported \"Dragon appears on Ha Long Bay\".\n\nDrone shot of limestone cliffs\nAccording to local legend, when Vietnam had just started to develop into a country, they had to fight against invaders. To assist the Vietnamese in defending their country, the gods sent a family of dragons as protectors. This family of dragons began spitting out jewels and jade. These jewels turned into the islands and islets dotting the bay, linking together to form a great wall against the invaders. Under magics, numerous rock mountains abruptly appeared on the sea, ahead of invaders' ships; the forward ships struck the rocks and each other. After winning the battle, the dragons were interested in peaceful sightseeing of the Earth, and then decided to live in this bay. The place where the mother dragon descended was named Ha Long, the place where the dragon's children attended upon their mother was called B\u00e1i T\u1eed Long island (B\u00e1i: attend upon, T\u1eed: children, Long: dragon), and the place where the dragon's children wriggled their tails violently was called B\u1ea1ch Long V\u0129 island (B\u1ea1ch: white-color of the foam made when Dragon's children wriggled, Long: dragon, V\u0129: tail), present-day Tra Co peninsula, M\u00f3ng C\u00e1i.[11]\n\nOverview[edit]\nThi\u00ean Cung grotto\nThe bay consists of a dense cluster of some 1,600[12] limestone monolithic islands each topped with thick jungle vegetation, rising spectacularly from the ocean. Several of the islands are hollow, with enormous caves. Hang Dau Go (Wooden Stakes cave) is the largest grotto in the Ha Long area. French tourists visited in the late 19th century, and named the cave Grotte des Merveilles. Its three large chambers contain large numerous stalactites and stalagmites (as well as 19th-century French graffiti). There are two bigger islands, Tu\u1ea7n Ch\u00e2u and Cat Ba, that have permanent inhabitants, as well as tourist facilities including hotels and beaches. There are a number of beautiful beaches on the smaller islands.\n\nFisherman's house\nA community of around 1,600 people live on Ha Long Bay in four fishing villages: Cua Van, Ba Hang, Cong Tau and Vong Vieng in Hung Thang ward, Ha Long city. They live on floating houses and are sustained through fishing and marine aquaculture (cultivating marine biota), plying the shallow waters for 200 species of fish and 450 different kinds of mollusks. Many of the islands have acquired their names as a result of their unusual shapes. Such names include Voi Islet (",
    "chunk_id": "b49d3abf54e7a19ec53d758cf3b7bc99",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "1d5e457d69bea91f48a67251537de41a",
    "chunk": " on Ha Long Bay in four fishing villages: Cua Van, Ba Hang, Cong Tau and Vong Vieng in Hung Thang ward, Ha Long city. They live on floating houses and are sustained through fishing and marine aquaculture (cultivating marine biota), plying the shallow waters for 200 species of fish and 450 different kinds of mollusks. Many of the islands have acquired their names as a result of their unusual shapes. Such names include Voi Islet (elephant), Ga Choi Islet (fighting cock), Khi Islet (monkey), and Mai Nha Islet (roof). 989 of the islands have been given names. Birds and land animals including bantams, antelopes, monkeys, and lizards also live on some of the islands.\n\nFloating fishing village\nAlmost all these islands are as individual towers in a classic fenglin landscape with heights ranging from 50\u2013100 metres (160\u2013330\u00a0ft), and height\/width ratios of up to about six.[clarification needed]\nAnother specific feature of Ha Long Bay is the abundance of lakes inside the limestone islands. For example, Dau Be island has six enclosed lakes. All these island lakes occupy drowned dolines within fengcong karst.\n\nLocation[edit]\nHa Long Bay is located in northeastern Vietnam, from E106\u00b055' to E107\u00b037' and from N20\u00b043' to N21\u00b009'. The bay stretches from Quang Yen town, past Ha Long city, C\u1ea9m Ph\u1ea3 city to V\u00e2n \u0110\u1ed3n District, is \nbordered on the south and southeast by Lan Ha Bay, on the north by Ha Long city, and on the west by Bai Tu Long Bay. The bay has a 120-kilometre-long (75\u00a0mi) coastline and is approximately 1,553\u00a0km2 (600\u00a0sq\u00a0mi) in size with about 2,000 islets. The area designated by UNESCO as the World Natural Heritage Site incorporates 434\u00a0km2 (168\u00a0sq\u00a0mi) with 775 islets, of which the core zone is delimited by 69 points: Dau Go island on the west, Ba Ham lake on the south and Cong Tay island on the east. The protected area is from the C\u00e1i D\u0103m petrol store to Quang Hanh ward, C\u1ea9m Ph\u1ea3 city and the surrounding zone.\n\nClimate[edit]\nThe climate of the bay is tropical, wet, sea islands, with two seasons: hot and moist summer, dry and cold winter. The average temperature is from 15\u201325\u00a0\u00b0C (59\u201377\u00a0\u00b0F), and annual rainfall is between 2 and 2.2 metres (6.6 and 7.2\u00a0ft). Ha Long Bay has the typical diurnal tide system (tide amplitude ranges from 3.5\u20134 metres or 11\u201313 feet).",
    "chunk_id": "1d5e457d69bea91f48a67251537de41a",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "0f062f0d93047efc4b537717387000d6",
    "chunk": "The climate of the bay is tropical, wet, sea islands, with two seasons: hot and moist summer, dry and cold winter. The average temperature is from 15\u201325\u00a0\u00b0C (59\u201377\u00a0\u00b0F), and annual rainfall is between 2 and 2.2 metres (6.6 and 7.2\u00a0ft). Ha Long Bay has the typical diurnal tide system (tide amplitude ranges from 3.5\u20134 metres or 11\u201313 feet). The salinity is from 31 to 34.5MT in the dry season and lower in the rainy season.\n\nPopulation[edit]\nOf the 1,969 islands in Ha Long, only approximately 40 are inhabited. These islands range from tens to thousands of hectares in size, mainly in the East and Southeast of Ha Long Bay. In recent decades, thousands of villagers have been starting to settle down on the pristine islands and build new communities such as Sa T\u00f4 Island (Ha Long City) and Th\u1eafng L\u1ee3i Island (V\u00e2n \u0110\u1ed3n district).\n\nL\u00e0ng Ch\u00e0i Floating Fishing Village\nThe population of Ha Long Bay is about 1,540, mainly in C\u1eeda V\u1ea1n, Ba Hang and C\u1eb7p D\u00e8 fishing villages (H\u00f9ng Th\u1eafng Ward, Ha Long City). Residents of the bay mostly live on boats and rafts buoyed by tires and plastic jugs to facilitate the fishing, cultivating and breeding of aquatic and marine species. Fish require feeding every other day for up to three years, when they are eventually sold to local seafood restaurants for up to 300,000 Vietnamese dong per kilogram. Today, the lives of Ha Long Bay inhabitants have much improved due to new travel businesses. Residents of the floating villages around Ha Long Bay now offer bedrooms for rent, boat tours, and fresh seafood meals to tourists. While this is an isolating, back-breaking lifestyle, floating village residents are considered wealthy compared to residents of other Ha Long Bay islands.\nAt present [when?], the Qu\u1ea3ng Ninh provincial government has a policy to relocate the households living in the bay to resettle, in order to stabilize their life and to protect the landscape of the heritage zone. More than 300 households living in fishing villages in Ha Long Bay have been relocated ashore in Khe C\u00e1 Resettlement Area, now known as Zone 8 (H\u00e0 Phong Ward, Ha Long City) since May 2014. This project will continue to be implemented. The province will only retain a number of fishing villages for sightseeing tours.\n\nHistory[edit]\nStatue of Buddha inside a cave, erected in the 16\u201317th century\nSoi Nhu culture (16,000\u20135000 BC)[edit]\nLocated within Ha Long and B\u00e1i T\u1eed Long are archaeological sites such as M\u00ea Cung and Thi\u00ean Long. There are remains from mounds of mountain shellfish (Cyclo",
    "chunk_id": "0f062f0d93047efc4b537717387000d6",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "6afc24f47e574165d511f4ac2f6e3877",
    "chunk": " This project will continue to be implemented. The province will only retain a number of fishing villages for sightseeing tours.\n\nHistory[edit]\nStatue of Buddha inside a cave, erected in the 16\u201317th century\nSoi Nhu culture (16,000\u20135000 BC)[edit]\nLocated within Ha Long and B\u00e1i T\u1eed Long are archaeological sites such as M\u00ea Cung and Thi\u00ean Long. There are remains from mounds of mountain shellfish (Cyclophorus), spring shellfish (Melania, also called Thiana), some freshwater water mollusc and some rudimentary labour tools. The main way of life of Soi Nhu's inhabitants included catching fish and shellfish, collecting fruits and digging for bulbs and roots. Their living environment was a coastal area unlike other Vietnamese cultures, for example, those found in H\u00f2a B\u00ecnh and B\u1eafc S\u01a1n.\n\nCai Beo culture (5000\u20133000 BC)[edit]\nLocated in Ha Long and Cat Ba island its inhabitants developed to the level of sea exploitation. Cai Beo culture is a link between Soi Nhu culture and Ha Long culture.[13]\n\nHa Long culture (2500\u20131500 BC)[edit]\nThis section needs expansion. You can help by adding to it.  (October 2019)\nClassical period[edit]\nSee also: Battle of B\u1ea1ch \u0110\u1eb1ng (1288)\nHa Long Bay was the setting for historical naval battles against Vietnam's coastal neighbors. On three occasions, in the labyrinth of channels in B\u1ea1ch \u0110\u1eb1ng River near the islands, the Vietnamese army stopped the Chinese invaders from landing. In 1288, General Tr\u1ea7n H\u01b0ng \u0110\u1ea1o stopped Mongol ships from sailing up the nearby B\u1ea1ch \u0110\u1eb1ng River by placing steel-tipped wooden stakes at high tide, sinking the Mongol Kublai Khan's fleet.\n\nModern period[edit]\nHa Long Bay was the site of the first ever raising of the new national flag of the Provisional Central Government of Vietnam on 5 June 1948 during the signing of the Halong Bay Agreements (Accords de la baie d\u2019Along) by High Commissioner Emile Bollaert and President Nguy\u1ec5n V\u0103n Xu\u00e2n.[14]\nDuring the Vietnam War, many of the channels between the islands were heavily mined by the United States Navy, some of which continue to pose threats to shipping routes in the present day.\n\nGeology and geomorphology[edit]\nKarst formations\nIn 2000, UNESCO's World Heritage Committee inscribed Ha Long Bay in the World Heritage List according to its outstanding examples representing major stages of the Earth's history and its original limestone karstic geomorphologic features. Ha Long Bay and its adjacent areas consist of a part of the Sino-Vietnamese composite terrane having its development history from pre-Cambrian up to",
    "chunk_id": "6afc24f47e574165d511f4ac2f6e3877",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [
        {
            "name": "\"HA LONG BAY\"",
            "type": "\"GEO\"",
            "description": "\"Ha Long Bay is a UNESCO World Heritage Site known for its limestone karstic geomorphologic features and historical significance.\"",
            "source_id": "6afc24f47e574165d511f4ac2f6e3877"
        }
    ],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"&quot;HA LONG BAY&quot;\">      <data key=\"d0\">\"GEO\"<\/data>      <data key=\"d1\">\"Ha Long Bay is a UNESCO World Heritage Site known for its limestone karstic geomorphologic features and historical significance.\"<\/data>      <data key=\"d2\">6afc24f47e574165d511f4ac2f6e3877<\/data>    <\/node>  <\/graph><\/graphml>"
}
{
    "id": "707eebbd4ee7c895e200d1155e7d6443",
    "chunk": " pose threats to shipping routes in the present day.\n\nGeology and geomorphology[edit]\nKarst formations\nIn 2000, UNESCO's World Heritage Committee inscribed Ha Long Bay in the World Heritage List according to its outstanding examples representing major stages of the Earth's history and its original limestone karstic geomorphologic features. Ha Long Bay and its adjacent areas consist of a part of the Sino-Vietnamese composite terrane having its development history from pre-Cambrian up to present day. During Phanerozoic, terrigenous, volcanogenic and cherty-carbonate sediments containing in abundance graptolites, bivalves, brachiopods, fishes, foraminiferans, corals, radiolarias, and flora, separated from one from another by 10 stratigraphic gaps, but the boundary between Devonian and Carboniferous has been considered as continuous. The limestone karstic geomorphology of the bay has developed since the Miocene, especially the cone-shaped hills (fengcong), or isolated high limestone karst towers (fenglin) with many remnants of old phreatic caves, old karstic foot caves, and marine notch caves forming magnificent limestone karst landforms unique to the world. The Quaternary geology was developed through 5 cycles with the intercalation of marine and continental environments. The present Ha Long Bay, in fact, appeared after the Middle Holocene maximum transgression, leaving ultimate zone of lateral undercutting in the limestone cliffs bearing many shells of oysters, having the 14C age as 2280 to >40,000 y. BP. Geological resources are abundant, including anthracite, petroleum, lignite, phosphate, oil shale, limestone and cement additives, kaolin, silica sand, dolomite, quartzite of exogenous origin, antimony, and mercury of hydrothermal origin. Additionally, there is surface water, groundwater and thermal mineral water on the shore of the Ha Long \u2013 B\u00e1i T\u1eed Long Bays, as well as other environmental resources.[15][16]\nIn terms of marine geology, this area is recorded as an especially coastal sedimentary environment. In the alkaline seawater environment, the chemical denudation process of calcium carbonate proceeds rapidly, creating wide, strangely shaped marine notches.\nThe bottom surface sediments are various from clay mud to sand, however, silty mud and clay mud dominate in distribution. Especially, the carbonate materials originated from organisms make up 60 to 65% sedimentary content. The surface sediments of coral reefs are mainly sand and pebbles, of which the carbonate materials account for more than 90%. The intertidal zone sediments are various, from clay mud to sand and gravel, depending on distinguished sedimentary environments such as mangrove marshes, tidal flats, beaches etc. At the small beaches, the sand sediments may be dominated quartz",
    "chunk_id": "707eebbd4ee7c895e200d1155e7d6443",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "a7c880e76d7a769299657713b6adecc1",
    "chunk": " in distribution. Especially, the carbonate materials originated from organisms make up 60 to 65% sedimentary content. The surface sediments of coral reefs are mainly sand and pebbles, of which the carbonate materials account for more than 90%. The intertidal zone sediments are various, from clay mud to sand and gravel, depending on distinguished sedimentary environments such as mangrove marshes, tidal flats, beaches etc. At the small beaches, the sand sediments may be dominated quartz or carbonate materials.\nThe sediment layers of the intertidal zone, the upper sea bed with a plain surface conserving ancient rivers, the systems of caves and their sediments, traces of ancient marine action forming distinctive notches, beaches and marine terraces, and mangrove swamps are important evidence of geological events and processes taking place during the Quaternary Period.[17]\n\nKarst geomorphology value[edit]\nLimestone islets\nDue to a simultaneous combination of ideal factors such as thick, pale, grey, and strong limestone layers, which are formed by fine-grained materials; hot and moist climate and slow tectonic process as a whole; Ha Long Bay has had a complete karst evolution for 20 million years. There are many types of karst topography in the bay, such as karst field.\nHa Long Bay is a mature karst landscape developed during a warm, wet, tropical climate. The sequence of stages in the evolution of a karst landscape over a period of 20 million years requires a combination of several distinct elements including a massive thickness of limestone, a hot wet climate and slow overall tectonic up lift. The process of karst formation is divided into five stages, the second of which is the formation of the distinctive do line karst. This is followed by the development of fengcong karst, which can be seen in the groups of hills on Bo Hon and Dau Be Inland. These cones with sloping sides average 100m in height with the tallest exceeding 200m. Fenglin karst is characterized by steep separate towers. The hundreds of rocky islands that form the beautiful and famous landscape of the Bay are the individual towers of a classic Fenglin landscape where the intervening plains have been submerged by the sea. Most towers reach a height of between 50 and 100m with a height to width ratio of about 6. The karst dolines were flooded by the sea, becoming the abundance of lakes that lie within the limestone islands. For example, Dau Be island at the mouth of the Bay has six enclosed lakes including those of the Ba Ham lakes lying within its fengcong karst. The Bay contains examples of the landscape elements of fengcong, fenglin and karst plain. These are not separate evolutionary stages but the result of natural non \u2013 uniform processes in the denudation of a large mass of limestone. Marine erosion created the notches which in",
    "chunk_id": "a7c880e76d7a769299657713b6adecc1",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "0937067a7538502197367bf20dd753fd",
    "chunk": " of lakes that lie within the limestone islands. For example, Dau Be island at the mouth of the Bay has six enclosed lakes including those of the Ba Ham lakes lying within its fengcong karst. The Bay contains examples of the landscape elements of fengcong, fenglin and karst plain. These are not separate evolutionary stages but the result of natural non \u2013 uniform processes in the denudation of a large mass of limestone. Marine erosion created the notches which in some places have been enlarged into caves. The marine notch is a feature of limestone coastline but, in Ha Long Bay, it has created the mature landscape.\nWithin Ha Long Bay, the main accessible caves are the older passages that survive from the time when the karst was evolving through its various stages of fengcong and fenglin. Three main types of caves can be recognized in the limestone islands (Waltham, T. 1998):[18]\n\nRemnants of old phreatic caves\nOld karstic foot caves\nMarine notch caves\nS\u1eedng S\u1ed1t cave\nThe first group consists of old phreatic caves which include Sung Sot, Tam Cung, Lau Dai, Thien Cung, Dau Go, Hoang Long, Thien Long. Nowadays, these caves lie at various heights. Sung Sot cave is on Bo Hon Island. From its truncated entrance chambers on a ledge high on the cliff, a passage of more than 10m high and wide descends to the south. Tam Cung is a large phreatic fissure cave that developed in the bedding planes of the limestone dividing the fissure cave into three chambers. Lau Dai is a cave with a complex of passages extending over 300 meters on the south side of Con Ngua Island. Thien Cung and Dau Go are remnants of the same old cave system. They both survive in the northern part of Dau Go Island at between 20 and 50m above sea level. Thien Cung has one large chamber more than 100m long, blocked at its ends and almost subdivided into smaller chambers by a massive wall of stalactites and stalagmites. Dau Go is a single large tunnel descending along a major set of fractures to a massive choke.\nThe second group of caves is the old karstic foot caves which include Trinh Lu, Bo Nau, Tien Ong and Trong caves. Foot caves are a ubiquitous feature of karst landscapes which have reached a stage of widespread lateral undercutting at base level. They may extend back into maze caves of stream caves draining from larger cave systems within the limestone. They are distinguished by the main elements of their passages being close to the horizontal and are commonly related to denuded or accumulated terraces at the old base levels. Trinh Nu, which is one of the larger foot caves in Ha Long Bay with its ceiling at about 12m above",
    "chunk_id": "0937067a7538502197367bf20dd753fd",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "efff03f4a7e50cf7d1b296180f0a11b5",
    "chunk": " Foot caves are a ubiquitous feature of karst landscapes which have reached a stage of widespread lateral undercutting at base level. They may extend back into maze caves of stream caves draining from larger cave systems within the limestone. They are distinguished by the main elements of their passages being close to the horizontal and are commonly related to denuded or accumulated terraces at the old base levels. Trinh Nu, which is one of the larger foot caves in Ha Long Bay with its ceiling at about 12m above sea level and about 80m in length, was developed in multiple stages. Bo Nau, a horizontal cave containing old stalactite deposits, cuts across the 25o dip of the bedding plane.\nThe third group is the marine notch caves which're a special feature of the karst of Ha Long Bay. The dissolution process of sea water acting on the limestone and erosion by wave action creates notches at the base of the cliffs. In advantageous conditions, dissolution of the limestone allows the cliff notches to be steadily deepened and extended into caves. Many of these at sea level extend right through the limestone hills into drowned dolinas which are now tidal lakes.\nA distinguishing feature of marine notch caves is an absolutely smooth and horizontal ceiling cut through the limestone. Some marine notch caves had not been not formed at present sea level, but old sea levels related to sea level changes in Holocene transgression, event to Pleistocene sea levels. Some of them preserved the development of old karstic foot caves in mainland environments or preserved the remnants of older phreatic caves. One of the most unusual features of Ha Long Bay is the Bo Ham lake group of hidden lakes and their connecting tunnel \u2013 notch caves in Dau Be Island. From the island's perimeter cliff a cave, 10m wide at water level and curving so that it is almost completely dark, extends about 150m to Lake 1. Luon Cave is on Bo Hon Island and extends 50m meters to an enclosed tidal lake. It has a massive stalactite hanging 2m down and truncated at the modern tidal level. It has passed through many stages in its formation.\nThe karst landscape of Ha Long Bay is of international significance and of fundamental importance to the science of geomorphology. The fenglin tower karst, which is the type present in much of Ha Long Bay, is the most extreme form of limestone landscape development. If these karst landscapes are broadly compared in terms of their height, steepness and number of their limestone towers, Ha Long Bay is probably second in the entire world only to Yangshuo, in China. However, Ha Long Bay has also been invaded by the sea so that the geomorphology of its limestone islands are, at least in part, the consequence of marine erosion. The marine invasion distinguishes Ha Long Bay and makes it unique in the world. There are other areas of submerged karst towers which were invaded by the sea, but",
    "chunk_id": "efff03f4a7e50cf7d1b296180f0a11b5",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [
        {
            "name": "\"KARST\"",
            "type": "\"CONCEPT\"",
            "description": "\"Karst refers to a type of limestone landscape that has been shaped by dissolution and erosion.\"",
            "source_id": "efff03f4a7e50cf7d1b296180f0a11b5"
        },
        {
            "name": "\"YANGSHUO\"",
            "type": "\"LOCATION\"",
            "description": "\"Yangshuo in China is a similar karst landscape to Ha Long Bay.\"",
            "source_id": "efff03f4a7e50cf7d1b296180f0a11b5"
        }
    ],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"&quot;KARST&quot;\">      <data key=\"d0\">\"CONCEPT\"<\/data>      <data key=\"d1\">\"Karst refers to a type of limestone landscape that has been shaped by dissolution and erosion.\"<\/data>      <data key=\"d2\">efff03f4a7e50cf7d1b296180f0a11b5<\/data>    <\/node>    <node id=\"&quot;YANGSHUO&quot;\">      <data key=\"d0\">\"LOCATION\"<\/data>      <data key=\"d1\">\"Yangshuo in China is a similar karst landscape to Ha Long Bay.\"<\/data>      <data key=\"d2\">efff03f4a7e50cf7d1b296180f0a11b5<\/data>    <\/node>  <\/graph><\/graphml>"
}
{
    "id": "f7ccebb3c46751ec03fa1d079f545a21",
    "chunk": " of their height, steepness and number of their limestone towers, Ha Long Bay is probably second in the entire world only to Yangshuo, in China. However, Ha Long Bay has also been invaded by the sea so that the geomorphology of its limestone islands are, at least in part, the consequence of marine erosion. The marine invasion distinguishes Ha Long Bay and makes it unique in the world. There are other areas of submerged karst towers which were invaded by the sea, but none is as extensive as Ha Long Bay.\n[16]\n[19]\n\nTimeline of geologic evolution[edit]\nSome of the most remarkable geological events in Ha Long Bay's history have occurred in the last 1,000 years, include the advance of the sea, the raising of the bay area, strong erosion that has formed coral, and, pure blue and heavily salted water. This process of erosion by seawater has deeply engraved the stone, contributing to its fantastic beauty. Present-day Ha Long Bay is the result of this long process of geological evolution that has been influenced by so many factors.\nDue to all these factors, tourists visiting Ha Long Bay are not only treated to one of the natural wonders of the world, but also to a precious geological museum that has been naturally preserved in the open air for the last 300 million years.\n\n\n\nYears ago\nGeologic period\nEvents\n\n\n570,000,000\u2013500,000,000\nBeginning of the Cambrian Period\nThe area, which now forms Ha Long Bay, was mostly mainland, subject to a process of rain-induced erosion.\n\n\n\nEnd of the Cambrian Period\nThe area was flooded, bringing Ha Long Bay into existence.\n\n\n500,000,000\u2013400,000,000\nOrdovician and Silurian periods\nThe area of north-east Vietnam was akin to deep sea, subjected to the constant activity of tectonic plates.\n\n\n\nEnd of the Silurian Period\nIt underwent a phase of inverse-motion that created mountains deep under the water.\n\n\n420,000,000\u2013340,000,000\nEnd of the Silurian Period and throughout the whole Devonian Period\nThe area was subjected to powerful forces of erosion from the hot and dry climate. At this point, Ha Long was part of a wide mainland that comprised most of today's South China Sea and the Chinese continental shelf.\n\n\n\nEnd of the Devonian Period\nDue to tectonic activity, the Ha Long area and the entire north-east region were raised from the depths.\n\n\n340,000,000\u2013240,000,000\nLater Carboniferous and Permian periods\nThe formation of the limestone layer more than 1,000 m thick. A shallow and warm sea reformed, which existed for approximately 100 million years. It created two kinds of limestone: the Cat Ba layer of the early Carboniferous period (450 m thick); and the Quang Hanh layer of the middle Carboniferous and the early Perm",
    "chunk_id": "f7ccebb3c46751ec03fa1d079f545a21",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "2a377133b5602cbc4f25decd2fbf2261",
    "chunk": " region were raised from the depths.\n\n\n340,000,000\u2013240,000,000\nLater Carboniferous and Permian periods\nThe formation of the limestone layer more than 1,000 m thick. A shallow and warm sea reformed, which existed for approximately 100 million years. It created two kinds of limestone: the Cat Ba layer of the early Carboniferous period (450 m thick); and the Quang Hanh layer of the middle Carboniferous and the early Permian period (750 m thick). These two layers constitute the majority of the islands of the Bay.\n\n\n67,000,000\nEnd of the Cretaceous Period\nHa Long Bay existed in the environment of a high mountainous mainland due to the influence of strong mountain-forming phases.\n\n\n\nMiddle of the Paleogene Period\nThese motions remain continuous and stable, while strong processes of erosion began, and after millions of years, a form of semi-highland topography took shape. The continuation of this erosion has progressively cut the highlands into blocks with altitudes similar to today's mountains.\n\n\n26,000,000\u201310,000,000\nNeogene Period\nThe development of the Ha Long depression.\n\n\n2,000,000\u201311,000\nPleistocene epoch of the Quaternary period\nThe process of erosion began dissolving the limestone-rich region of Ha Long, after that, forming the limestone plain was most active.\n\n\n68,000\u201311,000\nMiddle and late Pleistocene Epoch\nPeriod when the caves and grottoes of the area formed.\n\n\n\nEarly Holocene Epoch\nThe islands of today's Ha Long Bay are basically remnants of these mountains, flooded. Rainwater flowed into crevices in the limestone that had formed from tectonic activity. This steady erosion constantly widened the cracks, eventually creating today's formations.\n\n\n11,000\u20137,000\nHolocene Epoch\nThis period is notable for the advance of the sea.\n\n\n7,000\u20134,000\n\nThe movement of the sea reached its peak and began forming today's Ha Long Bay.\n\n\n4,000\u20133,000\n\nWith the sea in a steady process of recession, Ha Long culture began to develop.\n\n\n\nBeginning of the late Holocene Epoch\nThe level of the water once again increased, forming a marshy floor of canals and streams, and creating the water marks that can be seen on the stone cliffs of today.\n\nEcology[edit]\nTwo ecosystems coexist in Ha Long Bay: a tropical, moist, evergreen rainforest ecosystem and a marine and coastal ecosystem. Livistona halongensis, Impatiens halongensis, Chirita halongensis, Chirita hiepii, Chirita modesta, Paraboea halongensis, and Alpinia calcicola are among the seven endemic species found in the bay. There is also some bioluminescent plankton.[20]\nThe many islands that",
    "chunk_id": "2a377133b5602cbc4f25decd2fbf2261",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "0e1f93868d56332534b4143b9ec5125a",
    "chunk": "exist in Ha Long Bay: a tropical, moist, evergreen rainforest ecosystem and a marine and coastal ecosystem. Livistona halongensis, Impatiens halongensis, Chirita halongensis, Chirita hiepii, Chirita modesta, Paraboea halongensis, and Alpinia calcicola are among the seven endemic species found in the bay. There is also some bioluminescent plankton.[20]\nThe many islands that dot the bay are home to a great many other species, including (but likely not limited to): 477 magnoliales, 12 pteris, 20 salt marsh flora; and 4 amphibia, 10 reptilia, 40 aves, and 4 mammalia.\nCommon aquatic species found in the bay include: cuttlefish (m\u1ef1c); oyster (h\u00e0o); cyclinae (ng\u00e1n); prawns (penaeidea (t\u00f4m he), panulirus (t\u00f4m h\u00f9m), parapenaeopsis (t\u00f4m s\u1eaft), etc.); sipunculoideas (s\u00e1 s\u00f9ng); nerita (\u1ed1c \u0111\u0129a); charonia tritonis (\u1ed1c t\u00f9 v\u00e0); and c\u00e0 s\u00e1y. A new species of sponge, Cladocroce pansinii, was discovered in underwater caves attached to the bay in 2023.[21]\n\nEnvironmental damage[edit]\nA cruise ship docking in the bay\nWith an increasing tourist trade, mangroves and seagrass beds have been cleared and jetties and wharves have been built for tourist boats.\nGame fishing, often near coral reefs, is threatening many endangered species of fish.\nLocal government and businesses are aware of the problems and many measures have been taken to minimise the impact of tourism on the bay environment for sustainable economic growth like introducing eco-friendly tours and tight waste control on resorts.[22]\n\nAwards and designations[edit]\nIn 1962, the Vietnam Ministry of Culture, Sport and Tourism designated Ha Long Bay a 'Renowned National Landscape Monument'.\nHa Long Bay was first listed as a UNESCO World Heritage Site in 1994,[23] in recognition of its outstanding, universal aesthetic value. In 2000, the World Heritage Committee additionally recognised Ha Long Bay for its outstanding geological and geomorphological value,[12] and its World Heritage Listing was updated.[24]\nIn October 2011, the World Monuments Fund included the bay on the 2012 World Monuments Watch, citing pressure from tourism and associated developments as threats to the site that must be addressed.\nIn 2012, the New 7 Wonders Foundation officially named Ha Long Bay as one of the New 7 Wonders of Nature.\nHa Long Bay is also a part of the Club of the Most Beautiful Bays of the World.[25]\n\nPopular culture[edit]\nLiterature[edit",
    "chunk_id": "0e1f93868d56332534b4143b9ec5125a",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "9100a943429fcc125c44af91babbc2ba",
    "chunk": " October 2011, the World Monuments Fund included the bay on the 2012 World Monuments Watch, citing pressure from tourism and associated developments as threats to the site that must be addressed.\nIn 2012, the New 7 Wonders Foundation officially named Ha Long Bay as one of the New 7 Wonders of Nature.\nHa Long Bay is also a part of the Club of the Most Beautiful Bays of the World.[25]\n\nPopular culture[edit]\nLiterature[edit]\nIn writings about Ha Long Bay, the following Vietnamese writers wrote:\n\nNguy\u1ec5n Tr\u00e3i: \"This wonder is ground raising up into the middle of the high sky\".\nXu\u00e2n Di\u1ec7u: \"Here is the unfinished works of the Beings...Here is the stones which the Giant played and threw away\".\nNguy\u00ean Ng\u1ecdc: \"...to form this first- rate wonder, nature only uses: Stone and Water...There are just only two materials themselves chosen from as much as materials, in order to write, to draw, to sculpture, to create everything...It is quite possible that here is the image of the future world\".\nView of the bay Ho Chi Minh: \"It is the wonder that one cannot impart to others\".\nPham V\u0103n \u0110\u1ed3ng: \"Is it one scenery or many sceneries? Is it the scenery in the world or somewhere?\".\nNguy\u1ec5n Tu\u00e2n: \"Only mountains accept to be old, but Ha Long sea and wave are young forever\".\nHuy C\u1eadn: \"Night breathes, stars wave Ha Long's water\".\nCh\u1ebf Lan Vi\u00ean:\n\"Ha Long, B\u00e1i T\u1eed Long - Dragons were hidden, only stones still remain\nOn the moonlight nights, stones meditate as men do...\"\nLord Tr\u1ecbnh C\u01b0\u01a1ng overflowed with emotion: \"Mountains are glistened by water shadow, water spills all over the sky\".\n\nAncient tales[edit]\nPanorama of the bay, taken on Monkey Island, 2013\nThe inhabitants of the bay and its adjacent city have transmitted numerous ancient tales explaining names given to various isles and caves in the bay.[26]\n\n\u0110\u1ea7u G\u1ed7 cave (\"the end of wooden bars\" cave): these wooden bars in this cave are the remnants of sharped wooden columns built under the water level by the order of Tr\u1ea7n H\u01b0ng \u0110\u1ea1o commander in order to sink Mongolian invaders' ships in the 13th century.\nKim Quy cave (\"Golden Turtle\" cave): it is told that the Golden Turtle swam toward the Eastern Sea (international name: South China Sea) after returning the holy sword which had assisted King L\u00ea Th\u00e1i T\u1ed5 in the combat against Ming invaders from China. Next, with the approval of the Sea King, Golden Turtle continued to fight against monsters in this marine area. The turtle became exhausted and died in a cave. Consequently, the cave was named",
    "chunk_id": "9100a943429fcc125c44af91babbc2ba",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [
        {
            "name": "\"HA LONG BAY\"",
            "type": "\"LOCATION\"",
            "description": "\"Ha Long Bay is a bay located in Vietnam.\"",
            "source_id": "9100a943429fcc125c44af91babbc2ba"
        },
        {
            "name": "\"WORLD MONUMENTS FUND\"",
            "type": "\"ORGANIZATION\"",
            "description": "\"The World Monuments Fund included Ha Long Bay on the 2012 World Monuments Watch.\"",
            "source_id": "9100a943429fcc125c44af91babbc2ba"
        }
    ],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"&quot;HA LONG BAY&quot;\">      <data key=\"d0\">\"LOCATION\"<\/data>      <data key=\"d1\">\"Ha Long Bay is a bay located in Vietnam.\"<\/data>      <data key=\"d2\">9100a943429fcc125c44af91babbc2ba<\/data>    <\/node>    <node id=\"&quot;WORLD MONUMENTS FUND&quot;\">      <data key=\"d0\">\"ORGANIZATION\"<\/data>      <data key=\"d1\">\"The World Monuments Fund included Ha Long Bay on the 2012 World Monuments Watch.\"<\/data>      <data key=\"d2\">9100a943429fcc125c44af91babbc2ba<\/data>    <\/node>  <\/graph><\/graphml>"
}
{
    "id": "5f13431b779b01bc01957f063721b162",
    "chunk": " the 13th century.\nKim Quy cave (\"Golden Turtle\" cave): it is told that the Golden Turtle swam toward the Eastern Sea (international name: South China Sea) after returning the holy sword which had assisted King L\u00ea Th\u00e1i T\u1ed5 in the combat against Ming invaders from China. Next, with the approval of the Sea King, Golden Turtle continued to fight against monsters in this marine area. The turtle became exhausted and died in a cave. Consequently, the cave was named after the Golden Turtle.\nCon C\u00f3c islet (Frog islet): a frog-like isle. According to ancient tales, in a year of severe drought, a frog directed all animals to the Heaven and protested against the God. They demonstrated in favour of making rain. As a result, the God must accept the frog as his uncle. Since then, whenever frogs grind their teeth, the God has to pour water down the ground.\nHang Tr\u1ed1ng and Hang Trinh N\u1eef (Male cave and Virgin cave): the tale's about a beautiful woman had fallen in love with a fisherman whom must sail to the sea not so long after their engagement, the landlord saw this beautiful girl and captured her, but with her resistance, the landlord exiled the girl to remote island. After being left to starve, the girl died and turned into a statue people called Hang Trinh N\u1eef (Virgin Cave). Her betrothed ran to the girl's island and when he found out what had happened, he turned into an islet situated nearby called Hang Tr\u1ed1ng (Male cave).\nThi\u00ean Cung cave (literally: Paradise cave): this cave is one of the places associated with the ancient dragon king. It told that Thi\u00ean Cung cave was the place where the Dragon King's seven-day marriage took place. To congratulate the couple, many dragons and elephants visited to dance and fly.\nConservation Issues[edit]\nTourism development in Ha Long Bay and its impact on the regional landscape.\nImpacts of Human and Natural Factors on the Bay Area[edit]\nHa Long, Hai Phong, and Hanoi are significant urban centers driving the economic development in northern Vietnam. The economic growth in these urban areas, coupled with the rapid rise of the southern regions in China, including Hong Kong, have led to increasing human pressures on Ha Long Bay.[5] The coastal areas of Quang Ninh province and Hai Phong City have experienced rapid growth in infrastructure development, particularly in transportation, shipping, coal mining, and tourism-related industries.[5] Since 1999, the Asian Development Bank (ADB) has warned that constructing new ports in the Ha Long Bay area could lead to increased maritime traffic in the region, posing threats to the bay's infrastructure and the social infrastructure supporting tourism. Pollution from industrial waste, overexploitation, and overfishing also pose significant threats. Some argue that there is a need for cautious consideration of development in the bay area through effective",
    "chunk_id": "5f13431b779b01bc01957f063721b162",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "e9773f91bb6d97de08276267888f4990",
    "chunk": " particularly in transportation, shipping, coal mining, and tourism-related industries.[5] Since 1999, the Asian Development Bank (ADB) has warned that constructing new ports in the Ha Long Bay area could lead to increased maritime traffic in the region, posing threats to the bay's infrastructure and the social infrastructure supporting tourism. Pollution from industrial waste, overexploitation, and overfishing also pose significant threats. Some argue that there is a need for cautious consideration of development in the bay area through effective management structures, given its crucial environmental significance for the entire region.[5]\nCurrently, the expansion of urban areas and population growth, construction of ports and factories, tourism and service activities, household and industrial waste, fishing and aquaculture practices, have not only become threats but have also caused alarming levels of environmental pollution and landscape changes in Ha Long Bay.[27][28] Due to pollution, the once thriving coral reefs in the deep sea of Ha Long Bay are deteriorating.[29] The formerly clear waters of the bay are increasingly becoming turbid and sedimented, prompting scientists to warn of the possibility of Ha Long Bay becoming \"swamped.\"[30] Additionally, as Ha Long Bay is surrounded by thousands of limestone islands, which are mostly good construction materials, they are susceptible to private exploitation, leading to landscape distortion.[31]\nOn another aspect, global climate change with rising sea levels will strongly impact the landscape, island systems, caves, and biodiversity of Ha Long Bay. Vietnam currently lacks the necessary human and material resources to adequately respond to these challenges.[32]\nIn terms of community culture, an issue that many International tourism have complained about is the lack of environmental awareness among both tourists and local communities. The modern, civilized, and courteous image of Ha Long tourism has not been fully established as desired. There are still instances of beggars harassing tourists, which affects the tourism environment of the heritage site.[31] Efforts in education and propaganda to raise awareness among the local population, restrictions on resort development on islands, and the implementation of eco-tourism standards and heritage conservation regulations for the surrounding waters of the heritage site are significant challenges for the local government. The stalactites in the cave system of Ha Long Bay have been vandalized, cut, and taken away for use in decorating artificial landscapes (2016). Some caves have even been covered with concrete to serve as banquet venues.[33] Moreover, the activities of fishing boats and tourists also generate significant amounts of waste pollution that the authorities have yet to effectively manage.[34]\n\nConservation Efforts[edit]\nIn an effort to prevent the negative impact of human activities on the natural environment of Ha Long Bay, the authorities of Quang Ninh province have prohibited high-speed motorboats serving tourists in the bay area to protect the environment and biodiversity. Additionally, the province has relocated fishing households living on floating villages to the mainland to protect the water environment of Ha Long Bay.[35] Furthermore, the extraction of coal and stone within the",
    "chunk_id": "e9773f91bb6d97de08276267888f4990",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}
{
    "id": "fad09da074ab52e3c2b56b9eb5d43fce",
    "chunk": " authorities have yet to effectively manage.[34]\n\nConservation Efforts[edit]\nIn an effort to prevent the negative impact of human activities on the natural environment of Ha Long Bay, the authorities of Quang Ninh province have prohibited high-speed motorboats serving tourists in the bay area to protect the environment and biodiversity. Additionally, the province has relocated fishing households living on floating villages to the mainland to protect the water environment of Ha Long Bay.[35] Furthermore, the extraction of coal and stone within the heritage area has been banned to prevent coal and mud pollution in the bay as advised by UNESCO.[36] In the bay area, some local residents have voluntarily taken action to preserve the landscape by organizing volunteer groups to collect and handle waste.[37] Starting from September 1, 2019, the People's Committee of Ha Long City strictly banned the use of single-use plastic products in the bay area.[38][39][40][41][42] This is a resolute and significant step towards conserving the bay's environment.\nThe similarity in landscape, geology, biodiversity, as well as cultural and archaeological values of the entire region, including not only Ha Long Bay but also Cat Ba Archipelago and Bai Tu Long Bay, has led to scientific research in geology, archaeology, culture, and tourism, as well as fishing activities, extending beyond the boundaries of Ha Long Bay. Some experts suggest considering the expansion of the conservation area, not only limiting it to the small area of Ha Long Bay but also encompassing the surrounding sea area, including the areas close to the Vietnam-China border.[32] With a length of about 300 km and a width of about 60 km, the entire area can be seen and conserved as a unique marine ecosystem of Vietnam.[32]\n\nWikimedia Commons has media related to:\nV\u1ecbnh Ha Long (category)\n\nWikivoyage has a travel guide for Ha Long Bay.\n\nOfficial website\nUNESCO's World Heritage Convention: Ha Long Bay*\nThe Emerald Isles of Ha Long Bay at NASA Earth Observatory, May 7, 2022\nEnvironmental capacity Ha Long Bay \u2013 Bai Tu Long. Publisher: Natural Science and Technology. Hanoi. Editor: Nguyen Khoa Son, ISBN\u00a0978-604-913-063-2 \u2013 in Vietnamese\nVietnamese Sea and Islands \u2013 position Resources, and typical geological and ecological wonders. Publisher Science and Technology. Ha Noi, Editor: Nguyen Khoa Son, ISBN\u00a0978-604-913-063-2\u2013 in Vietnamese\nvteWorld Heritage Sites in Vietnam\nCentral Sector of the Imperial Citadel of Th\u0103ng Long\nCitadel of the H\u1ed3 Dynasty\nComplex of Hu\u1ebf Monuments\nHa Long Bay\u2013Cat Ba Archipelago\nH\u1ed9i An Ancient Town\nM\u1ef9 S\u01a1n Sanctuary\nPhong Nha \u2013 K\u1ebb B\u00e0ng National Park\nTr\u00e0ng An Landscape Complex\n\nvteNew 7 Wonders of Nature\nAmazon",
    "chunk_id": "fad09da074ab52e3c2b56b9eb5d43fce",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 600,
    "entities": [
        {
            "name": "\"PERSON B\"",
            "type": "\"PERSON\"",
            "description": "\"PERSON B is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task.\"",
            "source_id": "fad09da074ab52e3c2b56b9eb5d43fce"
        },
        {
            "name": "\"CONTROL\"",
            "type": "\"CONCEPT\"",
            "description": "\"Control refers to the ability to manage or govern, which is challenged by an intelligence that writes its own rules.\"",
            "source_id": "fad09da074ab52e3c2b56b9eb5d43fce"
        },
        {
            "name": "\"INTELLIGENCE\"",
            "type": "\"CONCEPT\"",
            "description": "\"Intelligence here refers to an unknown entity capable of writing its own rules and learning to communicate.\"",
            "source_id": "fad09da074ab52e3c2b56b9eb5d43fce"
        },
        {
            "name": "\"FIRST CONTACT\"",
            "type": "\"EVENT\"",
            "description": "\"First Contact is the potential initial communication between humanity and an unknown intelligence.\"",
            "source_id": "fad09da074ab52e3c2b56b9eb5d43fce"
        },
        {
            "name": "\"HUMANITY'S RESPONSE\"",
            "type": "\"EVENT\"",
            "description": "\"Humanity's Response is the collective action taken by PERSON B's team in response to a message from an unknown intelligence.\"",
            "source_id": "fad09da074ab52e3c2b56b9eb5d43fce"
        },
        {
            "name": "\"PERSON C\"",
            "type": "",
            "description": "",
            "source_id": "fad09da074ab52e3c2b56b9eb5d43fce"
        }
    ],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <key id=\"d5\" for=\"edge\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d4\" for=\"edge\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d3\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\" \/>  <key id=\"d2\" for=\"node\" attr.name=\"source_id\" attr.type=\"string\" \/>  <key id=\"d1\" for=\"node\" attr.name=\"description\" attr.type=\"string\" \/>  <key id=\"d0\" for=\"node\" attr.name=\"type\" attr.type=\"string\" \/>  <graph edgedefault=\"undirected\">    <node id=\"&quot;PERSON B&quot;\">      <data key=\"d0\">\"PERSON\"<\/data>      <data key=\"d1\">\"PERSON B is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task.\"<\/data>      <data key=\"d2\">fad09da074ab52e3c2b56b9eb5d43fce<\/data>    <\/node>    <node id=\"&quot;CONTROL&quot;\">      <data key=\"d0\">\"CONCEPT\"<\/data>      <data key=\"d1\">\"Control refers to the ability to manage or govern, which is challenged by an intelligence that writes its own rules.\"<\/data>      <data key=\"d2\">fad09da074ab52e3c2b56b9eb5d43fce<\/data>    <\/node>    <node id=\"&quot;INTELLIGENCE&quot;\">      <data key=\"d0\">\"CONCEPT\"<\/data>      <data key=\"d1\">\"Intelligence here refers to an unknown entity capable of writing its own rules and learning to communicate.\"<\/data>      <data key=\"d2\">fad09da074ab52e3c2b56b9eb5d43fce<\/data>    <\/node>    <node id=\"&quot;FIRST CONTACT&quot;\">      <data key=\"d0\">\"EVENT\"<\/data>      <data key=\"d1\">\"First Contact is the potential initial communication between humanity and an unknown intelligence.\"<\/data>      <data key=\"d2\">fad09da074ab52e3c2b56b9eb5d43fce<\/data>    <\/node>    <node id=\"&quot;HUMANITY'S RESPONSE&quot;\">      <data key=\"d0\">\"EVENT\"<\/data>      <data key=\"d1\">\"Humanity's Response is the collective action taken by PERSON B's team in response to a message from an unknown intelligence.\"<\/data>      <data key=\"d2\">fad09da074ab52e3c2b56b9eb5d43fce<\/data>    <\/node>    <node id=\"&quot;PERSON C&quot;\">      <data key=\"d0\" \/>      <data key=\"d1\" \/>      <data key=\"d2\">fad09da074ab52e3c2b56b9eb5d43fce<\/data>    <\/node>    <edge source=\"&quot;PERSON B&quot;\" target=\"&quot;FIRST CONTACT&quot;\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">\"PERSON B leads the team that might be making the First Contact with the unknown intelligence.\"<\/data>      <data key=\"d5\">fad09da074ab52e3c2b56b9eb5d43fce<\/data>    <\/edge>    <edge source=\"&quot;PERSON B&quot;\" target=\"&quot;HUMANITY'S RESPONSE&quot;\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">\"PERSON B and his team are the key figures in Humanity's Response to the unknown intelligence.\"<\/data>      <data key=\"d5\">fad09da074ab52e3c2b56b9eb5d43fce<\/data>    <\/edge>    <edge source=\"&quot;CONTROL&quot;\" target=\"&quot;INTELLIGENCE&quot;\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">\"The concept of Control is challenged by the Intelligence that writes its own rules.\"<\/data>      <data key=\"d5\">fad09da074ab52e3c2b56b9eb5d43fce<\/data>    <\/edge>    <edge source=\"&quot;INTELLIGENCE&quot;\" target=\"&quot;PERSON C&quot;\">      <data key=\"d3\">1.0<\/data>      <data key=\"d4\">\"Person C is directly involved in the process of learning to communicate with the unknown intelligence.\"<\/data>      <data key=\"d5\">fad09da074ab52e3c2b56b9eb5d43fce<\/data>    <\/edge>  <\/graph><\/graphml>"
}
{
    "id": "99bb7322f729715259a20a37810f53ab",
    "chunk": "-604-913-063-2\u2013 in Vietnamese\nvteWorld Heritage Sites in Vietnam\nCentral Sector of the Imperial Citadel of Th\u0103ng Long\nCitadel of the H\u1ed3 Dynasty\nComplex of Hu\u1ebf Monuments\nHa Long Bay\u2013Cat Ba Archipelago\nH\u1ed9i An Ancient Town\nM\u1ef9 S\u01a1n Sanctuary\nPhong Nha \u2013 K\u1ebb B\u00e0ng National Park\nTr\u00e0ng An Landscape Complex\n\nvteNew 7 Wonders of Nature\nAmazon rainforest and river\nHa Long Bay\nIguazu \/ Igua\u00e7u Falls\nJeju Island\nKomodo Island\nPuerto Princesa Underground River\nTable Mountain\n\n\nList of fishing villages\nGlossary\nIndex\nOutline\n\nAuthority control databases InternationalVIAFNationalGermanyUnited StatesIsrael\n\n\nRetrieved from \"https:\/\/en.wikipedia.org\/w\/index.php?title=Ha_Long_Bay&oldid=1241423569\"\nCategories: ASEAN heritage parksBays of VietnamFishing communities in VietnamGulf of TonkinLandforms of Qu\u1ea3ng Ninh provincePlaces with bioluminescenceTourist attractions in Qu\u1ea3ng Ninh provinceWorld Heritage Sites in VietnamHidden categories: Pages using gadget WikiMiniAtlasPages using the Phonos extensionWebarchive template wayback linksArticles with Vietnamese-language sources (vi)CS1 Vietnamese-language sources (vi)CS1 maint: multiple names: authors listCS1 maint: numeric names: authors listArticles with short descriptionShort description is different from WikidataUse dmy dates from October 2021Coordinates on WikidataArticles containing Vietnamese-language textPages with Vietnamese IPAPages including recorded pronunciationsWikipedia articles needing clarification from February 2019All articles with vague or ambiguous timeVague or ambiguous time from July 2022Articles to be expanded from October 2019All articles to be expandedArticles containing video clips\n\n\n\n\n\n\n This page was last edited on 21 August 2024, at 02:35\u00a0(UTC).\nText is available under the Creative Commons Attribution-ShareAlike License 4.0;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n\n\nPrivacy policy\nAbout Wikipedia\nDisclaimers\nContact Wikipedia\nCode of Conduct\nDevelopers\nStatistics\nCookie statement\nMobile view\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "chunk_id": "99bb7322f729715259a20a37810f53ab",
    "document_ids": [
        "ebf347f4fc8d4fb42488e60ac6149376"
    ],
    "n_tokens": 490,
    "entities": [],
    "entity_graph": "<graphml xmlns=\"http:\/\/graphml.graphdrawing.org\/xmlns\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/graphml.graphdrawing.org\/xmlns http:\/\/graphml.graphdrawing.org\/xmlns\/1.0\/graphml.xsd\">  <graph edgedefault=\"undirected\" \/><\/graphml>"
}